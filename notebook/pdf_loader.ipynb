{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c879503",
   "metadata": {},
   "source": [
    "### RAG Pipelines- Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6377b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a312dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for PDF files in: /home/adil/Builds/RAG/data\n",
      "Found 1 PDF files to process\n",
      "\n",
      "Processing: Getting Started with SQL.pdf\n",
      "  ✓ Loaded 133 pages\n",
      "\n",
      "Total documents loaded: 133\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"Searching for PDF files in: {pdf_dir.resolve()}\")\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96748c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 0, 'page_label': 'Cover', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Thomas Nield\\n Getting Started with\\n  SQL\\nA HANDS-ON APPROACH  \\nFOR BEGINNERS'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 1, 'page_label': 'BackCover', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 2, 'page_label': 'i', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Thomas Nield\\nBoston\\nGetting Started with SQL\\nA Hands-on Approach for Beginners'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 3, 'page_label': 'ii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='978-1-491-93861-4\\n[LSI]\\nGetting Started with SQL\\nby Thomas Nield\\nCopyright © 2016 Thomas Nield. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://safaribooksonline.com). For more information, contact our corporate/\\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\\nEditor: Shannon Cutt\\nProduction Editor: Shiny Kalapurakkel\\nCopyeditor: Jasmine Kwityn\\nProofreader: Rachel Head\\nIndexer: Ellen Troutman-Zaig\\nInterior Designer: David Futato\\nCover Designer: Randy Comer\\nIllustrator: Rebecca Demarest\\nFebruary 2016:  First Edition\\nRevision History for the First Edition\\n2016-02-08: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491938614 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Getting Started with SQL , the cover\\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 4, 'page_label': 'iii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Table of Contents\\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\\n1. Why Learn SQL?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nWhat Is SQL and Why Is It Marketable?                                                                        1\\nWho Is SQL For?                                                                                                                2\\n2. Databases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is a Database?                                                                                                           3\\nExploring Relational Databases                                                                                       3\\nWhy Separate Tables?                                                                                                        4\\nChoosing a Database Solution                                                                                         5\\nLightweight Databases                                                                                                       5\\nCentralized Databases                                                                                                       6\\n3. SQLite. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  9\\nWhat Is SQLite?                                                                                                                  9\\nSQLiteStudio                                                                                                                       9\\nImporting and Navigating Databases                                                                            10\\n4. SELECT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19\\nRetrieving Data with SQL                                                                                               19\\nExpressions in SELECT Statements                                                                              23\\nText Concatenation                                                                                                          27\\nSummary                                                                                                                           28\\niii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 5, 'page_label': 'iv', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='5. WHERE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  29\\nFiltering Records                                                                                                              29\\nUsing WHERE on Numbers                                                                                           30\\nAND, OR, and IN Statements                                                                                        31\\nUsing WHERE on Text                                                                                                   32\\nUsing WHERE on Booleans                                                                                           34\\nHandling NULL                                                                                                               34\\nGrouping Conditions                                                                                                      36\\nSummary                                                                                                                           37\\n6. GROUP BY and ORDER BY. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  39\\nGrouping Records                                                                                                            39\\nOrdering Records                                                                                                             41\\nAggregate Functions                                                                                                        42\\nThe HAVING Statement                                                                                                 45\\nGetting Distinct Records                                                                                                46\\nSummary                                                                                                                           46\\n7. CASE Statements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  47\\nThe CASE Statement                                                                                                       47\\nGrouping CASE Statements                                                                                           48\\nThe “Zero/Null” CASE Trick                                                                                          49\\nSummary                                                                                                                           52\\n8. JOIN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  53\\nStitching Tables Together                                                                                                53\\nINNER JOIN                                                                                                                    55\\nLEFT JOIN                                                                                                                        58\\nOther JOIN Types                                                                                                            61\\nJoining Multiple Tables                                                                                                   61\\nGrouping JOINs                                                                                                               63\\nSummary                                                                                                                           66\\n9. Database Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  67\\nPlanning a Database                                                                                                        67\\nThe SurgeTech Conference                                                                                             69\\nATTENDEE                                                                                                                  69\\nCOMPANY                                                                                                                   69\\nPRESENTATION                                                                                                         70\\nROOM                                                                                                                           70\\nPRESENTATION_ATTENDANCE                                                                          70\\nPrimary and Foreign Keys                                                                                              70\\niv | Table of Contents'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 6, 'page_label': 'v', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='The Schema                                                                                                                       71\\nCreating a New Database                                                                                                73\\nCREATE TABLE                                                                                                              76\\nSetting the Foreign Keys                                                                                                 84\\nCreating Views                                                                                                                 86\\nSummary                                                                                                                           89\\n10. Managing Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  91\\nINSERT                                                                                                                              91\\nMultiple INSERTs                                                                                                         93\\nTesting the Foreign Keys                                                                                             93\\nDELETE                                                                                                                            94\\nTRUNCATE TABLE                                                                                                        94\\nUPDATE                                                                                                                           95\\nDROP TABLE                                                                                                                   95\\nSummary                                                                                                                           95\\n11. Going Forward. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  97\\nA. Operators and Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  101\\nB. Supplementary Topics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  107\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  113\\nTable of Contents | v'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 7, 'page_label': 'vi', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 8, 'page_label': 'vii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Foreword\\nOver the past three decades, computers have taken over the world. Twenty-five years\\nago, we lived analog. We communicated using an analog POTS telephone, we tuned\\nin to analog FM radio stations, and we went to the library and browsed the stacks for\\ninformation. Buildings were constructed using hand-drawn blueprints; graphic artists\\nworked with pen, brush, and ink; musicians plucked strings and blew into horns and\\nrecorded on analog tape; and airplanes were controlled by physical cables connecting\\nthe yoke to the control surfaces.\\nBut now everything is computerized and digital. Consequently, every member of\\nsociety needs to be familiar with computers. That does not mean having the deep\\nknowledge of a techie, but just as poets need to study a little math and physics, and\\njust as mathematicians need to read a little poetry, so too does everybody today need\\nto know something about computers.\\nI think that this book really helps to address the knowledge gap between techies and\\nlaypeople, by providing an accessible and easy-to-read discussion of SQL—a core\\ndatabase technology.\\n—Richard Hipp, Creator of SQLite\\nvii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 9, 'page_label': 'viii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 10, 'page_label': 'ix', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Preface\\nNobody needs to learn how a car engine works in order to drive a car. The whole\\npoint of technologies like SQL is to allow you to focus on the business problem, and\\nnot worry about how the technical details are executed. This book will give you a\\npractical focus on using SQL, and will steer away from unnecessary technical details\\nthat are likely not pertinent to your immediate needs. Much of the content revolves\\naround hands-on exercises with real databases you can download so you see how\\nconcepts are applied. When you finish this book you will have practical knowledge to\\nwork with databases, as well as use them to overcome your business challenges.\\nHow to Use This Book\\nThis book is designed to teach the fundamentals of SQL and working with databases.\\nReaders who have experience using Excel spreadsheets should find this material\\naccessible but still challenging. Individuals who have not worked with Excel may be\\nmore challenged. It is helpful to be familiar with concepts used in Excel, such as rows,\\ncolumns, tables, mathematical expressions (e.g., Excel formulas), and aggregate calcu‐\\nlations (e.g., SUM, AVG, MIN, MAX, COUNT). These concepts will still be taught\\nhere, but some practical Excel experience will help expedite understanding.\\nBasic computer literacy is required, and readers should know how to navigate folders\\nand copy/paste files, as well as download and save files from the Web.\\nAs you go through the material, have a computer on hand to practice the examples.\\nWhile some people can learn by just reading, it is best to practice the material at some\\npoint to reinforce the knowledge.\\nProficiency comes through repeated use and practice. In your job, it is likely that you\\nwill use some SQL functionalities heavily and others not as much. That is OK. It is\\nmore important to become proficient in what your job requires, and consult this\\nbook (or Google) as a reference when you need answers about an unfamiliar topic.\\nix'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 11, 'page_label': 'x', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='When working with technology, you are never expected to know everything. As a\\nmatter of fact, technology topics are so vast in number it would be impossible. So it is\\nhelpful to develop a degree of tunnel vision and learn only enough to fulfill the task at\\nhand. Otherwise, you can get overwhelmed or distracted learning irrelevant topics.\\nHopefully this book will give you a foundation of knowledge, and afterward you can\\ncontinue to learn about topics that are pertinent to you.\\nY ou are always welcome to reach out to me at tmnield@outlook.com, and I will answer\\nany questions to the best of my ability. If you have questions about positioning your\\ncareer with technical skillsets or have a SQL question, I might be able to help. I hope\\nthat this material not only augments your skillset and career opportunities, but also\\nsparks new interests that excite you like it did for me.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nThis element signifies a general note.\\nUsing Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/thomasnield/oreilly_getting_started_with_sql.\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nx | Preface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 12, 'page_label': 'xi', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='the code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “Getting Started with SQL by Thomas\\nNield (O’Reilly). Copyright 2016 Thomas Nield, 978-1-4919-3861-4. ”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nSafari® Books Online\\nSafari Books Online is an on-demand digital library that deliv‐\\ners expert content in both book and video form from the\\nworld’s leading authors in technology and business.\\nTechnology professionals, software developers, web designers, and business and crea‐\\ntive professionals use Safari Books Online as their primary resource for research,\\nproblem solving, learning, and certification training.\\nSafari Books Online offers a range of plans and pricing  for enterprise, government,\\neducation, and individuals.\\nMembers have access to thousands of books, training videos, and prepublication\\nmanuscripts in one fully searchable database from publishers like O’Reilly Media,\\nPrentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que,\\nPeachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kauf‐\\nmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,\\nMcGraw-Hill, Jones & Bartlett, Course Technology, and hundreds more. For more\\ninformation about Safari Books Online, please visit us online.\\nPreface | xi'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 13, 'page_label': 'xii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='How to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/getting-started-with-sql.\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nAcknowledgments\\nI am blessed to have amazing people surrounding me, and I realize how central they\\nhave been in my life and everything I do. If it was not for them, this book would\\nprobably not have happened.\\nFirst and foremost, I would like to thank my mom and dad. They have given every‐\\nthing to secure my future. I know for a fact that I would not have the opportunities I\\nhave today if it was not for them. My dad worked hard to provide a better education\\nfor my brothers and me, and my mother always pushed me forward, even when I\\nresisted. She taught me to never settle and always struggle through my limits.\\nI cannot express enough gratitude toward my leaders, managers, and colleagues at\\nSouthwest Airlines Revenue Management. Justin Jones and Timothy Keeney have a\\nwarrior spirit and zeal for innovation that few possess. They truly define the leader‐\\nship and spirit of Southwest Airlines, but more importantly they are good guys. They\\nwill always be my friends and they’ve made it hard to imagine a life without South‐\\nwest Airlines.\\nxii | Preface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 14, 'page_label': 'xiii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Robert Haun, Brice Taylor, and Allison Russell continuously work to make our team\\nthe forefront of innovation and continuously pursue new ideas, and I am blessed to\\nwork in the environment they have helped create. I also have to thank Matt Louis for\\nbringing me on board at Revenue Management, and Steven Barsalou who made me\\nrealize how little I really knew about SQL. Steven is the first person who came to\\nmind when I needed a reviewer for this book, and I am grateful he came on board\\nthis project.\\nThen there is the project team I work with every day: Brian Denholm, Paul Zigler,\\nBridget Green, Todd Randolph, and Chris Solomon. As a team, the feats we pull off\\nnever cease to amaze me. Brian is the kind of project manager that can effectively\\nbridge technology and business jargon together, and he will not hesitate to get his\\nhands dirty with SQL and the occasional code review. I want to give a special thanks\\nto Chris Solomon for helping me with everything I do every day. He not only has a\\nrare talent to absorb high volumes of technical knowledge and maintain it in a busi‐\\nness perspective, but he is also a nice guy that I am privileged to be friends with.\\nChris is always a key player in any project, and I was thrilled when he agreed to\\nreview this book.\\nI cannot forget the great people who worked at Southwest Airlines Ground Ops\\nSafety Regulatory Compliance, including Marc Stank, Reuben Miller, Mary Noel\\nHennes, and everybody else I had the privilege of working with. I interned and con‐\\ntracted with that department a few years back and some of my fondest memories are\\nthere. It was there I discovered my passion for technology, and they provided many\\nopportunities for me to pursue that, whether it was throwing together databases or\\nprototyping an iPad app.\\nWhen I announced I was publishing this book I did not expect Richard Hipp, the\\nfounder and creator of SQLite, to reach out to me. Richard graciously stepped up to\\nbe the technical reviewer for this book and it has been a tremendous honor to have\\nhim on board. The technology community continues to amaze me, and the fact\\nRichard Hipp joined this project shows how unique and close-knit the community\\nreally is.\\nShannon Cutt has been my editor at O’Reilly for this book. This is my first book and I\\nwas uncertain what the publishing experience would be like. But Shannon made pub‐\\nlishing such a pleasant experience that I am eager to write again. Thanks Shannon,\\nyou have been awesome!\\nLast but not least, I want to thank Watermark Church and the volunteers at Careers\\nin Motion for creating the vehicle that made this book happen. I initially wrote this\\n“book” as a public service to help unemployed professionals in the Dallas area. It was\\nat their encouragement that I decided to publish it, and I want to give a special thanks\\nto Martha Garza for her insistence. I have learned remarkable things can happen\\nwhen you give your time to help others.\\nPreface | xiii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 15, 'page_label': 'xiv', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 16, 'page_label': '1', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 1\\nWhy Learn SQL?\\nWhat Is SQL and Why Is It Marketable?\\nIt is an obvious statement that the business landscape is shifting rapidly. A lot of this\\nis enabled by technology and the explosion of business data. Companies are investing\\nvast amounts of capital to gather and warehouse data. But what many business lead‐\\ners and managers currently struggle with is how to make sense of this data and use it.\\nThis is where SQL, which stands for Structured Query Language, comes in. It provides\\na means to access and manipulate this data in meaningful ways and provide business\\ninsights not possible before.\\nBusinesses are gathering data at exponential rates, and there is an equally growing\\nneed for people who know how to analyze and manage it. Stack Overflow, the most\\nactive programming community in the world, performed a comprehensive survey on\\nits members in 2015. Apple coding was the most in-demand technology and had an\\naverage salary nearing six figures. But SQL came in in fifth place, with a salary that\\nwas not far behind. In recent years, data has suddenly become ubiquitous—yet few\\npeople know how to access it meaningfully, which has put SQL talent in high\\ndemand.\\n1'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 17, 'page_label': '2', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Who Is SQL For?\\nOne misperception about SQL is that it is an IT skill and therefore only applicable to\\ntechnology (not business) professionals. In the world as it exists today, this is hardly\\nthe truth. Businesspeople, managers, IT professionals, and engineers can all reap ben‐\\nefits from learning SQL to better position their careers. SQL can open many career\\npaths because it enables individuals to know their businesses better through the data\\nthat is driving them. On the business side, interest in SQL can lead to roles that are\\nanalytical, managerial, strategic, and research- or project-based. On the IT front, it\\ncan lead to roles in database design, database administration, systems engineering, IT\\nproject management, and even software development.\\n2 | Chapter 1: Why Learn SQL?'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 18, 'page_label': '3', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 2\\nDatabases\\nWhat Is a Database?\\nIn the broadest definition, a database is anything that collects and organizes data. A\\nspreadsheet holding customer bookings is a database, and so is a plain-text file con‐\\ntaining flight schedule data. Plain-text data itself can be stored in a variety of formats,\\nincluding XML and CSV .\\nProfessionally, however, when one refers to a “database” they likely are referring to a\\nrelational database management system  (RDBMS). This term may sound technical\\nand intimidating, but an RDBMS is simply a type of database that holds one or more\\ntables that may have relationships to each other.\\nExploring Relational Databases\\nA table should be a familiar concept. It has columns and rows to store data, much like\\na spreadsheet. These tables can have relationships to each other, such as an ORDER\\ntable that refers to a CUSTOMER table for customer information.\\nFor example, suppose we have an ORDER table with a field called CUSTOMER_ID\\n(Figure 2-1).\\nFigure 2-1. An ORDER table with a CUSTOMER_ID\\n3'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 19, 'page_label': '4', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='We can reasonably expect there to be another table, maybe called CUSTOMER\\n(Figure 2-2), which holds the customer information for each CUSTOMER_ID.\\nFigure 2-2. A CUSTOMER table\\nWhen we go through the ORDER table, we can use the CUSTOMER_ID to look up the cus‐\\ntomer information in the CUSTOMER table. This is the fundamental idea behind a “rela‐\\ntional database, ” where tables may have fields that point to information in other\\ntables. This concept may sound familiar if you’ve used VLOOKUP in Excel to retrieve\\ninformation in one sheet from another sheet in a workbook.\\nWhy Separate Tables?\\nBut why are these tables separated and designed this way? The motivation is normal‐\\nization, which is separating the different types of data into their own tables rather\\nthan putting them in one table. If we had all information in a single table, it would be\\nredundant, bloated, and very difficult to maintain. Imagine if we stored customer\\ninformation in the ORDER table. Figure 2-3 shows what it would look like.\\nFigure 2-3. A table that is not normalized\\nNotice that for the Re-Barre Construction orders someone had to populate the cus‐\\ntomer information three times for all three orders (the name, region, street address,\\ncity, state, and zip). This is very redundant, takes up unnecessary storage space, and is\\ndifficult to maintain. Imagine if a customer had an address change and you had to\\nupdate all the orders to reflect that. This is why it is better to separate CUSTOMERS and\\nORDERS into two separate tables. If you need to change a customer’s address, you only\\nneed to change one record in the CUSTOMER table (Figure 2-4).\\n4 | Chapter 2: Databases'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 20, 'page_label': '5', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 2-4. A normalized table\\nWe will explore table relationships again in Chapter 8, and learn how to use the JOIN\\noperator to merge tables in a query so the customer information can be viewed along‐\\nside the order.\\nChoosing a Database Solution\\nRelational databases and SQL are not proprietary. However, there are several compa‐\\nnies and communities that have developed their own relational database software, all\\nof which use tables and leverage SQL. Some database solutions are lightweight and\\nsimple, storing data in a single file accessible to a small number of users. Other data‐\\nbase solutions are massive and run on a server, supporting thousands of users and\\napplications simultaneously. Some database solutions are free and open source, while\\nothers require commercial licenses.\\nFor the sake of practicality, we will divide database solutions into two categories: light‐\\nweight and centralized. These are not necessarily the industry vernacular, but they will\\nhelp clarify the distinction.\\nLightweight Databases\\nIf you are seeking a simple solution for one user or a small number of users (e.g., your\\ncoworkers), a lightweight database is a good place to start. Lightweight databases have\\nlittle to no overhead, meaning they have no servers and are very nimble. Databases\\nare typically stored in a file you can share with others, although it starts to break\\ndown when multiple people make edits to the file simultaneously. When you run into\\nthis problem, you may want to consider migrating to a centralized database.\\nThe two most common lightweight databases are SQLite and Microsoft Access.\\nSQLite is what we will use in this book. It is free, lightweight, and intuitive to use. It is\\nused in most of the devices we touch and can be found in smartphones, satellites, air‐\\ncraft, and car systems. It has virtually no size limitation and is ideal for environments\\nwhere it is not used by more than one person (or at most a few people). Among many\\nother uses, SQLite is ideal to learn SQL due to its ease of installation and simplicity.\\nMicrosoft Access has been around for a while and is inferior to SQLite in terms of\\nscalability and performance. But it is heavily used in business environments and\\nChoosing a Database Solution | 5'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 21, 'page_label': '6', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='worth being familiar with. It has many visual tools for writing queries without using\\nSQL, as well as visual form designers and macro abilities. There are many jobs avail‐\\nable to take ownership of Microsoft Access databases and maintain them, as well as\\nmigrating them to better database platforms such as MySQL.\\nCentralized Databases\\nIf you expect tens, hundreds, or thousands of users and applications to use a database\\nsimultaneously, lightweight databases are not going to cut it. Y ou need a centralized\\ndatabase that runs on a server and handles a high volume of traffic efficiently. There\\nis a wide array of centralized database solutions to choose from, including the follow‐\\ning:\\n• MySQL\\n• Microsoft SQL Server\\n• Oracle\\n• PostgreSQL\\n• Teradata\\n• IBM DB2\\n• MariaDB\\nY ou can install some of these solutions on any computer and turn that computer into\\na server. Y ou can then connect users’ computers (also known as clients) to the server\\nso they can access the data. The client can send a SQL statement requesting specific\\ndata, and the server processes the request and returns the answer. This is a classic\\nclient–server setup.  The client requests something, and the server gives it.\\nWhile you can turn any MacBook or cheap PC into a MySQL server, larger traffic vol‐\\numes require more specialized computers (called server computers ) optimized for\\nserver tasks. These are typically maintained by an IT department whose members\\nadministrate and control databases formally deemed critical to the business.\\nDo not be confused by the term “SQL ” being used to brand data‐\\nbase platforms such as MySQL, Microsoft SQL Server, and SQLite.\\nSQL is the universal language to work with data on all these plat‐\\nforms. They merely used “SQL ” in their names for marketing.\\nAs you enter a workplace, chances are an existing centralized database might exist\\nwith information you need, and you will need to request access to it. While we will\\nnot be covering centralized databases in this book, the experience between different\\ndatabase solutions should largely be the same. Across all database solutions, you use\\n6 | Chapter 2: Databases'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 22, 'page_label': '7', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SQL to interact with tables in a pretty uniform way, and even the SQL editor tools are\\nsomewhat similar. Each solution may have nuances to its implementation of SQL,\\nsuch as date functionalities, but everything in this book should be universally applica‐\\nble.\\nIf you ever do need to create a centralized database solution, I would highly recom‐\\nmend MySQL. It is open source, free to use, and straightforward to install and set up.\\nIt is used by Facebook, Google, eBay, Twitter, and hundreds of other Silicon Valley\\ncompanies.\\nWith a conceptual understanding of databases, we can now start working with them.\\nAlthough we will use SQLite in this book, keep in mind it uses SQL, so the knowledge\\nyou gain is applicable to all database platforms.\\nChoosing a Database Solution | 7'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 23, 'page_label': '8', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 24, 'page_label': '9', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 3\\nSQLite\\nWhat Is SQLite?\\nAs discussed in the previous chapter, there are many places to put data. But often‐\\ntimes we want a quick, easy place to put data without all the hassle of a client–server\\nsetup. We want to store data in a simple file and edit it just as easily as a Word docu‐\\nment. This is an optimal situation to use SQLite.\\nSQLite is the most widely distributed database in the world. It is put on iPhones,\\niPads, Android devices, Windows phones, thermostats, car consoles, satellites, and\\nmany other modern devices that need to store and retrieve data easily. It is used heav‐\\nily in the Windows 10 operating system as well as the Airbus A350 XWB aircraft. It\\nexcels where simplicity and low overhead is needed. It is also great for prototyping\\nbusiness databases.\\nBut every technology has a trade-off. Because it has no server managing access to it, it\\nfails in multiuser environments where multiple people can simultaneously edit the\\nSQLite file. Still, for our training purposes, SQLite is perfect.\\nSQLiteStudio\\nThere are many SQL editors you can use to work with a SQLite database. I strongly\\nrecommend using SQLiteStudio, as it is intuitive and makes it easy to explore and\\nmanage a database. We are going to use that application in this book. Y ou can down‐\\nload it at http://sqlitestudio.pl/?act=download. Be sure to choose Windows, Mac, or\\nLinux for your respective OS. Then open the downloaded folder and copy it to a loca‐\\ntion of your choice. No installation is needed. To start SQLiteStudio, double-click\\nSQLiteStudio.exe (Figure 3-1). Y ou can also create a shortcut on your desktop so you\\ncan easily launch the application in the future.\\n9'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 25, 'page_label': '10', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-1. The SQLiteStudio folder\\nNote that SQLiteStudio is an independent, third-party program not associated with\\nSQLite or its developers. SQLite is a database engine built by Richard Hipp and a tal‐\\nented team of programmers. SQLiteStudio merely takes this engine and wraps a nice\\nuser interface around it. Therefore, if you ever have issues with SQLiteStudio, you\\nshould contact the SQLiteStudio team, not the SQLite team.\\nImporting and Navigating Databases\\nWhen you first start SQLiteStudio, you will probably see a dashboard with no content\\n(Figure 3-2).  The left pane is the database navigator, and the gray area on the right is\\nthe SQL work area where you will write SQL against the databases.\\n10 | Chapter 3: SQLite'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 26, 'page_label': '11', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-2. The SQLiteStudio dashboard\\nLet’s get some databases into SQLiteStudio. Some SQLite database samples used in\\nthis book are provided at http://bit.ly/1TLw1Gr.\\nDownload the databases by clicking the Download ZIP button and copy the contents\\nto a folder of your choice. Y ou will probably want to dedicate this folder to all the\\ndatabases you will work with in this book.\\nAfter downloading the databases, navigate in the top menu to Database → Add a\\nDatabase (Figure 3-3).\\nImporting and Navigating Databases | 11'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 27, 'page_label': '12', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-3. Adding a database\\nY ou will come to a dialog box prompting for a database file. Click the yellow folder\\nicon to select a database file and import it (Figure 3-4).\\n12 | Chapter 3: SQLite'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 28, 'page_label': '13', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-4. Opening a database\\nBrowse for the folder with the saved databases, and double-click the rexon_metals.db\\ndatabase file to load it into SQLiteStudio (Figure 3-5).\\nFigure 3-5. Browsing and opening database files\\nImporting and Navigating Databases | 13'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 29, 'page_label': '14', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Y ou will now see rexon_metals added to the database navigator (Figure 3-6). Double-\\nclick on it to see its contents, which include three tables and two views. Take some\\ntime to poke around and explore this database in the navigator.\\nFigure 3-6. Navigating a database\\nNotice you can click the arrows to get more detailed information on different data‐\\nbase objects, such as tables ( Figure 3-7 ). For example, clicking the arrow for the\\nCUSTOMER table can reveal information such as the columns it contains.\\n14 | Chapter 3: SQLite'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 30, 'page_label': '15', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-7. Expanding a table to see columns\\nY ou may be wondering what “views” are. Do not worry about them for now. They are\\nbasically prebuilt SQL queries that are used so frequently, they are conveniently\\nstored in the database.\\nIf you double-click the CUSTOMER table itself, a new window will pop out in the work\\narea holding all kinds of information about the table (Figure 3-8). It initially opens on\\nthe Structure tab, which provides detailed information about each column. At the\\nmoment, the only detail you need to be concerned with is the data type for each col‐\\numn.\\nImporting and Navigating Databases | 15'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 31, 'page_label': '16', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-8. Each column in a table has a data type, such as integer or text\\nThe CUSTOMER_ID and ZIP fields are stored as INTEGER, which is the data type for a\\nwhole (nondecimal) number. This means these fields should only hold INTEGER val‐\\nues. The rest of the columns are stored as TEXT. There are other data types that could\\nbe used, such as DATETIME, BOOLEAN (true/false), and DECIMAL, which are not used in\\nthis particular table.\\nFor now, if you understand the concept of data types, then that is all you need to\\nobserve in the Structure tab. We will explore table design in detail when we create our\\nown tables later.\\nClick the Data tab, and you will actually see the data in the table itself ( Figure 3-9).\\nThere are only five records (or rows) in this table, but SQLite could hold millions if it\\nneeded to. Y ou can also conveniently edit the values in this table (without using SQL)\\nby simply double-clicking and editing a cell, and then clicking the green checkmark\\nto save it.\\n16 | Chapter 3: SQLite'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 32, 'page_label': '17', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-9. The CUSTOMER table\\nTake some time and get familiar with SQLiteStudio. As soon as you are satisfied that\\nyou’ve poked around enough, close all the windows in the work area. Then, in the top\\nmenu, navigate to Tools→Open SQL Editor. While we’ve discovered that SQLiteStu‐\\ndio provides many ways to view and manipulate data without using any SQL, it does\\nnot come close to the flexibility and power that SQL offers.\\nNow that we know our tables and what we are working with, writing SQL will be\\nsomewhat more intuitive. It is difficult to query databases without knowing the tables\\nin them first.\\nImporting and Navigating Databases | 17'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 33, 'page_label': '18', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 34, 'page_label': '19', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 4\\nSELECT\\nWhen working with databases and SQL, the most common task is to request data\\nfrom one or more tables and display it. The SELECT statement accomplishes this. But\\nthe SELECT can do far more than simply retrieve and display data. As we will learn in\\ncoming chapters, we can transform this data in meaningful ways and build powerful\\nsummaries from millions of records.\\nBut first, we will learn how to SELECT columns from a single table as well as compose\\nexpressions in them.\\nRetrieving Data with SQL\\nIf you have not done so already, click on Tools→Open SQL Editor in the top menu,\\nand make sure the rexon_metals database is open, as mentioned in the previous\\nchapter. Y our SQLiteStudio workspace should look something like Figure 4-1. Notice\\nthat the SQL workspace is now divided into two panes, a SQL Editor pane and a \\nQuery Results pane.\\n19'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 35, 'page_label': '20', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 4-1. The SQL workspace\\nThe SQL Editor pane is where you will write your SQL, and the Query Results pane\\nwill display the results of your SQL.\\nLet’s write our first SQL statement. The most common SQL operation is a SELECT\\nstatement, which pulls data from a table and then displays the results. Click on the\\nSQL Editor pane and write the following statement:\\nSELECT * FROM CUSTOMER;\\nClick the blue triangle button or hit F9 to execute the SQL.\\nY ou just ran your first query, and the results should be displayed in the bottom pane\\n(Figure 4-2).\\n20 | Chapter 4: SELECT'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 36, 'page_label': '21', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 4-2. Running a SELECT query\\nLet’s break down exactly what happened. A SELECT statement allows you to choose\\nwhich columns to pull from a table. So the first part of the SQL shown here should be\\nread as “Select all columns, ” where * is a placeholder to specify all columns:\\nSELECT * FROM CUSTOMER;\\nAnd you are getting these columns from the CUSTOMER table:\\nSELECT * FROM CUSTOMER;\\nWhen you execute this SELECT statement, it brings back all the columns from the\\nCUSTOMER table and displays them to you (Figure 4-3).\\nRetrieving Data with SQL | 21'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 37, 'page_label': '22', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 4-3. Selecting all records from the CUSTOMER table\\nY ou do not have to pull all columns in a SELECT statement. Y ou can also pick and\\nchoose only the columns you are interested in. The following query will only pull the\\nCUSTOMER_ID and NAME columns:\\nSELECT CUSTOMER_ID, NAME FROM CUSTOMER;\\nAnd the output will only display those two columns (Figure 4-4).\\nFigure 4-4. Selecting only two columns from a table\\nA single SQL statement can optionally end with a semicolon ( ;), as\\nshown in the previous examples. However, the semicolon is neces‐\\nsary to run multiple SQL statements at once, which is helpful when\\nwriting data, as covered in Chapter 10.\\nBeing able to pick and choose columns may not seem interesting at the moment, but\\nit allows us to hone in on what we are interested in. Reducing scope to just certain\\ncolumns will assist with GROUP BY aggregation tasks as well, as we’ll see in Chapter 6.\\n22 | Chapter 4: SELECT'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 38, 'page_label': '23', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Expressions in SELECT Statements\\nThe SELECT statement can do far more than simply select columns. Y ou can also do\\ncalculations on one or more columns and include them in your query result.\\nLet’s work with another table called PRODUCT. First, do a SELECT all to see the data\\n(Figure 4-5):\\nSELECT * FROM PRODUCT;\\nFigure 4-5. The PRODUCT table\\nSuppose we wanted to generate a calculated column called TAXED_PRICE that is 7%\\nhigher than PRICE. We could use a SELECT query to dynamically calculate this for us\\n(Figure 4-6):\\nSELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE,\\nPRICE * 1.07 AS TAXED_PRICE\\nFROM PRODUCT;\\nExpressions in SELECT Statements | 23'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 39, 'page_label': '24', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 4-6. Using expressions to calculate a TAXED_PRICE column\\nNotice in the SELECT statement that we can spread our SQL across\\nmultiple lines to make it more legible. The software will ignore\\nextraneous whitespace and separate lines, so we can use them to\\nmake our SQL easier to read.\\nNotice how the TAXED_PRICE column was dynamically calculated in the SELECT query.\\nThis column is not stored in the table, but rather calculated and displayed to us every\\ntime we run this query. This is a powerful feature of SQL, which allows us to keep the\\nstored data simple and use queries to layer calculations on top of it.\\nLet’s take a look at our TAXED_PRICE column and break down how it was created. We\\nfirst see the PRICE is multiplied by 1.07 to calculate the taxed amount. We generate\\nthis TAXED_PRICE value for every record:\\nSELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE,\\nPRICE * 1.07 AS TAXED_PRICE\\nFROM PRODUCT\\nNotice too that we gave this calculated value a name using an AS statement (this is\\nknown as an alias):\\n24 | Chapter 4: SELECT'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 40, 'page_label': '25', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE,\\nPRICE * 1.07 AS TAXED_PRICE\\nFROM PRODUCT\\nWe can use aliases to give names to expressions. We can also use aliases to apply a\\nnew name to an existing column within the query. For example, we can alias the\\nPRICE column to UNTAXED_PRICE (Figure 4-7 ). This does not actually change the\\nname of the column in the table, but it gives it a new name within the scope of our\\nSELECT statement:\\nSELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE AS UNTAXED_PRICE,\\nPRICE * 1.07 AS TAXED_PRICE\\nFROM PRODUCT\\nFigure 4-7. Aliasing the PRICE column to UNTAXED_PRICE\\nWhen giving names to anything in SQL (whether it is an alias, a\\ncolumn name, a table name, or any other entity), always use an\\nunderscore (_) as a placeholder for spaces. Y ou will run into errors\\notherwise.\\nIf we were to distribute the results of this SQL statement as a report to our workplace,\\nwe would probably want to touch up the rounding on the TAXED_PRICE. Having more\\nthan two decimal places may not be desirable. Every database platform has built-in\\nExpressions in SELECT Statements | 25'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 41, 'page_label': '26', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='functions to assist with these kinds of operations, and SQLite provides a round()\\nfunction that accepts two arguments in parentheses separated by a comma: the num‐\\nber to be rounded, and the number of decimal places to round to. To round the\\nTAXED_PRICE to two decimal places, we can pass the multiplication expression PRICE\\n* 1.07 as the first argument, and a 2 as the second:\\nSELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE,\\nround(PRICE * 1.07, 2) AS TAXED_PRICE\\nFROM PRODUCT;\\nRun the statement and you will notice it rounds the TAXED_PRICE, which displays\\nmuch more nicely with two decimal places (Figure 4-8).\\nFigure 4-8. Using the round() function to limit decimal places for TAXED_PRICE\\nHere is a short summary of the mathematical operators you  can use in SQL (we will\\nsee these used throughout the book):\\nOperator Description Example\\n+ Adds two numbers STOCK + NEW_SHIPMENT\\n- Subtracts two numbers STOCK - DEFECTS\\n* Multiplies two numbers PRICE * 1.07\\n/ Divides two numbers STOCK / PALLET_SIZE\\n% Divides two numbers, but returns the remainder STOCK % PALLET_SIZE\\n26 | Chapter 4: SELECT'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 42, 'page_label': '27', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Text Concatenation\\nExpressions do not have to work only with numbers. Y ou can also use expressions\\nwith text and other data types. A helpful operator to use with text is concatenation,\\nwhich merges two or more pieces of data together.    The concatenate operator is\\nspecified by a double pipe ( ||), and you put the data values to concatenate on both\\nsides of it.\\nFor instance, you can concatenate the CITY and STATE fields from the CUSTOMER table\\nas well as put a comma and space between them to create a LOCATION value\\n(Figure 4-9):\\nSELECT NAME,\\nCITY || ', ' || STATE AS LOCATION\\nFROM CUSTOMER;\\nFigure 4-9. Concatenating CITY and STATE\\nY ou can even concatenate several fields into a single SHIP_ADDRESS value\\n(Figure 4-10):\\nSELECT NAME,\\nSTREET_ADDRESS || ' ' || CITY || ', ' || STATE || ' ' || ZIP AS SHIP_ADDRESS\\nFROM CUSTOMER;\\nFigure 4-10. Concatenating several fields to create a SHIP_ADDRESS\\nText Concatenation | 27\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 43, 'page_label': '28', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Concatenation should work with any data type (numbers, dates, etc.) and treat it as\\ntext when merging. The ZIP field shown here is a number, but it was implicitly con‐\\nverted to text during concatenation.\\nMore text operations will be covered in the next chapter, but concatenation is defi‐\\nnitely an important one.\\nMany database platforms use double pipes ( ||) to concatenate, but\\nMySQL and some others require using a CONCAT() function.\\nSummary\\nIn this chapter, we covered how to use the SELECT statement, the most common SQL\\noperation. It retrieves and transforms data from a table without affecting the table\\nitself. We also learned how to select columns and write expressions. Within expres‐\\nsions, we can use operators and functions to do tasks such as rounding, math, and\\nconcatenation.\\nIn the next chapter, we will learn about the WHERE statement, which will allow us to\\nfilter records based on criteria we specify.\\n28 | Chapter 4: SELECT'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 44, 'page_label': '29', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 5\\nWHERE\\nOver the next few chapters, we will be adding more functionalities to the SELECT\\nstatement. A very common task when working with data is filtering for records based\\non criteria, which can be done with a WHERE statement.\\nWe will be learning more functions and using them in the WHERE clause, but we can\\nalso use them in SELECT statements, as discussed in the previous chapter. For the most\\npart, expressions and functions can be used in any part of a SQL statement.\\nFiltering Records\\nWe are going to open another database called weather_stations. Add this database\\nto your database navigator (refer to Chapter 3  if you’ve forgotten how to do this).\\nDouble-click on the database and you will see there is a single table called STA\\nTION_DATA. This contains weather-related sample data gathered from various weather\\nstations.\\nExecute a SELECT on all columns to see the data inside:\\nSELECT * FROM station_data;\\nThere is a lot of data here: about 28,000 records ( Figure 5-1 ). We are not going to\\nglean a lot of interesting information by scrolling through these records one by one.\\nWe will need to learn some more SQL features to morph this data into something\\nmeaningful. We will start by learning the WHERE statement, which we can use to filter\\ndown records based on a criterion.\\n29'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 45, 'page_label': '30', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 5-1. The weather_stations database\\nTable names and columns can be defined in uppercase or lower‐\\ncase. SQL commands such as SELECT, FROM, and WHERE can be\\nuppercase or lowercase as well.\\nUsing WHERE on Numbers\\nLet’s say we are interested in station_data records for only the year 2010. Using a\\nWHERE is pretty straightforward for a simple criterion like this.  With this query, you\\nshould only get back records where the year field equals 2010 (Figure 5-2):\\nSELECT * FROM station_data\\nWHERE year = 2010;\\n30 | Chapter 5: WHERE'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 46, 'page_label': '31', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 5-2. Records for the year 2010\\nConversely, you can use != or <> to get everything but 2010. For example:\\nSELECT * FROM station_data\\nWHERE year != 2010\\nOr:\\nSELECT * FROM station_data\\nWHERE year <> 2010\\nThese two syntaxes do the same thing. SQLite and most platforms now support both.\\nHowever, Microsoft Access and IBM DB2 only support <>.\\nWe can also qualify inclusive ranges using a BETWEEN statement, as shown here\\n(“inclusive” means that 2005 and 2010 are included in the range):\\nSELECT * FROM station_data\\nWHERE year BETWEEN 2005 and 2010\\nAND, OR, and IN Statements\\nA BETWEEN can alternatively be expressed using greater than or equal to and less than\\nor equal to expressions and an AND statement. It is a little more verbose, but it demon‐\\nstrates we can use two conditions with an AND. In this case, the year must be greater\\nthan or equal to 2005 and less than or equal to 2010:\\nSELECT * FROM station_data\\nWHERE year >= 2005 AND year <= 2010\\nIf we wanted everything between 2005 and 2010 exclusively—i.e., not including those\\ntwo years—we would just get rid of the = characters. Only 2006, 2007, 2008, and 2009\\nwould then qualify:\\nAND, OR, and IN Statements | 31'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 47, 'page_label': '32', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT * FROM station_data\\nWHERE year > 2005 AND year < 2010\\nWe also have the option of using OR. In an OR statement, at least one of the criteria\\nmust be true for the record to qualify. If we wanted only records with months 3, 6, 9,\\nor 12, we could use an OR to accomplish that:\\nSELECT * FROM station_data\\nWHERE MONTH = 3\\nOR MONTH = 6\\nOR MONTH = 9\\nOR MONTH = 12\\nThis is a little verbose. A more efficient way to accomplish this is using an IN state‐\\nment to provide valid list of values:\\nSELECT * FROM station_data\\nWHERE MONTH IN (3,6,9,12)\\nIf we wanted everything except 3, 6, 9, and 12, we could use NOT IN:\\nSELECT * FROM station_data\\nWHERE MONTH NOT IN (3,6,9,12)\\nY ou can use other math expressions in your WHERE statements too. Earlier, we were\\ntrying to filter on months 3, 6, 9, and 12. If you wanted only months divisible by 3\\n(the “quarter” months), you could use the modulus operator (%). The modulus is simi‐\\nlar to the division operator (/), but it returns the remainder instead of the quotient. A\\nremainder of 0 means there is no remainder at all, so you can leverage this logic by\\nlooking for a remainder of 0 with modulus 3.\\nIn English, this means “give me all months where dividing by 3 gives me a remainder\\nof 0”:\\nSELECT * FROM station_data\\nWHERE MONTH % 3 = 0\\nOracle does not support the modulus operator. It instead uses the\\nMOD() function.\\nUsing WHERE on Text\\nWe’ve covered several examples of how to qualify number fields in WHERE statements. \\nThe rules for qualifying text fields follow the same structure, although there are sub‐\\ntle differences. Y ou can use =, AND, OR, and IN statements with text. However, when\\nusing text, you must wrap literals (or text values you specify) in single quotes. For\\nexample, if you wanted to filter for a specific report_code, you could run this query:\\n32 | Chapter 5: WHERE'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 48, 'page_label': '33', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"SELECT * FROM station_data\\nWHERE report_code = '513A63'\\nNotice that because the report_code field is text (not a number), we need to put sin‐\\ngle quotes around '513A63' to qualify it. If we do not do this, the SQL software will\\nget confused and think 513A63 is a column rather than a text value. This will cause an\\nerror and the query will fail.\\nThis single-quote rule applies to all text operations, including this IN operation:\\nSELECT * FROM station_data\\nWHERE report_code IN ('513A63','1F8A7B','EF616A')\\nThere are other helpful text operations and functions you can use in WHERE and\\nSELECT statements. For example, the length() function will count the number of\\ncharacters for a given value. So, if we were assigned quality control and needed to\\nensure every report_code was six characters in length, we would want to make sure\\nno records come back when running this query:\\nSELECT * FROM station_data\\nWHERE length(report_code) != 6\\nAnother common operation is to use wildcards with a LIKE expression, where % is\\nany number of characters and _ is any single character. Any other character is inter‐\\npreted literally. So, if you wanted to find all report codes that start with the letter “ A, ”\\nyou would run this statement to find “ A ” followed by any characters:\\nSELECT * FROM station_data\\nWHERE report_code LIKE 'A%'\\nIf you wanted to find all report codes that have a “B” as the first character and a “C”\\nas the third character, you would specify an underscore ( _) for the second position,\\nand follow with any number of characters after the “C”:\\nSELECT * FROM station_data\\nWHERE report_code LIKE 'B_C%'\\nDo not be confused by the % being used for two different purposes.\\nEarlier we used it to perform a modulus operation, but in a LIKE\\nstatement it is a wildcard in a text pattern. Like some other symbols\\nand characters in SQL, the context in which it is used defines its\\nfunctionality.\\nThere are many other handy text functions, such as INSTR, SUBSTR, and REPLACE. In\\nthe interest of brevity, we will stop covering text functions here, but you can refer to\\n“ APPENDIX A6 – Common Core Functions” on page 103 and “ APPENDIX A8 –\\nDate and Time Functions” on page 104 for more coverage on these functions.\\nUsing WHERE on Text | 33\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 49, 'page_label': '34', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Text functions such as LIKE, SUBSTR, and INSTR can start to become\\ntedious and verbose when qualifying complex text patterns. I\\nhighly recommend researching regular expressions when you find\\nyourself experiencing this. They are not beginner material, but they\\nare handy once you hit that intermediate need.\\nUsing WHERE on Booleans\\nBooleans are true/false values. In the database world, typically false is expressed as 0\\nand true is expressed as 1. Some database platforms (like MySQL) allow you to\\nimplicitly use the words true and false to qualify, as shown here:\\nSELECT * FROM station_data\\nWHERE tornado = true AND hail = true;\\nSQLite, however, does not support this. It expects you to explicitly use 1 for true and\\n0 for false. If you wanted all records where there was tornado and hail, you would run\\nthis statement:\\nSELECT * FROM station_data\\nWHERE tornado = 1 AND hail = 1;\\nIf you are looking for just true values, you do not even have to use the = 1 expression.\\nBecause the fields are already Boolean (behind the scenes, every WHERE condition\\nboils down to a Boolean expression), they inherently qualify by themselves. Hence,\\nyou can achieve the same results by running the following query:\\nSELECT * FROM station_data\\nWHERE tornado AND hail;\\nHowever, qualifying for false conditions needs to be explicit. To get all records with\\nno tornado but with hail, run this query:\\nSELECT * FROM station_data\\nWHERE tornado = 0 AND hail = 1;\\nY ou can also use the NOT keyword to qualify tornado as false:\\nSELECT * FROM station_data\\nWHERE NOT tornado AND hail;\\nHandling NULL\\nY ou may have noticed that some columns, such as station_pressure and\\nsnow_depth, have null values (Figure 5-3). A null is a value that has no value. It is the\\ncomplete absence of any content. It is a vacuous state. In layman’s terms, it is blank.\\n34 | Chapter 5: WHERE'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 50, 'page_label': '35', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Figure 5-3. The station_data table has NULL values\\nNull values cannot be determined with an =. Y ou need to use the IS NULL or IS NOT\\nNULL statements to identify null values. So, to get all records with no recorded\\nsnow_depth, you could run this query:\\nSELECT * FROM station_data\\nWHERE snow_depth IS NULL;\\nOften, null values are not desirable to have. The station_number column should be\\ndesigned so it never allows nulls, or else we could have orphan data that belongs to no\\nstation. It might make sense to have null values for snow_depth or precipitation,\\nthough, not because it was a sunny day (in this case, it is better to record the values as\\n0), but rather because some stations might not have the necessary instruments to take\\nthose measurements. It might be misleading to set those values to 0 (which implies\\ndata was recorded), so those measurements should be left null.\\nThis shows that nulls can be ambiguous and it can be difficult to determine their\\nbusiness meaning. It is important that nullable columns (columns that are allowed to\\nhave null values) have documented what a null value means from a business perspec‐\\ntive. Otherwise, nulls should be banned from those table columns.\\nDo not confuse nulls with empty text, which is two single quotes\\nwith nothing in them (i.e., ''). This also applies to whitespace text\\n(i.e., ' '). These will be treated as values and never will be consid‐\\nered null. A null is definitely not the same as 0 either, because 0 is a\\nvalue, whereas null is an absence of a value.\\nNulls can be very annoying to handle when composing WHERE statements. If you\\nwanted to query all records where precipitation is less than 0.5, you could write\\nthis statement:\\nSELECT * FROM station_data\\nWHERE precipitation <= 0.5;\\nHandling NULL | 35\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 51, 'page_label': '36', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='But have you considered the null values? What if for this query you wanted nulls to\\nbe included? Because null is not 0 or any number, it will not qualify. Nulls rarely qual‐\\nify with anything and almost always get filtered out in a WHERE unless you explicitly\\nhandle them. So you have to use an OR to include nulls:\\nSELECT * FROM station_data\\nWHERE precipitation IS NULL OR precipitation <= 0.5;\\nA more elegant way of handling null values is to use the coalesce() function, which\\nwill turn a possibly null value into a specified default value if it is null. Otherwise, it\\nwill leave the value as is. The first argument is the possibly null value, and the second\\nis the value to use if it is null. So if we wanted all nulls to be treated as 0 within our\\ncondition, we could coalesce() the precipitation field to convert null to 0:\\nSELECT * FROM station_data\\nWHERE coalesce(precipitation, 0) <= 0.5;\\nLike any function, a coalesce() can be used in the SELECT statement too, and not just\\nthe WHERE. This is helpful if you want to pretty up a report and not have null values\\ndisplayed, but rather have some placeholder—for example, 0, “N/A ” or “None”—\\nwhich is more meaningful to most people:\\nSELECT report_code, coalesce(precipitation, 0) as rainfall\\nFROM station_data;\\nGrouping Conditions\\nWhen you start chaining AND and OR together, it is good to group them deliberately.\\nY ou need to make sure that you organize each set of conditions between each OR in a\\nway that groups related conditions. Say you were looking for sleet or snow condi‐\\ntions. For sleet to happen, there must be rain and a temperature less than or equal to\\n32 degrees Fahrenheit. Y ou can test for that sleet condition or a snow depth greater\\nthan 0, as shown here:\\nSELECT * FROM station_data\\nWHERE rain = 1 AND temperature <= 32\\nOR snow_depth > 0;\\nBut there is one possible problem here. While this technically works, there is a degree\\nof ambiguity that we were lucky SQLite interpreted correctly. The reason is due to the\\nunclear question of “What conditions belong to the AND and what conditions belong\\nto the OR?” The SQL interpreter could derail quickly and incorrectly interpret that we\\nare looking for rain AND another condition where either the temperature is below 32\\nOR the snow depth is greater than 0. The semantics are not clear, and in more compli‐\\ncated SQL this could confuse not only people but also the machine.\\nThis is why it is better to explicitly group conditions in parentheses:\\n36 | Chapter 5: WHERE'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 52, 'page_label': '37', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT * FROM station_data\\nWHERE (rain = 1 AND temperature <= 32)\\nOR snow_depth > 0\\nHere, we group up the sleet expression within parentheses so it is calculated as a sin‐\\ngle unit, and temperature is not mixed up with the OR operator and accidentally\\nmangled with the snow_depth. Grouping with parentheses in WHERE statements not\\nonly makes the semantics clearer, but also the execution safer. This is much like the\\norder of operations (PEMDAS) you probably remember from your middle school\\nmath days. Anything in parentheses gets calculated first. When you start writing\\ncomplicated WHERE conditions, this practice becomes even more critical.\\nSummary\\nIn this chapter, we learned to effectively filter out records in a SELECT statement using\\na WHERE clause. We also leveraged new functions and expression operators that can be\\nused in almost any part of the SQL statement. Finally, we covered how to deliberately\\nand safely chain multiple conditions together in a single WHERE statement.\\nHopefully SQL is already proving to be useful. Y ou can quickly and easily filter data\\non very explicit conditions, in a way that is difficult to achieve in everyday tools like\\nExcel.\\nDespite all we have covered, we have only just gotten started. The next chapter will\\ncover aggregating data, which will add even more value to your SQL repertoire. It is\\none thing to narrow down your records and filter on specific criteria. It is another to\\ncrunch down millions of records into a few that summarize the data.\\nWe covered a few functions in this chapter, but there are dozens\\nmore that you can research and use as needed. We will learn a few\\nmore throughout this book. “ APPENDIX A6 – Common Core\\nFunctions” on page 103 and “ APPENDIX A8 – Date and Time\\nFunctions” on page 104 cover several more of these functions, and\\nyou can always see a full list of SQLite functions at https://\\nwww.sqlite.org/lang_corefunc.html.\\nSummary | 37'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 53, 'page_label': '38', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 54, 'page_label': '39', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 6\\nGROUP BY and ORDER BY\\nAggregating data (also referred to as rolling up, summarizing, or grouping data) is\\ncreating some sort of total from a number of records. Sum, min, max, count, and\\naverage are common aggregate operations. In SQL you can group these totals on any\\nspecified columns, allowing you to control the scope of these aggregations easily.\\nGrouping Records\\nFirst, perform the simplest aggregation: count the number of records in a table. Open\\nthe SQL editor and get a count of records for station data:\\nSELECT COUNT(*) AS record_count FROM station_data;\\nThe COUNT(*) means to count the records. We can also use this in combination with\\nother SQL operations, like WHERE. To count the number of records where a tornado\\nwas present, input the following:\\nSELECT COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1;\\nWe identified 3,000 records with tornadoes present. But what if we wanted to separate\\nthe count by year (Figure 6-1)? We can do that too with this query:\\nSELECT year, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year;\\n39'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 55, 'page_label': '40', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 6-1. Getting a tornado count by year\\nThis data suddenly becomes more meaningful. We now see the tornado sighting\\ncount by year. Let’s break down this query to see how this happened.\\nFirst, we select the year, then we select the COUNT(*) from the records, and we filter\\nonly for records where tornado is true:\\nSELECT year, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year;\\nHowever, we also specify that we are grouping on year. This is what effectively allows\\nus to count the number of records by year. The last line, highlighted in bold, performs\\nthis grouping:\\nSELECT year, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year;\\nWe can slice this data on more than one field. If we wanted a count by year and\\nmonth, we could group on the month field as well (Figure 6-2):\\nSELECT year, month, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year, month\\n40 | Chapter 6: GROUP BY and ORDER BY'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 56, 'page_label': '41', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 6-2. Tornado count by year and month\\nAlternatively, we can use ordinal positions instead of specifying the columns in the\\nGROUP BY. The ordinal positions correspond to each item’s numeric position in the\\nSELECT statement. So, instead of writing GROUP BY year, month , we could instead\\nmake it GROUP BY 1, 2  (which is especially helpful if our SELECT has long column\\nnames or expressions, and we do not want to rewrite them in the GROUP BY):\\nSELECT year, month, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY 1, 2\\nNote that not all platforms support ordinal positions. With Oracle and SQL Server,\\nfor example, you will have to rewrite the entire column name or expression in the\\nGROUP BY. \\nOrdering Records\\nNotice that the month column is not in a natural sort we would expect. This is a good\\ntime to  bring up the ORDER BY operator, which you can put at the end of a SQL state‐\\nment (after any WHERE and GROUP BY). If you wanted to sort by year, and then month,\\nyou could just add this command:\\nOrdering Records | 41'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 57, 'page_label': '42', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT year, month, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year, month\\nORDER BY year, month\\nHowever, you are probably more interested in recent data and would like it at the top.\\nBy default, sorting is done with the ASC operator, which orders the data in ascending\\norder. If you want to sort in descending order instead, apply the DESC operator to the\\nordering of year to make more recent records appear at the top of the results:\\nSELECT year, month, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year, month\\nORDER BY year DESC, month\\nAggregate Functions\\nWe already used the COUNT(*) function to count records. But there are other aggrega‐\\ntion functions, including SUM(), MIN(), MAX(), and AVG(). We can use aggregation\\nfunctions on a specific column to perform some sort of calculation on it.\\nBut first let’s look at another way to use COUNT(). The COUNT() function can be used\\nfor a purpose other than simply counting records. If you specify a column instead of\\nan asterisk, it will count the number of non-null values in that column. For instance,\\nwe can take a count of snow_depth recordings, which will count the number of non-\\nnull values (Figure 6-3):\\nSELECT COUNT(snow_depth) as recorded_snow_depth_count\\nFROM STATION_DATA\\nFigure 6-3. Count of non-null snow depth recordings\\nTaking a count of non-null values in a column can be useful, so take note that\\nCOUNT() can fulfill that purpose as well when applied to a column.\\n42 | Chapter 6: GROUP BY and ORDER BY'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 58, 'page_label': '43', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Aggregate functions such as COUNT(), SUM(), AVG(), MIN(), and\\nMAX() will never include null values in their calculations.  Only\\nnon- null values will be considered.\\nLet’s move on to other aggregation tasks. If you wanted to find the average tempera‐\\nture for each month since 2000, you could filter for years 2000 and later, group by\\nmonth, and perform an average on temp (Figure 6-4):\\nSELECT month, AVG(temp) as avg_temp\\nFROM station_data\\nWHERE year >= 2000\\nGROUP BY month\\nFigure 6-4. Average temperature by month since the year 2000\\nAs always, you can use functions on the aggregated values and perform tasks such as\\nrounding to make them look nicer (Figure 6-5):\\nSELECT month, round(AVG(temp),2) as avg_temp\\nFROM station_data\\nWHERE year >= 2000\\nGROUP BY month\\nAggregate Functions | 43'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 59, 'page_label': '44', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 6-5. Rounding the average temperature by month\\nSUM() is another common aggregate operation. To find the sum of snow depth by\\nyear since 2000, run this query:\\nSELECT year, SUM(snow_depth) as total_snow\\nFROM station_data\\nWHERE year >= 2000\\nGROUP BY year\\nThere is no limitation on how many aggregate operations you can use in a single\\nquery. Here we find the total_snow and total_precipitation for each year since\\n2000 in a single query, as well as the max_precipitation:\\nSELECT year,\\nSUM(snow_depth) as total_snow,\\nSUM(precipitation) as total_precipitation,\\nMAX(precipitation) as max_precipitation\\nFROM station_data\\nWHERE year >= 2000\\nGROUP BY year\\nIt may not be apparent yet, but you can achieve some very specific aggregations by\\nleveraging the WHERE. If you wanted the total precipitation by year only when a tor‐\\nnado was present, you would just have to filter on tornado being true. This will only\\ninclude tornado-related precipitation in the totals:\\n44 | Chapter 6: GROUP BY and ORDER BY'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 60, 'page_label': '45', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT year,\\nSUM(precipitation) as tornado_precipitation\\nFROM station_data\\nWHERE tornado = 1\\nGROUP BY year\\nThe HAVING Statement\\nSuppose you wanted to filter out records based on an aggregated value.  While your\\nfirst instinct might be to use a WHERE statement, this actually will not work because the\\nWHERE filters records, and does not filter aggregations. For example, if you try to use a\\nWHERE to filter results where total_precipitation is greater than 30, this will error\\nout:\\nSELECT year,\\nSUM(precipitation) as total_precipitation\\nFROM station_data\\nWHERE total_precipitation > 30\\nGROUP BY year\\nWhy does this not work? Y ou cannot filter on aggregated fields using WHERE. Y ou have\\nto use the HAVING keyword to accomplish this. The way aggregation works is that the\\nsoftware processes record by record, finding which ones it wants to keep based on the\\nWHERE condition. After that, it crunches the records down on the GROUP BY and per‐\\nforms any aggregate functions, such as SUM(). If we wanted to filter on the SUM()\\nvalue, we would need the filter to take place after it is calculated. This is where HAVING\\ncan be applied:\\nSELECT year,\\nSUM(precipitation) as total_precipitation\\nFROM station_data\\nGROUP BY year\\nHAVING total_precipitation > 30\\nHAVING is the aggregated equivalent to WHERE. The WHERE keyword filters individual\\nrecords, but HAVING filters aggregations.\\nNote that some platforms, including Oracle, do not support aliases in the HAVING\\nstatement (just like the GROUP BY). This means you must specify the aggregate func‐\\ntion again in the HAVING statement. If you were running the preceding query on an\\nOracle database, you would have to write it like this:\\nSELECT year,\\nSUM(precipitation) as total_precipitation\\nFROM station_data\\nGROUP BY year\\nHAVING SUM(precipitation) > 30\\nThe HAVING Statement | 45'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 61, 'page_label': '46', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Getting Distinct Records\\nIt is not uncommon to want a set of distinct results from a query. We know there are\\n28,000 records in our station_data table. But suppose we want to get a distinct list\\nof the station_number values? If we run this query, we will get duplicates:\\nSELECT station_number FROM station_data\\nIf we want a distinct list of station numbers without any duplicates, we can use the\\nDISTINCT keyword:\\nSELECT DISTINCT station_number FROM station_data\\nY ou can also get distinct results for more than one column. If you need the distinct\\nstation_number and year sets, just include both of those columns in the SELECT\\nstatement:\\nSELECT DISTINCT station_number, year FROM station_data\\nSummary\\nIn this chapter, we learned how to aggregate and sort data using GROUP BY and ORDER\\nBY. We also leveraged the SUM(), MAX(), MIN(), AVG(), and COUNT() aggregate func‐\\ntions to crunch thousands of records into a few meaningful totaled records. Because\\nwe cannot use WHERE to filter aggregated fields, we used the HAVING keyword to\\naccomplish that. We also leveraged the DISTINCT operator to get distinct results in\\nour queries and eliminate duplicates.\\nI hope by now you see the flexibility SQL offers to quickly develop meaningful\\nreports based on thousands or millions of records. Before moving on, I would recom‐\\nmend experimenting with everything you’ve learned so far and trying out the SELECT,\\nWHERE, and GROUP BY in your queries. Ask yourself business questions such as “Has\\nthe temperature been getting warmer every January for the past 20 years?” or “How\\nmany times has hail been present versus not present during a tornado?” Try to create\\nSQL queries on the weather data to answer these questions.\\nGet comfortable with what you have learned so far, but do not fret about memorizing\\nevery SQL functionality. That will come with time as you repeatedly use and practice\\nSQL. Y ou’ll gain more knowledge in the coming chapters, and it’s OK to refer to Goo‐\\ngle or this guide if you forget how to compose things.\\nY ou can get the short, complete list of SQLite aggregate functions\\nin “ APPENDIX A7 – Aggregate Functions” on page 104 or at\\nhttps://www.sqlite.org/lang_aggfunc.html.\\n46 | Chapter 6: GROUP BY and ORDER BY'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 62, 'page_label': '47', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"CHAPTER 7\\nCASE Statements\\nWe are almost ready to learn the truly defining feature of SQL, the JOIN operator. But\\nbefore we do that, we should spend a short chapter covering a very handy operator\\ncalled CASE. This command allows us to swap a column value for another value based\\non one or more conditions.\\nThe CASE Statement\\nA CASE statement allows us to map one or more conditions to a corresponding value\\nfor each condition. Y ou start a CASE statement with the word CASE and conclude it\\nwith an END. Between those keywords, you specify each condition with a WHEN [condi\\ntion] THEN [value], where the [condition] and the corresponding [value] are\\nsupplied by you. After specifying the condition–value pairs, you can have a catch-all\\nvalue to default to if none of the conditions were met, which is specified in the ELSE.\\nFor example, we could categorize wind_speed into wind_severity categories\\n(Figure 7-1), where any speed greater than 40 is 'HIGH', 30 to 40 is 'MODERATE', and\\nanything less is 'LOW':\\nSELECT report_code, year, month, day, wind_speed,\\nCASE\\n    WHEN wind_speed >= 40 THEN 'HIGH'\\n    WHEN wind_speed >= 30 AND wind_speed < 40 THEN 'MODERATE'\\n    ELSE 'LOW'\\nEND as wind_severity\\nFROM station_data\\n47\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 63, 'page_label': '48', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Figure 7-1. Categorizing wind severity into HIGH, MODERATE, and LOW\\nWe can actually omit the AND wind_speed < 40 condition. Here is why: the machine\\nreads a CASE statement from top to bottom, and the first condition it finds true is the\\none it uses (and it will stop evaluating subsequent conditions). So if we have a record\\nwith a wind_speed of 43, we can be certain it will be evaluated as 'HIGH'. Although it\\nis greater than 30, it will not be assigned 'MODERATE' because it will not get to that\\npoint. Knowing this allows us to create a slightly more efficient query:\\nSELECT report_code, year, month, day, wind_speed,\\nCASE\\n    WHEN wind_speed >= 40 THEN 'HIGH'\\n    WHEN wind_speed >= 30 THEN 'MODERATE'\\n    ELSE 'LOW'\\nEND as wind_severity\\nFROM station_data\\nGrouping CASE Statements\\nWhen you create CASE statements and group them, you can create some very power‐\\nful transformations. Converting values based on one or more conditions before\\naggregating them gives us even more possibilities to slice data in interesting ways.\\nElaborating on our previous example, we can group on year and wind_severity and\\nget a count of records for each one as shown here (also notice we use GROUP BY with\\n48 | Chapter 7: CASE Statements\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 64, 'page_label': '49', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"ordinal positions so we do not have to rewrite the wind_severity case expression in\\nthe GROUP BY):\\nSELECT year,\\nCASE\\n    WHEN wind_speed >= 40 THEN 'HIGH'\\n    WHEN wind_speed >= 30 THEN 'MODERATE'\\n    ELSE 'LOW'\\nEND as wind_severity,\\nCOUNT(*) as record_count\\nFROM station_data\\nGROUP BY 1, 2\\nThe “Zero/Null” CASE Trick\\nY ou can do some clever tricks with the CASE statement. One simple but helpful pat‐\\ntern is the “zero/null” CASE trick. This allows you to apply different “filters” for differ‐\\nent aggregate values, all in a single SELECT query. Y ou could never accomplish this in\\na WHERE because the WHERE applies a filter to everything. But you can use a CASE to\\ncreate a different filter condition for each aggregate value.\\nSay you wanted to aggregate precipitation into two sums, tornado_precipitation\\nand non_tornado_precipitation, and GROUP BY year and month. The logic is pri‐\\nmarily dependent on two fields: precipitation and tornado. But how exactly do you\\ncode this?\\nIf you give it some thought, you will realize you cannot do this with a WHERE state‐\\nment unless you do two separate queries (one for tornado being true and the other\\nfalse):\\nTornado precipitation\\nSELECT year, month,\\nSUM(precipitation) as tornado_precipitation\\nFROM station_data\\nWHERE tornado = 1\\nGROUP BY year, month\\nNon-tornado precipitation\\nSELECT year, month,\\nSUM(precipitation) as non_tornado_precipitation\\nFROM station_data\\nWHERE tornado = 0\\nGROUP BY year, month\\nThe “Zero/Null” CASE Trick | 49\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 65, 'page_label': '50', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='But it is possible to do this in a single query using a CASE statement. Y ou can move the\\ntornado conditions from the WHERE to a CASE, and make the value 0 if the condition is\\nfalse. Then you can SUM those CASE statements (Figure 7-2):\\nSELECT year, month,\\nSUM(CASE WHEN tornado = 1 THEN precipitation ELSE 0 END) as tornado_precipitation,\\nSUM(CASE WHEN tornado = 0 THEN precipitation ELSE 0 END) as \\n    non_tornado_precipitation\\n    \\nFROM station_data\\nGROUP BY year, month\\nFigure 7-2. Getting tornado and non-tornado precipitation by year and month\\nThe CASE statement can do an impressive amount of work, especially in complex\\naggregation tasks. By leveraging a condition to make a value 0 if the condition is not\\nmet, we effectively ignore that value and exclude it from the SUM (because adding 0\\nhas no impact).\\nY ou could also do this with MIN or MAX operations, and use a null instead of 0 to make\\nsure values with certain conditions are never considered. Y ou can find the maximum\\nprecipitation when tornadoes were present and when they were not ( Figure 7-3) as\\nfollows:\\n50 | Chapter 7: CASE Statements'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 66, 'page_label': '51', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT year,\\nMAX(CASE WHEN tornado = 0 THEN precipitation ELSE NULL END) as\\n    max_non_tornado_precipitation,\\nMAX(CASE WHEN tornado = 1 THEN precipitation ELSE NULL END) as\\n    max_tornado_precipitation\\nFROM station_data\\nGROUP BY year\\nFigure 7-3. Maximum tornado and non-tornado precipitations by year\\nJust like with the WHERE statement, you can use any Boolean expressions in a CASE\\nstatement, including functions and AND, OR, and NOT statements. The following query\\nwill find the average temperatures by month when rain/hail was present versus not\\npresent after the year 2000:\\nSELECT month,\\nAVG(CASE WHEN rain OR hail THEN temperature ELSE null END)\\nAS avg_precipitation_temp,\\nAVG(CASE WHEN NOT (rain OR hail) THEN temperature ELSE null END)\\nAS avg_non_precipitation_temp\\nFROM STATION_DATA\\nWHERE year > 2000\\nGROUP BY month\\nThe “Zero/Null” CASE Trick | 51'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 67, 'page_label': '52', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='The zero/null CASE trick is a great use of the CASE statement. It offers many possibili‐\\nties to perform several aggregations with different criteria and therefore is worth\\nknowing.\\nSummary\\nWe dedicated a chapter to learning about CASE statements because they offer a lot of\\nflexibility. We can swap values in a column with another set of values based on condi‐\\ntions we provide. When we aggregate CASE statements, we bring more possibilities to\\nslice data in interesting ways and pack more information into a single query.\\nHopefully by now you have a solid foundation and are ready to learn the defining\\npart of SQL: the JOIN. Take a break. Down a few espressos. After you learn the JOIN,\\nyou can truly call yourself a SQL developer.\\n52 | Chapter 7: CASE Statements'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 68, 'page_label': '53', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 8\\nJOIN\\nStitching Tables Together\\nJoining is the defining functionality of SQL and sets it apart from other data technol‐\\nogies. Be sure you are somewhat comfortable with the material we’ve covered so far,\\nand take your time practicing and reviewing before moving on.\\nLet’s rewind back to the beginning of this book, when we were discussing relational\\ndatabases. Remember how “normalized” databases often have tables with fields that\\npoint to other tables? For example, consider this CUSTOMER_ORDER table, which has a\\nCUSTOMER_ID field (Figure 8-1).\\nFigure 8-1. The CUSTOMER_ORDER table has a CUSTOMER_ID field\\nThis CUSTOMER_ID field gives us a key to look up in the table CUSTOMER. Knowing this,\\nit should be no surprise that the CUSTOMER table also has a CUSTOMER_ID field\\n(Figure 8-2).\\n53'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 69, 'page_label': '54', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-2. The CUSTOMER table has a CUSTOMER_ID key field that can be used to\\nget customer information\\nWe can retrieve customer information for an order from this table, very much like a\\nVLOOKUP in Excel.\\nThis is an example of a relationship between the CUSTOMER_ORDER table and the\\nCUSTOMER table. We can say  that CUSTOMER is a parent to CUSTOMER_ORDER. Because\\nCUSTOMER_ORDER depends on CUSTOMER for information, it is a child of CUSTOMER. Con‐\\nversely, CUSTOMER cannot be a child of CUSTOMER_ORDER because it does not rely on it\\nfor any information.  The diagram in Figure 8-3 shows this relationship; the arrow\\nshows that CUSTOMER supplies customer information to CUSTOMER_ORDER via the\\nCUSTOMER_ID.\\nFigure 8-3. CUSTOMER is the parent to CUSTOMER_ORDER, because\\nCUSTOMER_ORDER depends on it for CUSTOMER information\\nThe other aspect to consider in a relationship is how many records in the child can be\\ntied to a single record of the parent. Take the CUSTOMER and CUSTOMER_ORDER tables\\nand you will  see it is a one-to-many relationship, where a single customer record can\\nline up with multiple orders. Let’s take a look at Figure 8-4 to see a specific example:\\nthe customer “Re-Barre Construction” with CUSTOMER_ID 3 is tied to three orders.\\n54 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 70, 'page_label': '55', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-4. A one-to-many relationship between CUSTOMER and\\nCUSTOMER_ORDER\\nOne-to-many is the most common type of relationship because it accommodates\\nmost business needs, such as a single customer having multiple orders. Business data\\nin a well-designed database should strive for a one-to-many pattern. Less common\\nare the one-to-one and many-to-many relationships (sometimes referred to as a Carte‐\\nsian product). These are worth researching later, but in the interest of focusing the\\nscope of this book, we will steer clear of them.\\nINNER JOIN\\nUnderstanding table relationships, we can consider that it might be nice to stitch two\\ntables together, so we can see CUSTOMER and CUSTOMER_ORDER information alongside\\neach other. Otherwise, we will have to manually perform tons of lookups with\\nCUSTOMER_ID, which can be quite tedious. We can avoid that with JOIN operators, and\\nwe will start by learning the INNER JOIN.\\nThe INNER JOIN allows us to merge two tables together. But if we are going to merge\\ntables, we need to define a commonality between the two so records from both tables\\nline up. We need to define one or more fields they have in common and join on them.\\nIf we are going to query the CUSTOMER_ORDER table and join it to CUSTOMER to bring in\\ncustomer information, we need to define the commonality on CUSTOMER_ID.\\nOpen up the rexon_metals database and open a new SQL editor window. We are\\ngoing to execute our first INNER JOIN:\\nINNER JOIN | 55'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 71, 'page_label': '56', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT ORDER_ID,\\nCUSTOMER.CUSTOMER_ID,\\nORDER_DATE,\\nSHIP_DATE,\\nNAME,\\nSTREET_ADDRESS,\\nCITY,\\nSTATE,\\nZIP,\\nPRODUCT_ID,\\nORDER_QTY\\nFROM CUSTOMER INNER JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nThe first thing you may notice is we were able to query fields from both CUSTOMER\\nand CUSTOMER_ORDER. It is almost like we took those two tables and temporarily\\nmerged them into a single table, which we queried off of. In effect, that is exactly what\\nwe did!\\nLet’s break down how this was accomplished. First, we select the fields we want from\\nthe CUSTOMER and CUSTOMER_ORDER tables:\\nSELECT CUSTOMER.CUSTOMER_ID,\\nNAME,\\nSTREET_ADDRESS,\\nCITY,\\nSTATE,\\nZIP,\\nORDER_DATE,\\nSHIP_DATE,\\nORDER_ID,\\nPRODUCT_ID,\\nORDER_QTY\\nFROM CUSTOMER INNER JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nIn this case, we want to show customer address information for each order. Also\\nnotice that because CUSTOMER_ID is in both tables, we had to explicitly choose one\\n(although it should not matter which). In this case, we chose the CUSTOMER_ID in\\nCUSTOMER using an explicit syntax, CUSTOMER.CUSTOMER_ID.\\nFinally, the important part that temporarily merges two tables into one.  The FROM\\nstatement is where we execute our INNER JOIN. We specify that we are pulling from\\nCUSTOMER and inner joining it with CUSTOMER_ORDER, and that the commonality is on\\nthe CUSTOMER_ID fields (which have to be equal to line up):\\n56 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 72, 'page_label': '57', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT CUSTOMER.CUSTOMER_ID,\\nNAME,\\nSTREET_ADDRESS,\\nCITY,\\nSTATE,\\nZIP,\\nORDER_DATE,\\nSHIP_DATE,\\nORDER_ID,\\nPRODUCT_ID,\\nORDER_QTY\\nFROM CUSTOMER INNER JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nIf you have worked with Excel, think of this as a VLOOKUP on steroids, where\\ninstead of looking up CUSTOMER_ID and getting one value from another table, we are\\ngetting the entire matching record. This enables us to select any number of fields\\nfrom the other table.\\nNow take a look at the results ( Figure 8-5 ). Thanks to the INNER JOIN, this query\\ngives us a view that includes the customer details with each order.\\nFigure 8-5. CUSTOMER inner joined with CUSTOMER_ORDER\\nJoins truly give us the best of both worlds. We store data efficiently through normal‐\\nization, but can use joins to merge tables together on common fields to create more\\ndescriptive views of the data.\\nThere is one behavior with INNER JOIN to be aware of. Take a moment to look at the\\nresults of the preceding query. We can see that we have three “Re-Barre Construction”\\norders, as well as an order from “LITE Industrial” and another from “Marsh Lane\\nMetal Works. ” But are we missing anybody?\\nIf you go look at the CUSTOMER table, you will see there are five customers. Our INNER\\nJOIN query captured only three. “Rex Tooling Inc” and “Prairie Construction” are\\nnowhere to be found in our query results. So what exactly happened? There are no\\norders for Rex Tooling Inc and Prairie Construction, and because of this the INNER\\nJOIN excluded them from the query. It will only show records that inclusively exist in\\nboth tables (Figure 8-6).\\nINNER JOIN | 57'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 73, 'page_label': '58', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-6. A visualized inner join between CUSTOMER and CUSTOMER_ORDER\\n(note the two customers getting omitted, as they have no orders to join to)\\nWith an INNER JOIN, any records that do not have a common joined value in both\\ntables will be excluded. If we want to include all records from the CUSTOMER table, we\\ncan accomplish this with a LEFT JOIN.\\nLEFT JOIN\\nThose two customers, Rex Tooling Inc and Prairie Construction, were excluded from\\nthe INNER JOIN on CUSTOMER_ID because they had no orders to join on. But suppose\\nwe did want to include them anyway. Often, we may want to join tables and see, for\\nexample, all customers, even if they had no orders.\\nIf you are comfortable with the INNER JOIN, the left outer join is not much different.\\nBut there is one very subtle difference. Modify your query from before and replace\\nthe INNER JOIN with LEFT JOIN, the keywords for a left outer join. As shown in\\nFigure 8-7, the table specified on the “left” side of the LEFT JOIN operator (CUSTOMER)\\nwill have all its records included, even if they do not have any child records in the\\n“right” table (CUSTOMER_ORDER).\\n58 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 74, 'page_label': '59', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-7. The LEFT JOIN will include all records on the “left” table, even if they have\\nnothing to join to on the “right” table (which will be null)\\nRunning this, we have similar results to what we got from the INNER JOIN query ear‐\\nlier, but we have two additional records for the customers that have no orders\\n(Figure 8-8 ). For those two customers, notice all the fields that come from CUS\\nTOMER_ORDER are null, because there were no orders to join to. Instead of omitting\\nthem like the INNER JOIN did, the LEFT JOIN just made them null (Figure 8-9).\\nFigure 8-8. CUSTOMER left joined with CUSTOMER_ORDER (note the null\\nCUSTOMER_ORDER fields mean no orders were found for those two customers)\\nLEFT JOIN | 59'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 75, 'page_label': '60', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-9. CUSTOMER left joined with CUSTOMER_ORDER (note that “Rex Tooling”\\nand “Prairie Construction” joined to NULL, as they have no orders to join to)\\nIt is also common to use LEFT JOIN to check for “orphaned” child records that have\\nno parent, or conversely a parent that has no children (e.g., orders that have no cus‐\\ntomers, or customers that have no orders). Y ou can use a WHERE statement to check\\nfor null values that are a result of the LEFT JOIN. Modifying our previous example, we\\ncan filter for customers that have no orders by filtering any field from the right table\\nthat is null:\\nSELECT\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME\\nFROM CUSTOMER LEFT JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nWHERE ORDER_ID IS NULL\\nSure enough, you will only see Rex Tooling Inc and Prairie Construction listed, as\\nthey have no orders.\\n60 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 76, 'page_label': '61', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Other JOIN Types\\nThere is a RIGHT JOIN operator, which performs a right outer join that is almost iden‐\\ntical to the left outer join. It flips the direction of the join and includes all records\\nfrom the right table. However, the RIGHT JOIN is rarely used and should be avoided.\\nY ou should stick to convention and prefer left outer joins with LEFT JOIN, and put\\nthe “all records” table on the left side of the join operator.\\nThere also is a full outer join operator called OUTER JOIN that includes all records\\nfrom both tables. It does a LEFT JOIN and a RIGHT JOIN simultaneously, and can have\\nnull records in both tables. It can be helpful to find orphaned records in both direc‐\\ntions simultaneously in a single query, but it also is seldom used.\\nRIGHT JOIN and OUTER JOIN are not supported in SQLite due to\\ntheir highly niche nature. But most database solutions feature\\nthem.\\nJoining Multiple Tables\\nRelational databases can be fairly complex in terms of relationships between tables. A\\ngiven table can be the child of more than one parent table, and a table can be the par‐\\nent to one table but a child to another. So how does this all work?\\nWe have observed the relationship between CUSTOMER and CUSTOMER_ORDER. But there\\nis another table we can include that will make our orders more meaningful: the\\nPRODUCT table. Notice that the CUSTOMER_ORDER table has a PRODUCT_ID column,\\nwhich corresponds to a product in the PRODUCT table.\\nWe can supply not only CUSTOMER information to the CUSTOMER_ORDER table, but also\\nPRODUCT information using PRODUCT_ID (Figure 8-10).\\nOther JOIN Types | 61'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 77, 'page_label': '62', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-10. Joining multiple tables\\nWe can use these two relationships to execute a query that displays orders with cus‐\\ntomer information and product information simultaneously. All we do is define the\\ntwo joins between CUSTOMER_ORDER and CUSTOMER, and CUSTOMER_ORDER and PRODUCT\\n(Figure 8-11 ). If you start to get confused, just compare the following query to the\\ndiagram in Figure 8-10 , and you will see the joins are constructed strictly on these\\nrelationships:\\nSELECT\\nORDER_ID,\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME,\\nSTREET_ADDRESS,\\nCITY,\\nSTATE,\\nZIP,\\nORDER_DATE,\\nPRODUCT_ID,\\nDESCRIPTION,\\nORDER_QTY\\nFROM CUSTOMER\\nINNER JOIN CUSTOMER_ORDER\\nON CUSTOMER_ORDER.CUSTOMER_ID = CUSTOMER.CUSTOMER_ID\\nINNER JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\n62 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 78, 'page_label': '63', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-11. Joining ORDER, CUSTOMER, and PRODUCT fields together\\nThese orders are much more descriptive now that we’ve leveraged CUSTOMER_ID and\\nPRODUCT_ID to bring in customer and product information. As a matter of fact, now\\nthat we’ve merged these three tables, we can use fields from all three tables to create\\nexpressions. If we want to find the revenue for each order, we can multiply ORDER_QTY\\nand PRICE, even though those fields exist in two separate tables:\\nSELECT\\nORDER_ID,\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME,\\nSTREET_ADDRESS,\\nCITY,\\nSTATE,\\nZIP,\\nORDER_DATE,\\nPRODUCT_ID,\\nDESCRIPTION,\\nORDER_QTY,\\nORDER_QTY * PRICE as REVENUE\\nFROM CUSTOMER\\nINNER JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nINNER JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\nNow we have the revenue for each order, even though the needed columns came from\\ntwo separate tables.\\nGrouping JOINs\\nLet’s keep going with this example. We have the orders with their revenue, thanks to\\nthe join we built. But suppose we want to find the total revenue by customer? We still\\nneed to use all three tables and merge them together with our current join setup,\\nbecause we need the revenue we just calculated. But also we need to do a GROUP BY.\\nThis is legitimate and perfectly doable. Because we want to aggregate by customer, we\\nneed to group on CUSTOMER_ID and CUSTOMER_NAME. Then we need to SUM the\\nGrouping JOINs | 63'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 79, 'page_label': '64', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='ORDER_QTY * PRICE expression to get total revenue (Figure 8-12). To focus our GROUP\\nBY scope, we omit all other fields:\\nSELECT\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME,\\nsum(ORDER_QTY * PRICE) as TOTAL_REVENUE\\nFROM CUSTOMER_ORDER\\nINNER JOIN CUSTOMER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nINNER JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\nGROUP BY 1,2\\nFigure 8-12. Calculating TOTAL_REVENUE by joining and aggregating three tables\\nBecause we may want to see all customers, including ones that have no orders, we can\\nuse LEFT JOIN instead of INNER JOIN for all our join operations (Figure 8-13):\\nSELECT\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME,\\nsum(ORDER_QTY * PRICE) as TOTAL_REVENUE\\nFROM CUSTOMER\\nLEFT JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nLEFT JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\nGROUP BY 1,2\\n64 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 80, 'page_label': '65', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-13. Using a LEFT JOIN to include all customers and their TOTAL_REVENUE\\nWe have to LEFT JOIN both table pairs, because mixing LEFT JOIN\\nand INNER JOIN would cause the INNER JOIN to win, resulting in\\nthe two customers without orders getting excluded. This is because\\nnull values cannot be inner joined on and will always get filtered\\nout. A LEFT JOIN tolerates null values.\\nRex Tooling Inc and Prairie Construction are now present even though they have no\\norders. But we may want the values to default to 0 instead of null if there are no sales. \\nWe can accomplish this simply with the coalesce() function we learned about in\\nChapter 5 to turn nulls into zeros (Figure 8-14):\\nSELECT\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME,\\ncoalesce(sum(ORDER_QTY * PRICE), 0) as TOTAL_REVENUE\\nFROM CUSTOMER\\nLEFT JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nLEFT JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\nGROUP BY 1,2\\nFigure 8-14. Coalescing null TOTAL_REVENUE values to 0\\nGrouping JOINs | 65'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 81, 'page_label': '66', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Summary\\nJoins are the most challenging topic in SQL, but they are also the most rewarding.\\nJoins allow us to take data scattered across multiple tables and stitch it together into\\nsomething more meaningful and descriptive. We can take two or more tables and\\njoin them together into a larger table that has more context. In the next chapter, we\\nwill learn more about joins and how they are naturally defined by table relationships.\\n66 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 82, 'page_label': '67', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 9\\nDatabase Design\\nPlanning a Database\\nSo far in this book, we have only learned how to be consumers of data with the\\nSELECT statement.  We have done analysis operations that read data and transform it\\nin interesting ways, but none of this physically changes the data in the tables. A\\nSELECT statement is a read-only operation. Sometimes, though, we will want to\\nCREATE new tables, as well as INSERT, UPDATE, and DELETE records.\\nWhen you create your own tables to support your business, it should not be done\\nlightly. Y ou need to plan carefully because bad database design is sure to cause regrets\\ndown the road. There are critical questions that should drive your design:\\nDesign questions\\n• What are the business requirements?\\n• What tables will I need to fulfill those requirements?\\n• What columns will each table contain?\\n• How will the tables be normalized?\\n• What will their parent/child relationships be?\\nIt might be a good idea to draft a diagram showing the tables and how they are\\nrelated. But design is not the only factor to consider. Populating data should be\\npart of the planning process too. If the data is not maintainable and kept up to\\ndate, then the design has already failed. This factor is often overlooked and can\\neasily cause a database project to fail.\\n67'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 83, 'page_label': '68', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Data questions\\n• How much data will be populated into these tables?\\n• Who/what will populate the data into these tables?\\n• Where will the data come from?\\n• Do we need processes to automatically populate the tables?\\nData inception has to happen somewhere. Depending on the nature of the data, it\\ncan be created within your organization or received from an external party. If you\\nneed to store a high volume of data that updates regularly, chances are a human\\ncannot do this task manually. Y ou will need a process written in Java, Python, or\\nanother coding language to do that.\\nAlthough security and administration are beyond the scope of this book, central‐\\nized databases usually are concerned with these areas. Administrating privileges\\nand security is a full-time job in itself and often done by database administrators\\n(DBAs). For centralized databases, security factors should be considered.\\nSecurity questions\\n• Who should have access to this database?\\n• Who should have access to which tables? Read-only access? Write access?\\n• Is this database critical to business operations?\\n• What backup plans do we have in the event of disaster/failure?\\n• Should changes to tables be logged?\\n• If the database is used for websites or web applications, is it secure?\\nSecurity is often a tough topic to address. Excessive security creates bureaucracy\\nand obstructs nimbleness, but insufficient security will invite calamity. Like with\\nany complex issue, a balance between the two extremes has to be found. But\\nsecurity should become a high priority when the database is used for a website.\\nConnecting anything to the Web makes it more vulnerable to leaks and malicious\\nattacks.\\nOne of the most common malicious hacks is SQL injection. If a\\nweb developer has failed to implement security measures in a\\nwebsite, you can type a carefully crafted SELECT statement\\nright inside a web browser, and get the query results displayed\\nright back to you! 130 million credit card numbers were stolen\\nthis way in 2009.\\nSQLite has few security or administrative features, as these features would be\\noverkill in a lightweight database. If your SQLite databases need to be secured,\\n68 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 84, 'page_label': '69', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='protect the database files the same way you would any other file. Either hide\\nthem, copy them to a backup, or distribute copies to your coworkers so they do\\nnot have access to the “master copy. ”\\nWith all these considerations in mind, let’s design our first database.\\nThe SurgeTech Conference\\nY ou are a staff member for the SurgeTech conference, a gathering of tech startup\\ncompanies seeking publicity and investors. The organizer has tasked you with creat‐\\ning a database to manage the attendees, companies, presentations, rooms, and pre‐\\nsentation attendance. How should this database be designed?\\nFirst, review the different entities and start thinking about how they will be structured\\ninto tables. This may seem like a large number of business asks to capture, but any\\ncomplex problem can be broken down into simple components.\\nATTENDEE\\nThe attendees are registered guests (including some VIPs) who are checking out the\\ntech startups. Each attendee’s ID, name, phone number, email, and VIP status will\\nneed to be tracked.\\nTaking all this information, we may design the ATTENDEE table with these columns:\\nCOMPANY\\nThe startup companies need to be tracked as well. The company ID, company name,\\ncompany description, and primary contact (who should be listed as an attendee) for\\neach must be tracked:\\nThe SurgeTech Conference | 69'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 85, 'page_label': '70', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='PRESENTATION\\nSome companies will schedule to do a presentation for a specific slot of time (with a\\nstart time and end time). The company leading the presentation as well as a room\\nnumber must also be booked for each presentation slot:\\nROOM\\nThere will be rooms available for the presentations, each with a room ID number, a\\nfloor number, and a seating capacity:\\nPRESENTATION_ATTENDANCE\\nIf attendees are interested in hearing a company’s presentation, they can acquire a\\nticket (with a ticket ID) and be allowed in. This will help keep track of who attended\\nwhat presentations. To implement this, the PRESENTATION_ATTENDANCE table will track\\nthe ticket IDs and pair the presentations with the attendees through their respective\\nIDs to show who was where:\\nPrimary and Foreign Keys\\nY ou should always strive to have a primary key on any table. A primary key is a spe‐\\ncial field (or combination of fields) that provides a unique identity to each record. A\\nprimary key often defines a relationship and is frequently joined on. The ATTENDEE\\n70 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 86, 'page_label': '71', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='table has an ATTENDEE_ID field as its primary key, COMPANY has COMPANY_ID, and so\\non. While you do not need to designate a field as a primary key to join on it, it allows\\nthe database software to execute queries much more efficiently. It also acts as a con‐\\nstraint to ensure data integrity. No duplicates are allowed on the primary key, which\\nmeans you cannot have two ATTENDEE records both with an ATTENDEE_ID of 2. The\\ndatabase will forbid this from happening and throw an error.\\nTo focus our scope in this book, we will not compose a primary key\\noff more than one field. But be aware that multiple fields can act as\\na primary key, and you can never have duplicate combinations of\\nthose fields. For example, if you specified your primary key on the\\nfields REPORT_ID and APPROVER_ID, you can never have two records\\nwith the same combination of REPORT_ID and APPROVER_ID.\\nDo not confuse the primary key with a foreign key. The primary key exists in the par‐\\nent table, but the foreign key exists in the child table. The foreign key in a child table\\npoints to the primary key in its parent table. For example, the ATTENDEE_ID in the\\nATTENDEE table is a primary key, but the ATTENDEE_ID in the\\nPRESENTATION_ATTENDANCE table is a foreign key. The two are joined together for a\\none-to-many relationship. Unlike a primary key, a foreign key does not enforce\\nuniqueness, as it is the “many” in a “one-to-many” relationship.\\nThe primary key and foreign key do not have to share the same field name. The\\nBOOKED_COMPANY_ID in the PRESENTATION table is a foreign key pointing to the\\nCOMPANY_ID in its parent table COMPANY. The field name can be different on the for‐\\neign key to make it more descriptive of its usage. In this case, BOOKED_COMPANY_ID is\\nmore descriptive than just COMPANY_ID. The semantics are subjective but still legiti‐\\nmate as long as the business wording is clear.\\nThe Schema\\nApplying our knowledge of primary keys and foreign keys, we can establish the rela‐\\ntionships between these five tables and draw a database schema as shown in\\nFigure 9-1. A database schema is a diagram showing tables, their columns, and their\\nrelationships. All the primary keys and foreign keys are connected by arrows. The\\nnon-tipped side of the arrow ties to a primary key, while the tipped side points to a\\nforeign key. These arrows visualize how each parent table supplies data to a child\\ntable.\\nThe Schema | 71'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 87, 'page_label': '72', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-1. The database schema for the SurgeTech conference, with all tables and rela‐\\ntionships\\nIt can be overwhelming to look at these tables and relationships all at once. But all\\ncomplex structures can be broken down into simple pieces. Chances are you will\\nnever write a SELECT query that uses all the tables, and you probably will only SELECT\\nfrom two (maybe three) tables. Therefore, the secret to observing a schema is to focus\\nonly on two or three tables and their relationships at a time. While you analyze your\\ndrafted design, you can ensure the tables are efficiently normalized and primary/\\nforeign keys are used effectively (Figure 9-2).\\n72 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 88, 'page_label': '73', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-2. Focusing on just two tables and their relationships (here we can easily see the\\nPRIMARY_CONTACT_ATTENDEE_ID supplies name and contact information from\\nthe ATTENDEE table)\\nIf you can successfully visualize different SELECT queries and joins you would typi‐\\ncally use on the data, the database schema is probably sound.\\nCreating a New Database\\nWith a well-planned design, it is now time to actually create this database. We are\\ngoing to use SQLiteStudio’s tools to create the tables and components. But along the\\nway, SQLiteStudio will show us the SQL it uses to create and modify our tables.\\nFirst, navigate to Database→Add a Database (Figure 9-3).\\nCreating a New Database | 73'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 89, 'page_label': '74', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-3. Adding a database\\nClick the green “plus” button circled in Figure 9-4 to create a new database.\\nFigure 9-4. Creating a database\\n74 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 90, 'page_label': '75', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Browse to the folder you would like to save the database to. In the “File name” field,\\nprovide a name for the database file. It usually is good practice to end the name with\\nthe file extension .db. In this case, we might name it surgetech_conference.db\\n(Figure 9-5).\\nFigure 9-5. Selecting a location to create a database\\nClick Save, then OK. Y ou should now see the new database in your navigator\\n(Figure 9-6).\\nFigure 9-6. Our new surgetech_conference database\\nThis database is empty, so next we will add some tables to it.\\nCreating a New Database | 75'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 91, 'page_label': '76', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CREATE TABLE\\nWhen we create a table in SQL, we use a CREATE TABLE statement. However, I am an\\nadvocate for using tools that make tasks easier. We are going to use SQLiteStudio’s\\nvisual tools to create the table, and when we are done it will generate and display the\\nCREATE TABLE statement it built for us.\\nRight-click on the Tables item in the navigator and click Create a table, as shown in\\nFigure 9-7.\\nFigure 9-7. Creating a table\\nY ou will then come to the table Structure tab. Here we add, modify, and remove col‐\\numns from our table (Figure 9-8).\\n76 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 92, 'page_label': '77', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-8. The table Structure tab, which we can use to add, modify, and remove col‐\\numns from a table\\nWe can also define various constraints to ensure data entered into the columns con‐\\nforms to rules we specify. We also supply a name for this table in the “Table name”\\nfield. Type in COMPANY for this table name. Also note there is a button to save your\\nedits and another to add a new column.\\nClick the Add Column button, and you will see a dialog to define a new column and\\nits attributes. Name this column COMPANY_ID and make its data type “INTEGER, ”\\nas shown in Figure 9-9.\\nCREATE TABLE | 77'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 93, 'page_label': '78', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-9. Defining a new COMPANY_ID column that holds integers; it also is config‐\\nured to be the primary key and will automatically populate a value via “ Autoincrement”\\nfor each inserted record\\nThis is the COMPANY_ID field, and we need to define this as the primary key for the\\nCOMPANY table. Typically, the easiest way to assign key values is to do it sequentially for\\neach new record. The first record will have a COMPANY_ID of 1, then the second record\\nwill have 2, then 3, and so on. When we INSERT records in the next chapter, this is a\\npain to do manually. But we can configure SQLite to automatically assign an ID for\\neach record we insert. Simply check Primary Key, then click Configure, then select\\nAutoincrement and click Apply (Figure 9-9).\\nFinally, click OK in the Column window and you will see our first column defined\\n(Figure 9-10).\\n78 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 94, 'page_label': '79', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-10. Our first column is defined; notice the key symbol indicating this column is\\nthe primary key\\nWe have now defined our first column, and because it was the primary key column, it\\ntook some extra work. The rest of the columns will be a little easier to set up.\\nClick on the Add Column button again to create another column (Figure 9-11). Label\\nthis column NAME and make it a VARCHAR type, which is for text that can be of\\nvarying lengths. Specify the maximum number of characters to be 30. Because we\\nlikely never want this field to be null, check the “Not NULL ” constraint. If any records\\nare added or modified with NAME set to null, then the database will reject the edits.\\nFigure 9-11. Creating a “NAME” column with type VARCHAR, a max character length\\nof 30, and a “Not NULL ” constraint\\nCREATE TABLE | 79'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 95, 'page_label': '80', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Click OK and then create two more columns, DESCRIPTION and PRIMARY_CON‐\\nTACT_ATTENDEE_ID, with the configurations shown in Figure 9-12. Note that PRI‐\\nMARY_CONTACT_ATTENDEE_ID should be a foreign key, but we have not defined\\nthat yet. We will come back to configure this after we have created its parent, the\\nATTENDEE table.\\nFigure 9-12. Creating the remaining two columns\\nFinally, click the Save Table button. Y ou will be presented with a CREATE TABLE state‐\\nment that SQLiteStudio has built for you, and will execute on your approval\\n(Figure 9-13).\\n80 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 96, 'page_label': '81', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-13. Click the Save Table button in the top toolbar, and SQLiteStudio will\\npresent the CREATE TABLE statement it is about to execute based on our inputted defi‐\\nnitions\\nHow cool is that? SQLiteStudio wrote SQL for you based on all the table definitions\\nyou built. Before you click OK, let’s take a quick look at the CREATE TABLE statement\\nto see how it works:\\nCREATE TABLE COMPANY (\\n    COMPANY_ID INTEGER PRIMARY KEY AUTOINCREMENT,\\n    NAME VARCHAR(30) NOT NULL,\\n    DESCRIPTION VARCHAR(60),\\n    PRIMARY_CONTACT_ID INTEGER NOT NULL\\n);\\nIf you inspect the SQL query, you will see the CREATE TABLE statement declares a new\\ntable named COMPANY. After that, everything in parentheses defines the table columns.\\nEach table column is defined by a name, followed by its type, and then any con‐\\nstraints or rules such as PRIMARY KEY, AUTOINCREMENT, or NOT NULL.\\nY ou could literally copy this statement and execute it in the SQL editor, but just click\\nOK and it will execute the statement for you. After that, you should see your new\\ntable in the navigator (Figure 9-14).\\nCREATE TABLE | 81'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 97, 'page_label': '82', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-14. The COMPANY table in the navigator\\nThe AUTOINCREMENT constraint in SQLite is actually not necessary.\\nWe use it here for practice because it is necessary for other plat‐\\nforms, including MySQL. In SQLite, making a column of type\\nINTEGER a primary key will automatically make it handle its own\\nID assignment. As a matter of fact, it is actually more efficient in\\nSQLite to not use AUTOINCREMENT and let the primary key implicitly\\ndo it.\\nCreate the remaining four tables in the same manner. The needed CREATE TABLE\\nstatements are shown here (you can choose to build the tables using the Structure tab\\nor just execute the CREATE TABLE statements verbatim in the SQL editor):\\nCREATE TABLE ROOM (\\n    ROOM_ID       INTEGER PRIMARY KEY AUTOINCREMENT,\\n    FLOOR_NUMBER  INTEGER NOT NULL,\\n    SEAT_CAPACITY INTEGER NOT NULL\\n);\\nCREATE TABLE PRESENTATION (\\n    PRESENTATION_ID   INTEGER PRIMARY KEY AUTOINCREMENT,\\n    BOOKED_COMPANY_ID INTEGER NOT NULL,\\n    BOOKED_ROOM_ID    INTEGER NOT NULL,\\n    START_TIME        TIME,\\n    END_TIME          TIME\\n);\\n82 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 98, 'page_label': '83', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CREATE TABLE ATTENDEE (\\n    ATTENDEE_ID INTEGER      PRIMARY KEY AUTOINCREMENT,\\n    FIRST_NAME  VARCHAR (30) NOT NULL,\\n    LAST_NAME   VARCHAR (30) NOT NULL,\\n    PHONE       INTEGER,\\n    EMAIL       VARCHAR (30),\\n    VIP         BOOLEAN      DEFAULT (0)\\n);\\nCREATE TABLE PRESENTATION_ATTENDANCE (\\n    TICKET_ID       INTEGER PRIMARY KEY AUTOINCREMENT,\\n    PRESENTATION_ID INTEGER,\\n    ATTENDEE_ID     INTEGER\\n);\\nNote that the ATTENDEE table has a VIP field which is a Boolean (true/false) value.  By\\ndefault, if a record does not specify a value for a column, the value will default to null.\\nIt might be a good idea to default this particular field to false (0) if a value is never\\nprovided. The preceding SQL snippet reflects this, but you can also accomplish this\\nin the column builder as shown in Figure 9-15.\\nFigure 9-15. Setting a default value for a column\\nBy now, you should have all five of your tables created with all constraints defined,\\nexcept the foreign keys (Figure 9-16).\\nCREATE TABLE | 83'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 99, 'page_label': '84', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-16. All tables have been built\\nMost database solutions enforce values in a column only by the\\nspecified data type. SQLite does not. In SQLite, you can put a TEXT\\nvalue in an INTEGER column. Other database solutions will disallow\\nthis. While this seems counterintuitive, the creators of SQLite made\\nit this way for technical reasons beyond the scope of this book.\\nSetting the Foreign Keys\\nThere is one last task remaining to make our tables airtight.  We have defined the pri‐\\nmary keys but not the foreign keys. Remember that the foreign key in a child table is\\ntied to the primary key of a parent table. Logically, we should never have a foreign key\\nvalue that does not have a corresponding primary key value.\\nFor example, we should never have a PRESENTATION record with a\\nBOOKED_COMPANY_ID value that does not exist in the COMPANY table’s COMPANY_ID col‐\\numn. If there is a BOOKED_COMPANY_ID value of 5, there had better be a record in\\nCOMPANY with a COMPANY_ID of 5 as well. Otherwise, it is an orphaned record. We can\\nenforce this by setting up foreign key constraints.\\nOpen up the PRESENTATION table and double-click the BOOKED_COMPANY_ID column to\\nmodify it (Figure 9-17). Check Foreign Key and then click Configure. Set the foreign\\ntable to CUSTOMER and the foreign column to CUSTOMER_ID. This will constrain\\nBOOKED_COMPANY_ID to only the values in the CUSTOMER_ID column in the CUSTOMER\\ntable. Click Apply, then OK.\\n84 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 100, 'page_label': '85', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-17. Making BOOKED_COMPANY_ID a foreign key to COMPANY_ID in the\\nCOMPANY table\\nClick the Commit Changes button on the Structure tab, and a series of SQL state‐\\nments will be generated to implement the foreign key. Y ou can look at the SQL if you\\nare curious, but it will only make you appreciate all the work that SQLiteStudio has\\ndone for you. Then click OK to commit the change.\\nSetting the Foreign Keys | 85'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 101, 'page_label': '86', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Using foreign keys keeps data tight and prevents deviant data from undermining the\\nrelationships. We should define foreign key constraints for all relationships in this\\ndatabase so no orphan records ever occur.\\nAt this point, you can create foreign keys for all of the following parent–child rela‐\\ntionships by repeating the same procedure:\\nCreate foreign key for [Table].[Field] Off parent primary key [Table].[Field]\\nPRESENTATION.BOOKED_ROOM_ID ROOM.ROOM_ID\\nPRESENTATION_ATTENDANCE.PRESENTATION_ID PRESENTATION.PRESENTATION_ID\\nPRESENTATION_ATTENDANCE.ATTENDEE_ID ATTENDEE.ATTENDEE_ID\\nCOMPANY.PRIMARY_CONTACT_ATTENDEE_ID ATTENDEE.ATTENDEE_ID\\nNow we have ensured every child record has a parent record, and no orphans will\\never be allowed into the database.\\nIf you ever use SQLite outside SQLiteStudio, note that the foreign\\nkey constraint enforcement might have to be turned on first.\\nSQLiteStudio has it enabled by default, but other SQLite environ‐\\nments may not.\\nCreating Views\\nIt is not uncommon to store frequently used SELECT queries in a database.  When you\\nsave a query in a database, it is called a view. A view behaves much like a table. Y ou\\ncan run SELECT statements against it and join it to other views and tables. But the data\\nis completely derived from a SELECT query you specify, so in many cases you cannot\\nmodify the data (nor would it make sense to).\\nSuppose we run a SELECT query very often to give us a more descriptive view of the\\nPRESENTATION table, which pulls in the booked company and booked room informa‐\\ntion:\\nSELECT\\nCOMPANY.NAME as BOOKED_COMPANY,\\nROOM.ROOM_ID as ROOM_NUMBER,\\nROOM.FLOOR_NUMBER as FLOOR,\\nROOM.SEAT_CAPACITY as SEATS,\\nSTART_TIME,\\nEND_TIME\\nFROM PRESENTATION\\n \\n \\n \\n86 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 102, 'page_label': '87', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='INNER JOIN COMPANY\\nON PRESENTATION.BOOKED_COMPANY_ID = COMPANY.COMPANY_ID\\nINNER JOIN ROOM\\nON PRESENTATION.BOOKED_ROOM_ID = ROOM.ROOM_ID\\nNow suppose we want to store this query in the database so it can easily be called. We\\ncan do that by right-clicking the Views item in the navigator, then clicking Create a\\nview (Figure 9-18).\\nFigure 9-18. Creating a view\\nY ou will then be taken to a view designer window ( Figure 9-19 ). Navigate to the\\nQuery tab. Here you will paste your SELECT statement. In the “View name” field,\\nname this view PRESENTATION_VW (with “VW” an abbreviation for “VIEW”),\\nand click the green checkmark to save it. Before it executes the SQL query to create\\nthe view, SQLiteStudio will present it for review. As you can observe, the SQL syntax\\nto create a view is fairly simple. It is CREATE VIEW [view_name] AS [a SELECT\\nquery].\\nCreating Views | 87'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 103, 'page_label': '88', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-19. Creating a view off a SELECT query\\nWhen you click OK, you should now see the view in your navigator under “Views”\\n(Figure 9-20 ). Double-click on it and in the Query tab you will see the query it is\\nusing, and the Data tab will have the query results.\\nFigure 9-20. Although there is no data yet, the SELECT query has been saved as a view\\ncalled PRESENTATION_VW\\nThe Data tab will be blank, until the queried tables are populated with data.\\nNote also that we can query from a view just like it was a table (and apply filters, do\\njoin operations, and do anything else you could do in a SELECT with a table):\\nSELECT * FROM PRESENTATION_VW\\nWHERE SEAT_CAPACITY >= 30\\n88 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 104, 'page_label': '89', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Summary\\nIn this chapter, we dived into creating our own databases and learned how to design\\nthem efficiently. We studied table relationships, which help us clearly define how\\ntables are joined. We also explored some of the various column constraints (including\\nPRIMARY KEY, FOREIGN KEY, NOT NULL, AUTOINCREMENT, and DEFAULT) to keep data\\nconsistent and ensure it follows rules we define.\\nIn the next chapter, we will actually populate and modify data in this database. We\\nwill witness our design at work and appreciate the time we put into planning it. A\\ngood design with well-defined constraints will make a resilient database.\\nOne topic this chapter did not cover is indexes. Indexes are useful\\nfor tables with a large number of records but have performance\\nissues with SELECT statements. “ APPENDIX B2 – Improving Per‐\\nformance with Indexes” on page 108 discusses indexes and when\\nand when not to use them. \\nSummary | 89'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 105, 'page_label': '90', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 106, 'page_label': '91', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 10\\nManaging Data\\nIn the previous chapter, we learned not only how to create a database but how to do it\\nwell. Well-considered table design, column constraints, and relationships will really\\nshine once we start putting data into the tables. With our strong table design and the\\nwell-thought-out relationships between them, we will be able to join efficiently and\\neasily. When a piece of data needs to be changed (e.g., a customer address), we only\\nneed to change it in one place rather than several. When a bad piece of data comes in,\\nhopefully we have set enough sensible constraints to prevent it from entering the\\ndatabase.\\nIn this chapter, we will learn how to INSERT, DELETE, and UPDATE records. Fortunately,\\nwriting operations like these is much simpler than writing SELECT statements.\\nThese SQL write operations do not have to be done by a human. Software (written in\\nJava, Python, or other coding languages) will often generate and execute SQL state‐\\nments to read and write data the same way a human would, but much more effi‐\\nciently. Although this is beyond the scope of this book, coding languages will be\\ntouched on in the next chapter if that is pertinent to you.\\nINSERT\\nIn a relational database, data only exists if the database first receives records. The\\nINSERT statement does just that and inserts a record into the database. Y ou can pick\\nwhat fields to populate in the record, and the rest of the fields will be null or use a\\ndefault value.\\nFirst, we will INSERT an ATTENDEE record into the SurgeTech database we created in\\nthe last chapter. An INSERT to add yourself to the database should look something like\\nthis. Execute the following statement with your name:\\n91'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 107, 'page_label': '92', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"INSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME)\\nVALUES ('Thomas','Nield')\\nLet’s break this statement down:\\nINSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME)\\nVALUES ('Thomas','Nield')\\nTo start, we declare that we are inserting a record into the ATTENDEE table, and the\\nfields we are choosing to populate are FIRST_NAME and LAST_NAME:\\nINSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME)\\nVALUES ('Thomas','Nield')\\nThen we specify the values for each of these fields. Note that we specify the values in\\nthe same order we declared the fields in: 'Thomas' corresponds to FIRST_NAME, and\\n'Nield' to LAST_NAME.\\nRun SELECT * FROM ATTENDEE  to see if our INSERT made it in. Sure enough, the\\nrecord now exists (Figure 10-1).\\nFigure 10-1. Our newly inserted record in the ATTENDEE table\\nThere are a number of observations to make here. We did not populate all the col‐\\numns in our INSERT, but due to the rules we created in the previous chapter, some of\\nthe columns were assigned a default value.\\nThe ATTENDEE_ID gave itself a value of 1 due to our PRIMARY KEY and AUTOINCREMENT\\nrule.  If we were to INSERT another record, it would automatically be assigned an\\nATTENDEE_ID of 2, then 3, and so on. On an INSERT, you should avoid populating the\\nATTENDEE_ID field yourself and let it assign its own ID.\\nAgain, the AUTOINCREMENT constraint in SQLite is actually not nec‐\\nessary. It is needed for MySQL and other platforms, though, hence\\nwhy we are doing it for practice. In SQLite, simply making a col‐\\numn of type INTEGER a primary key will automatically assign IDs to\\nnew records.\\nPHONE and EMAIL were not specified in our INSERT, so they were left null. If either of\\nthese columns had a NOT NULL constraint and no default value policy, our INSERT\\n92 | Chapter 10: Managing Data\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 108, 'page_label': '93', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"would have failed. But in our design, we have allowed these two fields to be null in\\ncase our attendees prefer to be off the grid.\\nThe VIP status was not specified in our INSERT either, but we gave this field a default\\nvalue of false (0). So instead of making it null, SQLite resorted to using the default\\nvalue we specified.\\nHopefully by now, you are already appreciating that the design is working efficiently.\\nBecause of the policies we set, the columns resorted to default values when they were\\nnot provided with any.\\nMultiple INSERTs\\nIf you have a lot of records to INSERT, you do not have to do it one at a time. Y ou can\\nspecify multiple records in a single INSERT command. Simply repeat the clause fol‐\\nlowing VALUES and separate each entry with a comma:\\nINSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME, PHONE, EMAIL, VIP)\\nVALUES\\n('Jon', 'Skeeter',4802185842,'john.skeeter@rex.net', 1),\\n('Sam','Scala',2156783401,'sam.scala@gmail.com', 0),\\n('Brittany','Fisher',5932857296,'brittany.fisher@outlook.com', 0)\\nDoing multiple inserts in this manner is far more efficient, especially if you have\\nthousands of records. If a process written in Java or Python is populating a table, it\\nshould use this syntax to insert large amounts of records rather than executing one\\nINSERT at a time. Otherwise, the process can run very slowly.\\nY ou can also INSERT records using the results from a SELECT query. This is helpful if\\nyou need to migrate data from one table to another. Just make sure the SELECT fields\\nline up with the INSERT fields and have the same order and data types:\\nINSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME, PHONE, EMAIL)\\nSELECT FIRST_NAME, LAST_NAME, PHONE, EMAIL\\nFROM SOME_OTHER_TABLE\\nTesting the Foreign Keys\\nLet’s take an opportunity to witness another policy of our design at work: the foreign\\nkeys.\\nRight now, we should only have four attendees with ATTENDEE_ID assignments of 1, 2,\\n3, and 4. But test this functionality by inserting a COMPANY record with a PRI\\nMARY_CONTACT_ID value of 5:\\nINSERT INTO COMPANY (NAME, DESCRIPTION, PRIMARY_CONTACT_ID)\\nVALUES ('RexApp Solutions', 'A mobile app delivery service', 5)\\nINSERT | 93\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 109, 'page_label': '94', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"This query should have failed and an error should be displayed at the bottom of the\\nwindow (Figure 10-2).\\nFigure 10-2. The foreign key constraint successfully raised an error\\nThis is good because it means our foreign key constraint has worked: it kept an\\norphan record out. The INSERT needs to have a PRIMARY_CONTACT_ID that is existent.\\nSo, if we change it to 3 (Sam Scala), the INSERT should now work correctly:\\nINSERT INTO COMPANY (NAME, DESCRIPTION, PRIMARY_CONTACT_ID)\\nVALUES ('RexApp Solutions', 'A mobile app delivery service', 3)\\nDELETE\\nThe DELETE statement is dangerously simple. It deletes all records in a table:\\nDELETE FROM ATTENDEE\\nHowever, you can conditionally delete records with a WHERE statement. If we wanted\\nto remove all records that have no contact information, we could filter to records\\nwhere PHONE and EMAIL are null:\\nDELETE FROM ATTENDEE\\nWHERE PHONE IS NULL\\nAND EMAIL IS NULL\\nBecause it is perilously easy to make mistakes with a DELETE statement, it is a good\\npractice to replace the DELETE with a SELECT * first. Executing that query gives us a\\npreview of what records will be deleted:\\nSELECT * FROM ATTENDEE\\nWHERE PHONE IS NULL\\nAND EMAIL IS NULL\\nTRUNCATE TABLE\\nIn the previous section, we looked at a means to delete all records from a table:\\nDELETE FROM ATTENDEE\\nAlthough not used in SQLite, on some database platforms (like MySQL) the preferred\\nway to delete all records from a table is to use TRUNCATE TABLE:\\nTRUNCATE TABLE ATTENDEE\\n94 | Chapter 10: Managing Data\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 110, 'page_label': '95', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Using this command will allow the database engine to reset the autoincrements for\\nany primary keys as well as any other constraint behaviors. It also allows it to make\\nsome optimizations behind the scenes to reset the table.\\nWhile SQLite does not support TRUNCATE TABLE, it does allow some similar optimiza‐\\ntions when you run a DELETE without a WHERE.\\nUPDATE\\nFinally, we come to the UPDATE command. The UPDATE modifies existing records. If\\nwe wanted to update the EMAIL values for all records to be uppercase, we could do\\nthat with this statement using the UPPER() function:\\nUPDATE ATTENDEE SET EMAIL = UPPER(EMAIL)\\nWe can also update multiple fields at once. Just separate each expression after the SET\\nkeyword with a comma. To update both the FIRST_NAME and LAST_NAME fields to\\nuppercase, run this command:\\nUPDATE ATTENDEE SET FIRST_NAME = UPPER(FIRST_NAME),\\nLAST_NAME = UPPER(LAST_NAME)\\nLike with DELETE, we can use a WHERE to conditionally apply updates to records. Exe‐\\ncute the following query to set the VIP field to true where the ATTENDEE_ID is 3 or 4:\\nUPDATE ATTENDEE SET VIP = 1\\nWHERE ATTENDEE_ID IN (3,4)\\nDROP TABLE\\nThere may be times where you want to remove a table altogether from the database.\\nJust type DROP TABLE followed by the name of the table you want to delete (this is a\\ndangerous statement as well because it deletes the table permanently, so be careful\\nand certain about what you are doing):\\nDROP TABLE MY_UNWANTED_TABLE\\nSummary\\nAt this point, you have the tools you need to go out and create your own database and\\nmanage its data. Y ou may have questions on how to do all of this efficiently or pro‐\\nvide practical means for users to input and update data, because chances are you can‐\\nnot teach all of them SQL and they will want a graphical user interface. Or maybe you\\nwant to know how to pump large amounts of data into a database automatically or\\nsynchronize it with another data source. This will be lightly addressed in the final\\nchapter.\\nUPDATE | 95'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 111, 'page_label': '96', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 112, 'page_label': '97', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 11\\nGoing Forward\\nIf you have gotten to this chapter and covered all the material to this point, congrats!\\nY ou have learned an adaptable and marketable skillset that can be applied to the fields\\nof business, information technology, and engineering. With consistent practice and\\nusage, you should feel comfortable positioning yourself professionally with SQL and\\ndatabase design.\\nWhen it comes to understanding and using technology, everyone has a different level\\nof ambition. Y ou may very well have read this book and see this material as not cen‐\\ntral to your career goals, but rather complementary. This may be the case if you are an\\ninterdepartmental manager of sorts and just want to understand IT a little better. Y ou\\nmay feel this book has given all the insight you need, which is perfectly acceptable.\\nOnly you can define your career goals and prioritize what maximizes your value!\\nBut perhaps SQL has really clicked with you, and you have questions on how you can\\ndo more. Maybe you want to have a deeper knowledge of SQL and business intelli‐\\ngence. Or maybe you want to create software and graphical user interfaces that inter‐\\nact with a database. After reading this material, you may feel you are not done and\\nhave unanswered questions on how to create full business solutions. If this is you, I\\nencourage you to keep researching and pursuing whatever knowledge you need to\\nfulfill your goals.\\nIf you want to learn more detailed functionalities in SQL, there are plenty of books\\nand resources available. Y ou can expand your SQL knowledge with more advanced\\nfunctionalities like subqueries and triggers. If you want a deep understanding of\\nSQLite, check out Using SQLite by Jay A. Kreibich (O’Reilly). Y ou can also pursue\\nlearning another database platform like MySQL in greater detail. But whatever you\\ndo, do not hesitate to use the Internet as a resource. It has an infinite number of tuto‐\\nrials, guides, walkthroughs, and documentation to help you expand your knowledge.\\n97'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 113, 'page_label': '98', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='While SQL alone can increase your opportunities, you can become very adaptable by\\nlearning another relevant technology or two. If you are eager to expand your skillsets,\\nthen consider learning and integrating another topic. This will open dozens of career\\npaths. Integrating a few technology skillsets can make you even more marketable\\nthan just specializing in one skillset.\\nPython is a great programming language that is accessible to beginners, and yet is\\npowerful enough to be used by professional programmers and security testing hack‐\\ners alike. Y ou can use Python to process data into databases or write applications and\\nprocesses that support a business. It is an adaptable programming language and pairs\\nwell with database development. A great resource to get started with Python is Al\\nSweigart’s popular book Automate the Boring Stuff with Python (No Starch Press).\\nR can be used for business intelligence programming. R is a statistics programming\\nlanguage used to perform deep machine learning and analysis on data. I have noticed\\nit is preferred by the business/science/math crowd because it focuses on those areas\\nvery well. It has endless functionalities to analyze anything from DNA sequences to\\nbusiness market trends. It does a great job applying classic statistical models like lin‐\\near regression. I have not used many resources on R, but I have heard Coursera offers\\nsome great online courses on it.\\nPython’s adaptability is catching up with R, as it now features libraries for data min‐\\ning. Both technologies are free and open source. When a SELECT statement does not\\nprovide enough functionality for a specific question you have about your data,\\nPython and R are powerful tools to glean that information.\\nIf you are interested in full-blown software development and not just casual scripting,\\nlanguages such as Java, Apple’s Swift, or Microsoft’s C# are great to pick up. With\\nthese languages you can make commercial-grade business software solutions and\\nmobile apps. Full-blown programming can take many hours to master, and it is chal‐\\nlenging. But if you become good at it, the work and opportunities are endless. If you\\ndecide to pursue Java, Herbert Schildt’s Java: A Beginner’s Guide (McGraw Hill Profes‐\\nsional) is a good book to start with.\\nThese definitely are not the only career paths you can take. Technology needs are so\\nniche nowadays, you may adapt yourself into a role that never has been done before.\\nJust be sure to focus on learning material that is pertinent to your immediate needs.\\nOn top of books, there are also great websites like Pluralsight and W3Schools to gain\\nfoundational knowledge in whatever topics you choose to pursue. And never under‐\\nestimate Google when you have a specific question. Chances are if you have a ques‐\\ntion or problem, someone else has probably had it too and the answer was posted\\nonline.\\nIf you cannot find an answer to your question, there is a great Q&A site called Stack\\nOverflow filled with  professionals and enthusiasts in all areas of technology. Y ou can\\n98 | Chapter 11: Going Forward'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 114, 'page_label': '99', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='post a well-defined, researched question and get answers from other members for\\nfree. The most productive answerers often work for Google, Oracle, Microsoft, and\\nother big technology companies. Some of them have written books or are Silicon Val‐\\nley celebrities in the technology community. These people provide expertise simply\\nbecause they are passionate about what they do.\\nFinally, you are more than welcome to email me if you have any questions, concerns,\\nor comments. If you have feedback on this book or have more general questions, I\\ncan try my best to help with that. Please email me at tmnield@outlook.com—I look\\nforward to hearing from you.\\nHappy querying!\\nGoing Forward | 99'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 115, 'page_label': '100', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 116, 'page_label': '101', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"APPENDIX A\\nOperators and Functions\\nThis appendix covers common operations and functions used in SQLite as a conve‐\\nnient reference. While this book does not cover all the functionalities of SQLite, the\\nfunctionalities likely to have an immediate need can be found here. The primary goal\\nis to showcase universal SQL concepts that apply to most platforms, not to teach the\\nnuances of the SQLite platform in detail.\\nA comprehensive coverage of SQLite’s features can be found at https://www.sqlite.org/\\ndocs.html.\\nAppendix A1 – Literal Expression Queries\\nY ou can test operators and functions easily without querying any tables at all. Y ou\\nsimply SELECT an expression of literals as in the following query, which will calculate\\na single value of 12:\\nSELECT 5 + 7\\nAny functions and literals, including text strings, can be tested in this manner as well.\\nThis query will check if the word 'TONY' is in the string 'TONY STARK', and it should\\nreturn 1:\\nSELECT INSTR('TONY STARK', 'TONY')\\nThis is a great way to test operators and functions without using any tables. This\\nappendix will show many examples with this approach, and you can use it for your\\nown experimentation.\\n101\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 117, 'page_label': '102', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Appendix A2 – Mathematical Operators\\nSQLite has a small set of basic math operators. More advanced tasks are usually done\\nwith functions, but here are the five core mathematical operators.\\nAssume x = 7 and y = 3\\nOperator Description Example Result\\n+ Adds two numbers x + y 10\\n- Subtracts two numbers x - y 4\\n* Multiplies two numbers x * y 21\\n/ Divides two numbers x / y 2\\n% Divides two numbers, but returns the remainder x % y 1\\nAppendix A3 – Comparison Operators\\nComparison operators yield a true (1) or false (0) value based on a comparative evalu‐\\nation.\\nAssume x = 5 and y = 10\\nOperator Description Example Result\\n= and == Checks if two values are equal x = y 0 (false)\\n!= and <> Checks if two values are not equal x != y 1 (true)\\n> Checks if value on left is greater than value on right x > y 0 (false)\\n< Checks if value on left is less than value on right x < y 1 (true)\\n>= Checks if value on left is greater than or equal to value on right x >= y 0 (false)\\n<= Checks if value on left is less than or equal to value on right x <= y 1 (true)\\nAPPENDIX A4 – Logical Operators\\nLogical operators allow you combine Boolean expressions as well as perform more\\nconditional operations.\\nAssume x = true (1) and y = false (0)\\nAssume a = 4 and b = 10\\nOperator Description Example Result\\nAND Checks for all Boolean expressions to be true x AND y 0 (false)\\nOR Checks for any Boolean expression to be true x OR y 1 (true)\\nBETWEEN Checks if a value inclusively falls inside a range a BETWEEN 1 and b 1 (true)\\n102 | Appendix A: Operators and Functions'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 118, 'page_label': '103', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Operator Description Example Result\\nIN Checks if a value is in a list of values a IN (1,5,6,7) 0 (false)\\nNOT Negates and flips a Boolean expression’s value a NOT IN (1,5,6,7) 1 (true)\\nIS NULL Checks if a value is null a IS NULL 0 (false)\\nIS NOT NULL Checks if a value is not null a IS NOT NULL 1 (true)\\nAPPENDIX A5 – Text Operators\\nText has a limited set of operators, as most text processing tasks are done with func‐\\ntions. There are a few, though. Keep in mind also that regular expressions are beyond\\nthe scope of this book, but they are worth studying if you ever work heavily with text\\npatterns.\\nAssume city = ‘Dallas’ and state = ‘TX’\\nOperator Description Example Result\\n|| Concatenates one or more values together into text city || ', ' || state Dallas, TX\\nLIKE Allows wildcards _ and % to look for text patterns state LIKE 'D_l%' 1 (true)\\nREGEXP Matches a text pattern using a regular expression state REGEXP '[A-Z]{2}' 1 (true)\\nA special note to programmers: REGEXP is not implemented out of\\nthe box for SQLite, so you may have to compile or implement it\\nwhen using SQLite for your app or program.\\nAPPENDIX A6 – Common Core Functions\\nSQLite has many core functions built in. While this is not a comprehensive list, these\\nare the most commonly used ones. A full list of functions and their documentation\\ncan be found at http://www.sqlite.org/lang_corefunc.html.\\nAssume x = –5, y = 2, and z is NULL\\nOperator Description Example Result\\nabs() Calculates the absolute value of a number abs(x) 5\\ncoalesce() Converts a possible null value into a default value if it is null coalesce(z, y) 2\\ninstr() Checks if a text string contains another text string; if so it\\nreturns the index for the found position, and otherwise it\\nreturns 0\\ninstr('HTX','TX') 2\\nlength() Provides the number of characters in a string length('Dallas') 6\\ntrim() Removes extraneous spaces on both sides of a string trim(' TX ') TX\\nOperators and Functions | 103\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 119, 'page_label': '104', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Operator Description Example Result\\nltrim() Removes extraneous spaces on the left side of a string ltrim(' TX') TX\\nrtrim() Removes extraneous spaces on the right side of a string rtrim('LA ') LA\\nrandom() Returns a pseudorandom number from the range\\n–9223372036854775808 to +9223372036854775807\\nrandom() 7328249\\nround() Rounds a decimal to a specified number of decimal places round(182.245, 2) 182.24\\nreplace() Replaces a substring of text in a string with another string replace('Tom \\nNield','Tom',\\n'Thomas')\\nThomas\\nNield\\nsubstr() Extracts a range of characters from a string with their numeric\\npositions\\nsubstr('DOG',2,3) OG\\nlower() Turns all letters in a string to lowercase lower('DoG') dog\\nupper() Turns all letters in a string to uppercase upper('DoG') DOG\\nAPPENDIX A7 – Aggregate Functions\\nSQLite has a set of aggregate functions you can use with a GROUP BY statement to get a\\nscoped aggregation in some form.\\nX = a column you specify the aggregation on\\nFunction Description\\navg(X) Calculates the average of all values in that column (omits null values).\\ncount(X) Counts the number of non-null values in that column.\\ncount(*) Counts the number of records.\\nmax(X) Calculates the maximum value in that column (omits null values).\\nmin(X) Calculates the minimum value in that column (omits null values).\\nsum(X) Calculates the sum of the values in that column (omits null values).\\ngroup_concat(X) Concatenates all non-null values in that column. You can also provide a second argument specifying\\na separator, like commas.\\nAPPENDIX A8 – Date and Time Functions\\nFunctionality for dates and times in SQL varies greatly between database platforms.\\nTherefore, this book does not cover this topic outside this appendix.  Y ou will need to\\nlearn the date/time syntax for your specific database platform. Some platforms, such\\nas MySQL, make working with date/time values very intuitive, while others can be\\nless intuitive.\\n104 | Appendix A: Operators and Functions\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 120, 'page_label': '105', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"For SQLite, date and time functions cannot be comprehensively covered here as that\\nwould be beyond the scope of this book. But the most common date and time tasks\\nwill be covered in this section. Full documentation of SQLite date and time handling\\ncan be found at the SQLite website (http://www.sqlite.org/lang_datefunc.html).\\nDate Functions\\nWhen working with dates, it is simplest to store them in the string format YYYY-\\nMM-DD as most database platforms inherently understand this format (technically\\ncalled the ISO8601 format). A four-digit year comes first, following by a two-digit\\nmonth, and a two-digit day, each separated by a dash (e.g., 2015-06-17). If you format\\nyour date strings like this, you will never have to do any explicit conversions.\\nWhen running a query, any string in the 'YYYY-MM-DD' date format will be inter‐\\npreted as a date. This means you can do chronological tasks like comparing one date\\nto another date:\\nSELECT '2015-05-14' > '2015-01-12'\\nIf you do not use this ISO8601 format, SQLite will compare them as text strings and\\ncheck if the first text comes before the second text alphabetically. This obviously is\\nnot desirable as you want dates to be evaluated, compared, and treated as dates.\\nConveniently, you can get today’s date by passing a 'now' string to the DATE() func‐\\ntion:\\nSELECT DATE('now')\\nSQLite also allows you to pass any number of modifier arguments to the DATE() func‐\\ntion. For instance, you can get yesterday’s date by passing '-1 day' as an argument:\\nSELECT DATE('now','-1 day')\\nY ou can pass also pass a date string to the DATE() function, and add any number of\\nmodifiers to transform the date. This example will add three months and subtract one\\nday from 2015-12-07:\\nSELECT DATE('2015-12-07','+3 month','-1 day')\\nThere are several advanced date transformations you can perform. Refer to the\\nSQLite date functions link at the beginning of this section to get a comprehensive\\noverview of these functionalities.\\nTime Functions\\nTime also has a typical format, which is HH:MM:SS (this also is ISO8601 standard).\\nHH is a two-digit military format of hours, MM is a two-digit format of minutes, and\\nSS is a two-digit format of seconds . The separator is a colon. If you format times like\\nOperators and Functions | 105\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 121, 'page_label': '106', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"this, the strings will always be interpreted as time values. This string represents a time\\nvalue of 4:31 PM and 15 seconds:\\nSELECT '16:31:15'\\nY ou can omit the seconds value if you are not concerned with it. SQLite will infer the\\nseconds value to be 00:\\nSELECT '16:31'\\nJust like with dates, you can do all kinds of operations with times, like comparing one\\ntime value to another:\\nSELECT '16:31' < '08:31'\\nThe 'now' string also works with the TIME() function to get the current time:\\nSELECT TIME('now')\\nAlso like with dates, you can use the TIME() function to perform transformations\\nsuch as adding or subtracting hours, minutes, and seconds:\\nSELECT TIME('16:31','+1 minute')\\nDate/Time Functions\\nY ou can have a date that also has a time value. Reasonably, the standard string format\\nis the date and time formats concatenated together, separated by a space: ‘YYYY-\\nMM-DD HH:MM:SS’ . SQLite will recognize a string in this format to be a date/time\\nvalue:\\nSELECT '2015-12-13 16:04:11'\\nAll the rules from the DATE() and TIME() functions can apply to the DATETIME()\\nfunction. Y ou can use 'now', transformations, and any other chronological opera‐\\ntions we have learned. For instance, you can subtract a day from a date/time value\\nand add three hours:\\nSELECT DATETIME('2015-12-13 16:04:11','-1 day','+3 hour')\\nThere are several other functions and features for working with dates and time in\\nSQLite, including converting dates into alternative formats and compensating for\\ntimes zones. There is also support for Unix time and the Julian day number system.\\nAs said earlier, go to http://www.sqlite.org/lang_datefunc.html to get a comprehensive\\nlist of these functionalities.\\n106 | Appendix A: Operators and Functions\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 122, 'page_label': '107', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='APPENDIX B\\nSupplementary Topics\\nThere are some database and SQL tasks that, at the time of writing, did not fit into the\\nmission of this book. The intention was to get your feet wet with SQL and not inun‐\\ndate you with every functionality available. But there are some topics that arguably do\\nnot fit into the core mission of this book, yet do not deserve to be omitted either.\\nThese are given mention here to help you progress in your proficiency.\\nAPPENDIX B1 – Further Topics of Interest\\nThis is a beginner’s book on SQL. Therefore, the scope and focus is limited to founda‐\\ntional topics. However, if you finished this book and are interested in expanding your\\ndatabase and SQL repertoire, here are some suggested topics you can explore and\\nresearch:\\nTopic Description\\nUNION and UNION \\nALL\\nAppend the results of two or more queries into a single result set.\\nSubqueries Query off other queries just like they were tables.\\nIndexes Improve the SELECT performance of a table with large amounts of data (addressed briefly in\\n“APPENDIX B2 – Improving Performance with Indexes” on page 108 ).\\nTransactions Perform multiple UPDATE/DELETE/INSERT statements as one fail-safe batch (addressed briefly in\\n“Appendix B3 – Transactions” on page 109 ).\\nTriggers React to UPDATE/DELETE/INSERT statements and perform tasks like logging and advanced data\\nvalidation.\\nRegular expressions Use a universal syntax to match advanced text patterns easily—basically, LIKE wildcards on\\nsteroids.\\nDatabase\\nadministration\\nFine-tune production databases for large corporate environments.\\n107'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 123, 'page_label': '108', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Y ou will probably encounter dozens of other topics, especially as you explore the\\nnuances of different database platforms. But these should provide plenty of leads to\\nexpand your database knowledge beyond this book.\\nAPPENDIX B2 – Improving Performance with Indexes\\nAs your database grows, the performance can start to slow down with SELECT queries.\\nThe machine has to process each record to find ones that match your WHERE condi‐\\ntion, and obviously having more records will slow this process down.\\nA common way to improve performance significantly is to use indexes, a mechanism\\nthat enables faster lookups in a way very similar to an index in a book. An index\\nspeeds up SELECT performance, but it slows down INSERT, UPDATE, and DELETE state‐\\nments. It will also make the database file larger. These are factors you have to balance\\nin your decision to use them. Y ou should not think about creating indexes when you\\nfirst design a database. Do it later, when you find you are having performance issues.\\nY ou specify an index on one or more columns, and you want these columns to be the\\nones you frequently qualify on. For example, if you frequently query the PRODUCT\\ntable and use a WHERE statement on the price column, you can apply an index on that\\ncolumn as shown here:\\nCREATE INDEX price_index ON PRODUCT(price);\\nWe name the index price_index and we apply it on the PRODUCT table, and in paren‐\\ntheses we specify it on the price column. SQLite will keep a map of which records\\nhave which price values. This will significantly speed up performance when we qual‐\\nify on price. But obviously, when we modify records it has to update this index, so\\nthis overhead will slow down INSERT, UPDATE, and DELETE operations.\\nY ou will notice in the SQLiteStudio database navigator that the table contains all the\\nindex objects you have created (see Figure B-1).\\n108 | Appendix B: Supplementary Topics'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 124, 'page_label': '109', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure B-1. The price_index was added to the PRODUCT table’s indexes\\nY ou can also create a UNIQUE index for a column that never has duplicate values, and\\nSQLite will make special optimizations for that case:\\nCREATE UNIQUE INDEX name_index ON CUSTOMER(name);\\nIn addition, you can use composite indexes if two or more fields are frequently quali‐\\nfied together, but that is beyond the scope of this book.\\nTo remove an index, just run a DROP INDEX statement on the index’s name:\\nDROP INDEX price_index;\\nAgain, indexes should only be used for very large tables that have noticeable perfor‐\\nmance issues with SELECT statements. Y ou should avoid using indexes on small tables\\nas the overhead will actually slow performance down (meaning this example was\\ndemonstrational, not something you should actually do to the PRODUCT table). Y ou\\nshould also avoid using indexes on tables that update heavily and frequently.\\nAppendix B3 – Transactions\\nThere may be situations where you will want to execute multiple INSERT, UPDATE, or\\nDELETE statements as a batch, but you want all of them to complete successfully, and if\\none fails you want all of them to fail. This is known as atomicity, which means these\\nactions must all happen successfully or none of them happen at all.\\nSupplementary Topics | 109'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 125, 'page_label': '110', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='A good example where this kind of behavior is needed is financial transactions, like\\nbank account transfers or payment services like PayPal. When you take money from\\none account and put it in another, you have to make sure both operations happen\\nsuccessfully.\\nTake these two INSERT statements that move $187.56 from one account and put it in\\nanother:\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (1563,-187.56);\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (3067,187.56);\\nWhat happens if the first INSERT succeeds but the second one fails? Well, that $187.56\\nhas effectively disappeared. Y ou have two upset customers and a possible auditing\\nmess. So how do you ensure that in the event of failure, that money returns back to\\nthe customer giving it and everything is restored to the way it was?\\nThe answer is to leverage a transaction. With a transaction you can make this transfer\\natomic and do a ROLLBACK if anything fails and a COMMIT if everything succeeds.\\nFirst, call the BEGIN or BEGIN TRANSACTION command (these are the same command):\\nBEGIN TRANSACTION;\\nNow any INSERT, UPDATE, and DELETE statements will be recorded so they can be\\nundone if necessary. Perform the two INSERTs. The actions will be performed while\\nbeing recorded in “transaction mode”:\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (1563,-187.56);\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (3067,187.56);\\nIf everything goes well and no errors occurred, you can call COMMIT or its alias, END\\nTRANSACTION, to finalize the INSERTs. The transfer has then happened successfully.\\nNow let’s start another transaction so we can do another transfer:\\nBEGIN TRANSACTION;\\nHowever, this time we are going to break it. Say we do another transfer between these\\ntwo accounts:\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (1563,-121.36);\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (121.36);\\nThe first statement will succeed, but the second SQL statement was messed up and\\nwill error out. It is missing the ACCOUNT_ID value, and now we have $121.36 in limbo.\\nFortunately, we are in “transaction mode. ” We can basically hit a rewind button and\\ncall ROLLBACK. This will undo everything since the last COMMIT or BEGIN TRANSACTION\\n110 | Appendix B: Supplementary Topics'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 126, 'page_label': '111', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='we called. The first INSERT will be undone and the $121.36 will be back in the 1563\\naccount.\\nIn the event that a database connection goes down, a bad SQL statement is composed,\\nor a validation rule train-wrecks a series of updates, transactions are a way to ensure\\ndata does not get corrupted in your database. Y ou will especially want to use them\\nwith automated processes or large jobs requiring you INSERT, UPDATE, and DELETE a\\nhigh volume of fragile records.\\nSupplementary Topics | 111'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 127, 'page_label': '112', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 128, 'page_label': '113', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Index\\nSymbols\\n!= (inequality) operator, 31, 102\\n% (percent sign)\\nmodulus operator, 32\\nwildcard character, 33\\n' ' (quotation marks, single), enclosing literals,\\n32\\n() (parentheses), grouping with, 36\\n; (semicolon), ending SQL statements, 22\\n< (less than) operator, 31, 102\\n<= (less than or equal to) operator, 31, 36, 102\\n<> (inequality) operator, 31, 102\\n= (equals) sign\\n= (equals) operator, 30, 32\\n= and == operators, 102\\n> (greater than) operator, 31, 36, 102\\n>= (greater than or equal to) operator, 31, 102\\n_ (underscore)\\nin SQL names, 25\\nwildcard character, 33\\n|| (concatenation) operator, 27, 103\\nA\\nAccess, 5\\naggregate functions, 42-45, 104\\nnull values and, 43\\naggregating data, 39\\nfiltering different aggregate values using\\nzero/null CASE trick, 49\\nfiltering out records based on aggregated\\nvalue, 45\\naliases, 24\\nin HAVING statement, 45\\nAND statement, 31\\nAS statement, 24\\nASC operator, 42\\natomicity, 109\\nAUTOINCREMENT constraint, 78, 92\\nAVG() function, 43\\nB\\nBEGIN TRANSACTION command, 110\\nBETWEEN statement, 31\\nboolean expressions\\nin CASE statements, 51\\nlogical operators with, 102\\nBoolean values, 83\\nC\\nCASE statement, 47-52\\ngrouping, 48\\nzero/null trick, 49-52\\ncase, SQL commands, 30\\ncentralized databases, 6\\nchild relationships, 54\\nclients, 6\\ncoalesce() function, 65\\ncolumns\\naliasing, 25\\nchoosing for SELECT statement, 21\\nCOMMIT statement, 110\\ncomparison operators, 102\\nconcatenation operator (||), 27\\nconstraints, 77\\nforeign key, setting up, 84\\nCOUNT() function, 39\\ncounting non-null values in a column, 42\\nCREATE INDEX statement, 108\\n113\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 129, 'page_label': '114', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CREATE TABLE statement, 76-84\\nCREATE VIEW AS statement, 87\\nD\\ndata questions (database design), 67\\ndata types, 16\\nDatabase Navigator (SQLiteStudio), 10\\ndatabase schemas, 71\\ndatabases, 3-7\\ncentralized, 6\\nchoosing a database solution, 5\\ncreating a new database, 73-89\\nCREATE TABLE, 76-84\\nselecting a location, 75\\nsetting foreign keys, 84\\nviews, 86\\ndefined, 3\\ndesigning, 67-73\\nexample, SurgeTech conference, 69-70\\nplanning a database, 67\\nprimary and foreign keys, 70\\nschema, 71\\nlightweight, 5\\nrelational databases, exploring, 3\\nseparate tables, reasons for, 4\\ndate and time functions, 104\\nDELETE statement, 94\\nDESC operator, 42\\ndesign questions (databases), 67\\nDISTINCT operator, 46\\nDROP INDEX statement, 109\\nDROP TABLE statement, 95\\nduplicates, eliminating from results, 46\\nE\\neditors (SQL)\\nSQLiteStudio, 9\\nELSE clauses in CASE statements, 47\\nEND keyword, ending CASE statements, 47\\nEND TRANSACTION command, 110\\nexpressions\\nin SELECT statements, 23\\nusing with data types other than numbers,\\n27\\nF\\nforeign keys, 71\\nsetting, 84\\ntesting, 93\\nFROM statement, INNER JOIN in, 56\\nfunctions\\naggregate, 42-45, 45, 104\\nbuilt-in functions in SQLite, 25\\ncommon core functions in SQLite, 103\\ndate and time, 104\\nG\\nGROUP BY operator, 39-41\\naliases and, 45\\ngrouping JOINs, 63\\nORDER BY operator and, 41\\nusing ordinal positions, 41, 49\\ngrouping records, 39\\nH\\nHAVING statement, 45\\nI\\nIN statement, 32\\nusing on text, 33\\nindexes, 108\\nINNER JOIN operator, 55\\nLEFT JOIN and, 65\\nrecords excluded in, 57\\nINSERT statement, 91\\nmultiple INSERTs with one command, 93\\ntesting foreign keys, 93\\ntransaction mode, 110\\nINTEGER type, 16\\nIS NULL operator, 60\\nJ\\nJava, learning more about, 98\\njoins, 53-66\\ngrouping, 63\\nINNER JOIN, 55\\njoining multiple tables, 61\\nleft outer join, 58\\nRIGHT JOIN and OUTER JOIN, 61\\nL\\nLEFT JOIN operator, 58, 64\\nnull values, inclusion of, 65\\nleft outer joins, 58\\nlightweight databases, 5\\nliterals, 32\\n114 | Index'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 130, 'page_label': '115', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='literal expression queries, 101\\nlogical operators, 102\\nM\\nmathematical operators, 26, 102\\nMAX() function, 44, 50\\nMicrosoft Access, 5\\nMIN() function, 50\\nmodulus operator (%), 32\\nMySQL, 7\\nN\\nnames in SQL\\nunderscore(_) in, 25\\nusing aliases, 24\\nnon-null values, counting in a column, 42\\nnormalization, 4\\nNOT IN statement, 32\\nNot NULL constraint, 79\\nnull values\\naggregate functions and, 43\\nand Boolean values in tables, 83\\nconverting to zeros with coalesce(), 65\\nin LEFT JOINs versus INNER JOINs, 65\\nresulting from LEFT JOIN, checking for, 60\\nzero/null CASE trick, 49-52\\nO\\none-to-many relationships, 54\\noperators\\ncomparison, 102\\nconcatenation, 27\\nlogical, 102\\nmathematical, 26, 102\\ntext, 103\\nOR statement, 32\\nORDER BY operator, 41\\norder of operations, 37\\nordinal positions, 41, 49\\nOUTER JOIN operator, 61\\nP\\nparent-child relationships, 54\\nperformance, improving with indexes, 108\\nPRIMARY KEY constraint, 81, 92\\nprimary keys, 70\\ndefining in SQLiteStudio, 78\\nin SQLite, 82\\nprogramming languages, resources for, 98\\nPython, learning more about, 98\\nQ\\nQuery Results pane (SQLiteEditor), 19\\nR\\nR language, learning more about, 98\\nrelational database management system\\n(RDBMS), 3\\nrelational databases, 3\\nrelationships between tables, 54\\nand joining multiple tables, 61\\nin the database schema, 71\\nRIGHT JOIN operator, 61\\nROLLBACK command, 110\\nrolling up data, 39\\nround() function, 26, 43\\nS\\nschemas (database), 71\\nsecurity questions (database design), 68\\nSELECT statement, 19-28\\nDISTINCT operator, 46\\nexecuting before DELETEs, 94\\nexpressions in, 23\\ninserting records using results from, 93\\nliteral expression queries, 101\\nspecifying columns for, 21\\nspreading across multiple lines, 24\\nstoring frequently used SELECT queries in a\\ndatabase, 86\\ntext concatenation in, 27\\nwriting and executing in SQLiteStudio, 20\\nsemicolon (;), ending SQL statements, 22\\nservers, 6\\nSET keyword, 95\\nsoftware development, 98\\nspaces in SQL names, underscore(_) as place‐\\nholder, 25\\nSQL\\ndatabase solutions, 5\\nmarketability of, 1\\nresources for further learning, 97\\nuses of, 2\\nSQL injection, 68\\nSQL Work Area (SQLiteStudio), 10\\nSQLite, 5, 9-17\\nIndex | 115'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 131, 'page_label': '116', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='advantages and limitations of, 9\\naggregate functions, 104\\ncommon core functions, 103\\ndate and time functions, 105\\nfunctions in, 25\\noperators, 101\\nsecurity, 68\\nSQLiteStudio, 9\\ncreating tables, 76-84\\nimporting and navigating databases, 10\\nSQL Editor and Query Results panes, 19\\nSUM() function, 44\\nfiltering on value of, 45\\nsumming CASE statements, 50\\nT\\ntables\\nCREATE TABLE statement, 76-84\\nexamining in SQLiteStudio, 14\\nin relational databases, 3\\nin the database schema, 71\\njoining, 53\\n(see also joins)\\nrelationships between, 54\\nseparation in relational databases, 4\\ntext\\nconcatenating, 27\\noperators for, 103\\nusing WHERE on, 32\\nTEXT type, 16\\nTHEN keyword, 47\\ntime functions, 105\\ntopics of interest, 107\\ntransactions, 109\\nTRUNCATE TABLE statement, 94\\nU\\nunique index, creating, 109\\nUPDATE statement, 95\\nUPPER() function, 95\\nV\\nV ALUES keyword, 91\\nin multiple INSERTs, 93\\nV ARCHAR type, 79\\nviews, creating, 86\\nW\\nwebsites for further learning, 98\\nWHEN keyword, 47\\nWHERE statement, 29-37\\nAND operator, 31\\nchecking for null values from LEFT JOIN,\\n60\\nconditionally applying updates with, 95\\nconditionally deleting records with, 94\\nfiltering aggregations and, 45, 49\\nfiltering records, 29\\ngrouping conditions with parentheses, 37\\nIN operator, 32\\nOR operator, 32\\nORDER BY operator, 41\\nusing on numbers, 30\\nusing on text, 32\\nwhitespace in SQL, 24\\nZ\\nZero/Null CASE trick, 49-52\\n116 | Index'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 132, 'page_label': '117', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='About the Author\\nThomas Nield has a business analyst background and works in revenue management\\nat Southwest Airlines. Early in his career, he became fascinated with technology and\\nbought dozens of books to master programming in Java, C#, and database design. He\\nis passionate about sharing what he learns and enabling others with new skillsets,\\neven if they do not work in IT. He enjoys making technical content relatable and rele‐\\nvant to those unfamiliar with or intimidated by it.\\nColophon\\nThe animal on the cover of Getting Started with SQL  is a Natterjack toad ( Epidalea\\ncalamita). It is part of the Bufonidae family and can be found in sand dune systems,\\nheathlands, and coastlines with low coverings of grass throughout western European.\\nAn identifying feature of natterjack toads is the yellow line that runs down the middle\\nof their backs. Adults range in length from 50–70 mm, with females being larger than\\nmales. Overall coloring is either brown, cream, or green, and they are covered in\\nwarts, like their toad brethren. Another distinguishing feature is its shorter hind legs,\\nwhich make it more of a runner than a hopper or walker, as other toads are.\\nLike more common toads, the natterjack diet consists of many invertebrate insects\\nsuch as beetles, spiders, and worms. They are nocturnal hunters, so meals are swal‐\\nlowed whole at night. Those in sand dune systems are also known to eat small crusta‐\\nceans, such as freshwater shrimp. Natterjacks release toxins from their skin, which\\nmake them unlikely to be made into meals themselves, but birds such as grey herons,\\nas well as grass snakes are able to consume them without issue.\\nAs with hunting, the natterjack’s mating rituals are nocturnal. Males have a dinstinct,\\nloud mating call that signals females to head to nearby warm, shallow waters (because\\nnatterjacks are terrible swimmers). This usually occurs between the months of April\\nand July, with females spawning 1,500–7,500 eggs. The eggs turn into tadpoles about\\na week after fertilization, which then turn into toadlets 3–8 weeks later.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com.\\nThe cover image is from Johnson’s Natural History. The cover fonts are URW Type‐\\nwriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is\\nAdobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab6b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b75a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 133 documents into 237 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Thomas Nield\n",
      " Getting Started with\n",
      "  SQL\n",
      "A HANDS-ON APPROACH  \n",
      "FOR BEGINNERS...\n",
      "Metadata: {'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 0, 'page_label': 'Cover', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 0, 'page_label': 'Cover', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Thomas Nield\\n Getting Started with\\n  SQL\\nA HANDS-ON APPROACH  \\nFOR BEGINNERS'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 2, 'page_label': 'i', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Thomas Nield\\nBoston\\nGetting Started with SQL\\nA Hands-on Approach for Beginners'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 3, 'page_label': 'ii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='978-1-491-93861-4\\n[LSI]\\nGetting Started with SQL\\nby Thomas Nield\\nCopyright © 2016 Thomas Nield. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://safaribooksonline.com). For more information, contact our corporate/\\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\\nEditor: Shannon Cutt\\nProduction Editor: Shiny Kalapurakkel\\nCopyeditor: Jasmine Kwityn\\nProofreader: Rachel Head\\nIndexer: Ellen Troutman-Zaig\\nInterior Designer: David Futato\\nCover Designer: Randy Comer\\nIllustrator: Rebecca Demarest\\nFebruary 2016:  First Edition\\nRevision History for the First Edition\\n2016-02-08: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491938614 for release details.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 3, 'page_label': 'ii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='February 2016:  First Edition\\nRevision History for the First Edition\\n2016-02-08: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491938614 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Getting Started with SQL , the cover\\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 3, 'page_label': 'ii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 4, 'page_label': 'iii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Table of Contents\\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\\n1. Why Learn SQL?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nWhat Is SQL and Why Is It Marketable?                                                                        1\\nWho Is SQL For?                                                                                                                2\\n2. Databases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is a Database?                                                                                                           3'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 4, 'page_label': 'iii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='What Is a Database?                                                                                                           3\\nExploring Relational Databases                                                                                       3\\nWhy Separate Tables?                                                                                                        4\\nChoosing a Database Solution                                                                                         5\\nLightweight Databases                                                                                                       5\\nCentralized Databases                                                                                                       6\\n3. SQLite. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  9'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 4, 'page_label': 'iii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='3. SQLite. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  9\\nWhat Is SQLite?                                                                                                                  9\\nSQLiteStudio                                                                                                                       9\\nImporting and Navigating Databases                                                                            10\\n4. SELECT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19\\nRetrieving Data with SQL                                                                                               19\\nExpressions in SELECT Statements                                                                              23'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 4, 'page_label': 'iii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Expressions in SELECT Statements                                                                              23\\nText Concatenation                                                                                                          27\\nSummary                                                                                                                           28\\niii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 5, 'page_label': 'iv', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='5. WHERE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  29\\nFiltering Records                                                                                                              29\\nUsing WHERE on Numbers                                                                                           30\\nAND, OR, and IN Statements                                                                                        31\\nUsing WHERE on Text                                                                                                   32\\nUsing WHERE on Booleans                                                                                           34\\nHandling NULL                                                                                                               34\\nGrouping Conditions                                                                                                      36'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 5, 'page_label': 'iv', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Grouping Conditions                                                                                                      36\\nSummary                                                                                                                           37\\n6. GROUP BY and ORDER BY. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  39\\nGrouping Records                                                                                                            39\\nOrdering Records                                                                                                             41\\nAggregate Functions                                                                                                        42\\nThe HAVING Statement                                                                                                 45'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 5, 'page_label': 'iv', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='The HAVING Statement                                                                                                 45\\nGetting Distinct Records                                                                                                46\\nSummary                                                                                                                           46\\n7. CASE Statements. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  47\\nThe CASE Statement                                                                                                       47\\nGrouping CASE Statements                                                                                           48\\nThe “Zero/Null” CASE Trick                                                                                          49'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 5, 'page_label': 'iv', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='The “Zero/Null” CASE Trick                                                                                          49\\nSummary                                                                                                                           52\\n8. JOIN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  53\\nStitching Tables Together                                                                                                53\\nINNER JOIN                                                                                                                    55\\nLEFT JOIN                                                                                                                        58\\nOther JOIN Types                                                                                                            61'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 5, 'page_label': 'iv', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Other JOIN Types                                                                                                            61\\nJoining Multiple Tables                                                                                                   61\\nGrouping JOINs                                                                                                               63\\nSummary                                                                                                                           66\\n9. Database Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  67\\nPlanning a Database                                                                                                        67\\nThe SurgeTech Conference                                                                                             69'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 5, 'page_label': 'iv', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='The SurgeTech Conference                                                                                             69\\nATTENDEE                                                                                                                  69\\nCOMPANY                                                                                                                   69\\nPRESENTATION                                                                                                         70\\nROOM                                                                                                                           70\\nPRESENTATION_ATTENDANCE                                                                          70\\nPrimary and Foreign Keys                                                                                              70\\niv | Table of Contents'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 6, 'page_label': 'v', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='The Schema                                                                                                                       71\\nCreating a New Database                                                                                                73\\nCREATE TABLE                                                                                                              76\\nSetting the Foreign Keys                                                                                                 84\\nCreating Views                                                                                                                 86\\nSummary                                                                                                                           89\\n10. Managing Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  91'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 6, 'page_label': 'v', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='10. Managing Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  91\\nINSERT                                                                                                                              91\\nMultiple INSERTs                                                                                                         93\\nTesting the Foreign Keys                                                                                             93\\nDELETE                                                                                                                            94\\nTRUNCATE TABLE                                                                                                        94\\nUPDATE                                                                                                                           95'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 6, 'page_label': 'v', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='UPDATE                                                                                                                           95\\nDROP TABLE                                                                                                                   95\\nSummary                                                                                                                           95\\n11. Going Forward. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  97\\nA. Operators and Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  101\\nB. Supplementary Topics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  107\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  113\\nTable of Contents | v'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 8, 'page_label': 'vii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Foreword\\nOver the past three decades, computers have taken over the world. Twenty-five years\\nago, we lived analog. We communicated using an analog POTS telephone, we tuned\\nin to analog FM radio stations, and we went to the library and browsed the stacks for\\ninformation. Buildings were constructed using hand-drawn blueprints; graphic artists\\nworked with pen, brush, and ink; musicians plucked strings and blew into horns and\\nrecorded on analog tape; and airplanes were controlled by physical cables connecting\\nthe yoke to the control surfaces.\\nBut now everything is computerized and digital. Consequently, every member of\\nsociety needs to be familiar with computers. That does not mean having the deep\\nknowledge of a techie, but just as poets need to study a little math and physics, and\\njust as mathematicians need to read a little poetry, so too does everybody today need\\nto know something about computers.\\nI think that this book really helps to address the knowledge gap between techies and'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 8, 'page_label': 'vii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='to know something about computers.\\nI think that this book really helps to address the knowledge gap between techies and\\nlaypeople, by providing an accessible and easy-to-read discussion of SQL—a core\\ndatabase technology.\\n—Richard Hipp, Creator of SQLite\\nvii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 10, 'page_label': 'ix', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Preface\\nNobody needs to learn how a car engine works in order to drive a car. The whole\\npoint of technologies like SQL is to allow you to focus on the business problem, and\\nnot worry about how the technical details are executed. This book will give you a\\npractical focus on using SQL, and will steer away from unnecessary technical details\\nthat are likely not pertinent to your immediate needs. Much of the content revolves\\naround hands-on exercises with real databases you can download so you see how\\nconcepts are applied. When you finish this book you will have practical knowledge to\\nwork with databases, as well as use them to overcome your business challenges.\\nHow to Use This Book\\nThis book is designed to teach the fundamentals of SQL and working with databases.\\nReaders who have experience using Excel spreadsheets should find this material\\naccessible but still challenging. Individuals who have not worked with Excel may be'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 10, 'page_label': 'ix', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Readers who have experience using Excel spreadsheets should find this material\\naccessible but still challenging. Individuals who have not worked with Excel may be\\nmore challenged. It is helpful to be familiar with concepts used in Excel, such as rows,\\ncolumns, tables, mathematical expressions (e.g., Excel formulas), and aggregate calcu‐\\nlations (e.g., SUM, AVG, MIN, MAX, COUNT). These concepts will still be taught\\nhere, but some practical Excel experience will help expedite understanding.\\nBasic computer literacy is required, and readers should know how to navigate folders\\nand copy/paste files, as well as download and save files from the Web.\\nAs you go through the material, have a computer on hand to practice the examples.\\nWhile some people can learn by just reading, it is best to practice the material at some\\npoint to reinforce the knowledge.\\nProficiency comes through repeated use and practice. In your job, it is likely that you'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 10, 'page_label': 'ix', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='point to reinforce the knowledge.\\nProficiency comes through repeated use and practice. In your job, it is likely that you\\nwill use some SQL functionalities heavily and others not as much. That is OK. It is\\nmore important to become proficient in what your job requires, and consult this\\nbook (or Google) as a reference when you need answers about an unfamiliar topic.\\nix'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 11, 'page_label': 'x', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='When working with technology, you are never expected to know everything. As a\\nmatter of fact, technology topics are so vast in number it would be impossible. So it is\\nhelpful to develop a degree of tunnel vision and learn only enough to fulfill the task at\\nhand. Otherwise, you can get overwhelmed or distracted learning irrelevant topics.\\nHopefully this book will give you a foundation of knowledge, and afterward you can\\ncontinue to learn about topics that are pertinent to you.\\nY ou are always welcome to reach out to me at tmnield@outlook.com, and I will answer\\nany questions to the best of my ability. If you have questions about positioning your\\ncareer with technical skillsets or have a SQL question, I might be able to help. I hope\\nthat this material not only augments your skillset and career opportunities, but also\\nsparks new interests that excite you like it did for me.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 11, 'page_label': 'x', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='sparks new interests that excite you like it did for me.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nThis element signifies a general note.\\nUsing Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/thomasnield/oreilly_getting_started_with_sql.\\nThis book is here to help you get your job done. In general, if example code is offered'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 11, 'page_label': 'x', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='https://github.com/thomasnield/oreilly_getting_started_with_sql.\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nx | Preface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 12, 'page_label': 'xi', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='the code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “Getting Started with SQL by Thomas\\nNield (O’Reilly). Copyright 2016 Thomas Nield, 978-1-4919-3861-4. ”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nSafari® Books Online\\nSafari Books Online is an on-demand digital library that deliv‐\\ners expert content in both book and video form from the'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 12, 'page_label': 'xi', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='above, feel free to contact us at permissions@oreilly.com.\\nSafari® Books Online\\nSafari Books Online is an on-demand digital library that deliv‐\\ners expert content in both book and video form from the\\nworld’s leading authors in technology and business.\\nTechnology professionals, software developers, web designers, and business and crea‐\\ntive professionals use Safari Books Online as their primary resource for research,\\nproblem solving, learning, and certification training.\\nSafari Books Online offers a range of plans and pricing  for enterprise, government,\\neducation, and individuals.\\nMembers have access to thousands of books, training videos, and prepublication\\nmanuscripts in one fully searchable database from publishers like O’Reilly Media,\\nPrentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que,\\nPeachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kauf‐\\nmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 12, 'page_label': 'xi', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Peachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kauf‐\\nmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,\\nMcGraw-Hill, Jones & Bartlett, Course Technology, and hundreds more. For more\\ninformation about Safari Books Online, please visit us online.\\nPreface | xi'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 13, 'page_label': 'xii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='How to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/getting-started-with-sql.\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nAcknowledgments\\nI am blessed to have amazing people surrounding me, and I realize how central they\\nhave been in my life and everything I do. If it was not for them, this book would'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 13, 'page_label': 'xii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Acknowledgments\\nI am blessed to have amazing people surrounding me, and I realize how central they\\nhave been in my life and everything I do. If it was not for them, this book would\\nprobably not have happened.\\nFirst and foremost, I would like to thank my mom and dad. They have given every‐\\nthing to secure my future. I know for a fact that I would not have the opportunities I\\nhave today if it was not for them. My dad worked hard to provide a better education\\nfor my brothers and me, and my mother always pushed me forward, even when I\\nresisted. She taught me to never settle and always struggle through my limits.\\nI cannot express enough gratitude toward my leaders, managers, and colleagues at\\nSouthwest Airlines Revenue Management. Justin Jones and Timothy Keeney have a\\nwarrior spirit and zeal for innovation that few possess. They truly define the leader‐\\nship and spirit of Southwest Airlines, but more importantly they are good guys. They'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 13, 'page_label': 'xii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='warrior spirit and zeal for innovation that few possess. They truly define the leader‐\\nship and spirit of Southwest Airlines, but more importantly they are good guys. They\\nwill always be my friends and they’ve made it hard to imagine a life without South‐\\nwest Airlines.\\nxii | Preface'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 14, 'page_label': 'xiii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Robert Haun, Brice Taylor, and Allison Russell continuously work to make our team\\nthe forefront of innovation and continuously pursue new ideas, and I am blessed to\\nwork in the environment they have helped create. I also have to thank Matt Louis for\\nbringing me on board at Revenue Management, and Steven Barsalou who made me\\nrealize how little I really knew about SQL. Steven is the first person who came to\\nmind when I needed a reviewer for this book, and I am grateful he came on board\\nthis project.\\nThen there is the project team I work with every day: Brian Denholm, Paul Zigler,\\nBridget Green, Todd Randolph, and Chris Solomon. As a team, the feats we pull off\\nnever cease to amaze me. Brian is the kind of project manager that can effectively\\nbridge technology and business jargon together, and he will not hesitate to get his\\nhands dirty with SQL and the occasional code review. I want to give a special thanks\\nto Chris Solomon for helping me with everything I do every day. He not only has a'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 14, 'page_label': 'xiii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='hands dirty with SQL and the occasional code review. I want to give a special thanks\\nto Chris Solomon for helping me with everything I do every day. He not only has a\\nrare talent to absorb high volumes of technical knowledge and maintain it in a busi‐\\nness perspective, but he is also a nice guy that I am privileged to be friends with.\\nChris is always a key player in any project, and I was thrilled when he agreed to\\nreview this book.\\nI cannot forget the great people who worked at Southwest Airlines Ground Ops\\nSafety Regulatory Compliance, including Marc Stank, Reuben Miller, Mary Noel\\nHennes, and everybody else I had the privilege of working with. I interned and con‐\\ntracted with that department a few years back and some of my fondest memories are\\nthere. It was there I discovered my passion for technology, and they provided many\\nopportunities for me to pursue that, whether it was throwing together databases or\\nprototyping an iPad app.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 14, 'page_label': 'xiii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='there. It was there I discovered my passion for technology, and they provided many\\nopportunities for me to pursue that, whether it was throwing together databases or\\nprototyping an iPad app.\\nWhen I announced I was publishing this book I did not expect Richard Hipp, the\\nfounder and creator of SQLite, to reach out to me. Richard graciously stepped up to\\nbe the technical reviewer for this book and it has been a tremendous honor to have\\nhim on board. The technology community continues to amaze me, and the fact\\nRichard Hipp joined this project shows how unique and close-knit the community\\nreally is.\\nShannon Cutt has been my editor at O’Reilly for this book. This is my first book and I\\nwas uncertain what the publishing experience would be like. But Shannon made pub‐\\nlishing such a pleasant experience that I am eager to write again. Thanks Shannon,\\nyou have been awesome!\\nLast but not least, I want to thank Watermark Church and the volunteers at Careers'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 14, 'page_label': 'xiii', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='lishing such a pleasant experience that I am eager to write again. Thanks Shannon,\\nyou have been awesome!\\nLast but not least, I want to thank Watermark Church and the volunteers at Careers\\nin Motion for creating the vehicle that made this book happen. I initially wrote this\\n“book” as a public service to help unemployed professionals in the Dallas area. It was\\nat their encouragement that I decided to publish it, and I want to give a special thanks\\nto Martha Garza for her insistence. I have learned remarkable things can happen\\nwhen you give your time to help others.\\nPreface | xiii'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 16, 'page_label': '1', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 1\\nWhy Learn SQL?\\nWhat Is SQL and Why Is It Marketable?\\nIt is an obvious statement that the business landscape is shifting rapidly. A lot of this\\nis enabled by technology and the explosion of business data. Companies are investing\\nvast amounts of capital to gather and warehouse data. But what many business lead‐\\ners and managers currently struggle with is how to make sense of this data and use it.\\nThis is where SQL, which stands for Structured Query Language, comes in. It provides\\na means to access and manipulate this data in meaningful ways and provide business\\ninsights not possible before.\\nBusinesses are gathering data at exponential rates, and there is an equally growing\\nneed for people who know how to analyze and manage it. Stack Overflow, the most\\nactive programming community in the world, performed a comprehensive survey on\\nits members in 2015. Apple coding was the most in-demand technology and had an'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 16, 'page_label': '1', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='active programming community in the world, performed a comprehensive survey on\\nits members in 2015. Apple coding was the most in-demand technology and had an\\naverage salary nearing six figures. But SQL came in in fifth place, with a salary that\\nwas not far behind. In recent years, data has suddenly become ubiquitous—yet few\\npeople know how to access it meaningfully, which has put SQL talent in high\\ndemand.\\n1'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 17, 'page_label': '2', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Who Is SQL For?\\nOne misperception about SQL is that it is an IT skill and therefore only applicable to\\ntechnology (not business) professionals. In the world as it exists today, this is hardly\\nthe truth. Businesspeople, managers, IT professionals, and engineers can all reap ben‐\\nefits from learning SQL to better position their careers. SQL can open many career\\npaths because it enables individuals to know their businesses better through the data\\nthat is driving them. On the business side, interest in SQL can lead to roles that are\\nanalytical, managerial, strategic, and research- or project-based. On the IT front, it\\ncan lead to roles in database design, database administration, systems engineering, IT\\nproject management, and even software development.\\n2 | Chapter 1: Why Learn SQL?'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 18, 'page_label': '3', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 2\\nDatabases\\nWhat Is a Database?\\nIn the broadest definition, a database is anything that collects and organizes data. A\\nspreadsheet holding customer bookings is a database, and so is a plain-text file con‐\\ntaining flight schedule data. Plain-text data itself can be stored in a variety of formats,\\nincluding XML and CSV .\\nProfessionally, however, when one refers to a “database” they likely are referring to a\\nrelational database management system  (RDBMS). This term may sound technical\\nand intimidating, but an RDBMS is simply a type of database that holds one or more\\ntables that may have relationships to each other.\\nExploring Relational Databases\\nA table should be a familiar concept. It has columns and rows to store data, much like\\na spreadsheet. These tables can have relationships to each other, such as an ORDER\\ntable that refers to a CUSTOMER table for customer information.\\nFor example, suppose we have an ORDER table with a field called CUSTOMER_ID\\n(Figure 2-1).'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 18, 'page_label': '3', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='table that refers to a CUSTOMER table for customer information.\\nFor example, suppose we have an ORDER table with a field called CUSTOMER_ID\\n(Figure 2-1).\\nFigure 2-1. An ORDER table with a CUSTOMER_ID\\n3'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 19, 'page_label': '4', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='We can reasonably expect there to be another table, maybe called CUSTOMER\\n(Figure 2-2), which holds the customer information for each CUSTOMER_ID.\\nFigure 2-2. A CUSTOMER table\\nWhen we go through the ORDER table, we can use the CUSTOMER_ID to look up the cus‐\\ntomer information in the CUSTOMER table. This is the fundamental idea behind a “rela‐\\ntional database, ” where tables may have fields that point to information in other\\ntables. This concept may sound familiar if you’ve used VLOOKUP in Excel to retrieve\\ninformation in one sheet from another sheet in a workbook.\\nWhy Separate Tables?\\nBut why are these tables separated and designed this way? The motivation is normal‐\\nization, which is separating the different types of data into their own tables rather\\nthan putting them in one table. If we had all information in a single table, it would be\\nredundant, bloated, and very difficult to maintain. Imagine if we stored customer'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 19, 'page_label': '4', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='than putting them in one table. If we had all information in a single table, it would be\\nredundant, bloated, and very difficult to maintain. Imagine if we stored customer\\ninformation in the ORDER table. Figure 2-3 shows what it would look like.\\nFigure 2-3. A table that is not normalized\\nNotice that for the Re-Barre Construction orders someone had to populate the cus‐\\ntomer information three times for all three orders (the name, region, street address,\\ncity, state, and zip). This is very redundant, takes up unnecessary storage space, and is\\ndifficult to maintain. Imagine if a customer had an address change and you had to\\nupdate all the orders to reflect that. This is why it is better to separate CUSTOMERS and\\nORDERS into two separate tables. If you need to change a customer’s address, you only\\nneed to change one record in the CUSTOMER table (Figure 2-4).\\n4 | Chapter 2: Databases'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 20, 'page_label': '5', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 2-4. A normalized table\\nWe will explore table relationships again in Chapter 8, and learn how to use the JOIN\\noperator to merge tables in a query so the customer information can be viewed along‐\\nside the order.\\nChoosing a Database Solution\\nRelational databases and SQL are not proprietary. However, there are several compa‐\\nnies and communities that have developed their own relational database software, all\\nof which use tables and leverage SQL. Some database solutions are lightweight and\\nsimple, storing data in a single file accessible to a small number of users. Other data‐\\nbase solutions are massive and run on a server, supporting thousands of users and\\napplications simultaneously. Some database solutions are free and open source, while\\nothers require commercial licenses.\\nFor the sake of practicality, we will divide database solutions into two categories: light‐\\nweight and centralized. These are not necessarily the industry vernacular, but they will\\nhelp clarify the distinction.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 20, 'page_label': '5', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='weight and centralized. These are not necessarily the industry vernacular, but they will\\nhelp clarify the distinction.\\nLightweight Databases\\nIf you are seeking a simple solution for one user or a small number of users (e.g., your\\ncoworkers), a lightweight database is a good place to start. Lightweight databases have\\nlittle to no overhead, meaning they have no servers and are very nimble. Databases\\nare typically stored in a file you can share with others, although it starts to break\\ndown when multiple people make edits to the file simultaneously. When you run into\\nthis problem, you may want to consider migrating to a centralized database.\\nThe two most common lightweight databases are SQLite and Microsoft Access.\\nSQLite is what we will use in this book. It is free, lightweight, and intuitive to use. It is\\nused in most of the devices we touch and can be found in smartphones, satellites, air‐\\ncraft, and car systems. It has virtually no size limitation and is ideal for environments'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 20, 'page_label': '5', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='used in most of the devices we touch and can be found in smartphones, satellites, air‐\\ncraft, and car systems. It has virtually no size limitation and is ideal for environments\\nwhere it is not used by more than one person (or at most a few people). Among many\\nother uses, SQLite is ideal to learn SQL due to its ease of installation and simplicity.\\nMicrosoft Access has been around for a while and is inferior to SQLite in terms of\\nscalability and performance. But it is heavily used in business environments and\\nChoosing a Database Solution | 5'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 21, 'page_label': '6', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='worth being familiar with. It has many visual tools for writing queries without using\\nSQL, as well as visual form designers and macro abilities. There are many jobs avail‐\\nable to take ownership of Microsoft Access databases and maintain them, as well as\\nmigrating them to better database platforms such as MySQL.\\nCentralized Databases\\nIf you expect tens, hundreds, or thousands of users and applications to use a database\\nsimultaneously, lightweight databases are not going to cut it. Y ou need a centralized\\ndatabase that runs on a server and handles a high volume of traffic efficiently. There\\nis a wide array of centralized database solutions to choose from, including the follow‐\\ning:\\n• MySQL\\n• Microsoft SQL Server\\n• Oracle\\n• PostgreSQL\\n• Teradata\\n• IBM DB2\\n• MariaDB\\nY ou can install some of these solutions on any computer and turn that computer into\\na server. Y ou can then connect users’ computers (also known as clients) to the server'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 21, 'page_label': '6', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='• IBM DB2\\n• MariaDB\\nY ou can install some of these solutions on any computer and turn that computer into\\na server. Y ou can then connect users’ computers (also known as clients) to the server\\nso they can access the data. The client can send a SQL statement requesting specific\\ndata, and the server processes the request and returns the answer. This is a classic\\nclient–server setup.  The client requests something, and the server gives it.\\nWhile you can turn any MacBook or cheap PC into a MySQL server, larger traffic vol‐\\numes require more specialized computers (called server computers ) optimized for\\nserver tasks. These are typically maintained by an IT department whose members\\nadministrate and control databases formally deemed critical to the business.\\nDo not be confused by the term “SQL ” being used to brand data‐\\nbase platforms such as MySQL, Microsoft SQL Server, and SQLite.\\nSQL is the universal language to work with data on all these plat‐'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 21, 'page_label': '6', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Do not be confused by the term “SQL ” being used to brand data‐\\nbase platforms such as MySQL, Microsoft SQL Server, and SQLite.\\nSQL is the universal language to work with data on all these plat‐\\nforms. They merely used “SQL ” in their names for marketing.\\nAs you enter a workplace, chances are an existing centralized database might exist\\nwith information you need, and you will need to request access to it. While we will\\nnot be covering centralized databases in this book, the experience between different\\ndatabase solutions should largely be the same. Across all database solutions, you use\\n6 | Chapter 2: Databases'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 22, 'page_label': '7', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SQL to interact with tables in a pretty uniform way, and even the SQL editor tools are\\nsomewhat similar. Each solution may have nuances to its implementation of SQL,\\nsuch as date functionalities, but everything in this book should be universally applica‐\\nble.\\nIf you ever do need to create a centralized database solution, I would highly recom‐\\nmend MySQL. It is open source, free to use, and straightforward to install and set up.\\nIt is used by Facebook, Google, eBay, Twitter, and hundreds of other Silicon Valley\\ncompanies.\\nWith a conceptual understanding of databases, we can now start working with them.\\nAlthough we will use SQLite in this book, keep in mind it uses SQL, so the knowledge\\nyou gain is applicable to all database platforms.\\nChoosing a Database Solution | 7'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 24, 'page_label': '9', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 3\\nSQLite\\nWhat Is SQLite?\\nAs discussed in the previous chapter, there are many places to put data. But often‐\\ntimes we want a quick, easy place to put data without all the hassle of a client–server\\nsetup. We want to store data in a simple file and edit it just as easily as a Word docu‐\\nment. This is an optimal situation to use SQLite.\\nSQLite is the most widely distributed database in the world. It is put on iPhones,\\niPads, Android devices, Windows phones, thermostats, car consoles, satellites, and\\nmany other modern devices that need to store and retrieve data easily. It is used heav‐\\nily in the Windows 10 operating system as well as the Airbus A350 XWB aircraft. It\\nexcels where simplicity and low overhead is needed. It is also great for prototyping\\nbusiness databases.\\nBut every technology has a trade-off. Because it has no server managing access to it, it\\nfails in multiuser environments where multiple people can simultaneously edit the'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 24, 'page_label': '9', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='business databases.\\nBut every technology has a trade-off. Because it has no server managing access to it, it\\nfails in multiuser environments where multiple people can simultaneously edit the\\nSQLite file. Still, for our training purposes, SQLite is perfect.\\nSQLiteStudio\\nThere are many SQL editors you can use to work with a SQLite database. I strongly\\nrecommend using SQLiteStudio, as it is intuitive and makes it easy to explore and\\nmanage a database. We are going to use that application in this book. Y ou can down‐\\nload it at http://sqlitestudio.pl/?act=download. Be sure to choose Windows, Mac, or\\nLinux for your respective OS. Then open the downloaded folder and copy it to a loca‐\\ntion of your choice. No installation is needed. To start SQLiteStudio, double-click\\nSQLiteStudio.exe (Figure 3-1). Y ou can also create a shortcut on your desktop so you\\ncan easily launch the application in the future.\\n9'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 25, 'page_label': '10', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-1. The SQLiteStudio folder\\nNote that SQLiteStudio is an independent, third-party program not associated with\\nSQLite or its developers. SQLite is a database engine built by Richard Hipp and a tal‐\\nented team of programmers. SQLiteStudio merely takes this engine and wraps a nice\\nuser interface around it. Therefore, if you ever have issues with SQLiteStudio, you\\nshould contact the SQLiteStudio team, not the SQLite team.\\nImporting and Navigating Databases\\nWhen you first start SQLiteStudio, you will probably see a dashboard with no content\\n(Figure 3-2).  The left pane is the database navigator, and the gray area on the right is\\nthe SQL work area where you will write SQL against the databases.\\n10 | Chapter 3: SQLite'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 26, 'page_label': '11', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-2. The SQLiteStudio dashboard\\nLet’s get some databases into SQLiteStudio. Some SQLite database samples used in\\nthis book are provided at http://bit.ly/1TLw1Gr.\\nDownload the databases by clicking the Download ZIP button and copy the contents\\nto a folder of your choice. Y ou will probably want to dedicate this folder to all the\\ndatabases you will work with in this book.\\nAfter downloading the databases, navigate in the top menu to Database → Add a\\nDatabase (Figure 3-3).\\nImporting and Navigating Databases | 11'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 27, 'page_label': '12', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-3. Adding a database\\nY ou will come to a dialog box prompting for a database file. Click the yellow folder\\nicon to select a database file and import it (Figure 3-4).\\n12 | Chapter 3: SQLite'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 28, 'page_label': '13', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-4. Opening a database\\nBrowse for the folder with the saved databases, and double-click the rexon_metals.db\\ndatabase file to load it into SQLiteStudio (Figure 3-5).\\nFigure 3-5. Browsing and opening database files\\nImporting and Navigating Databases | 13'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 29, 'page_label': '14', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Y ou will now see rexon_metals added to the database navigator (Figure 3-6). Double-\\nclick on it to see its contents, which include three tables and two views. Take some\\ntime to poke around and explore this database in the navigator.\\nFigure 3-6. Navigating a database\\nNotice you can click the arrows to get more detailed information on different data‐\\nbase objects, such as tables ( Figure 3-7 ). For example, clicking the arrow for the\\nCUSTOMER table can reveal information such as the columns it contains.\\n14 | Chapter 3: SQLite'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 30, 'page_label': '15', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-7. Expanding a table to see columns\\nY ou may be wondering what “views” are. Do not worry about them for now. They are\\nbasically prebuilt SQL queries that are used so frequently, they are conveniently\\nstored in the database.\\nIf you double-click the CUSTOMER table itself, a new window will pop out in the work\\narea holding all kinds of information about the table (Figure 3-8). It initially opens on\\nthe Structure tab, which provides detailed information about each column. At the\\nmoment, the only detail you need to be concerned with is the data type for each col‐\\numn.\\nImporting and Navigating Databases | 15'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 31, 'page_label': '16', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-8. Each column in a table has a data type, such as integer or text\\nThe CUSTOMER_ID and ZIP fields are stored as INTEGER, which is the data type for a\\nwhole (nondecimal) number. This means these fields should only hold INTEGER val‐\\nues. The rest of the columns are stored as TEXT. There are other data types that could\\nbe used, such as DATETIME, BOOLEAN (true/false), and DECIMAL, which are not used in\\nthis particular table.\\nFor now, if you understand the concept of data types, then that is all you need to\\nobserve in the Structure tab. We will explore table design in detail when we create our\\nown tables later.\\nClick the Data tab, and you will actually see the data in the table itself ( Figure 3-9).\\nThere are only five records (or rows) in this table, but SQLite could hold millions if it\\nneeded to. Y ou can also conveniently edit the values in this table (without using SQL)\\nby simply double-clicking and editing a cell, and then clicking the green checkmark\\nto save it.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 31, 'page_label': '16', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='needed to. Y ou can also conveniently edit the values in this table (without using SQL)\\nby simply double-clicking and editing a cell, and then clicking the green checkmark\\nto save it.\\n16 | Chapter 3: SQLite'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 32, 'page_label': '17', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 3-9. The CUSTOMER table\\nTake some time and get familiar with SQLiteStudio. As soon as you are satisfied that\\nyou’ve poked around enough, close all the windows in the work area. Then, in the top\\nmenu, navigate to Tools→Open SQL Editor. While we’ve discovered that SQLiteStu‐\\ndio provides many ways to view and manipulate data without using any SQL, it does\\nnot come close to the flexibility and power that SQL offers.\\nNow that we know our tables and what we are working with, writing SQL will be\\nsomewhat more intuitive. It is difficult to query databases without knowing the tables\\nin them first.\\nImporting and Navigating Databases | 17'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 34, 'page_label': '19', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 4\\nSELECT\\nWhen working with databases and SQL, the most common task is to request data\\nfrom one or more tables and display it. The SELECT statement accomplishes this. But\\nthe SELECT can do far more than simply retrieve and display data. As we will learn in\\ncoming chapters, we can transform this data in meaningful ways and build powerful\\nsummaries from millions of records.\\nBut first, we will learn how to SELECT columns from a single table as well as compose\\nexpressions in them.\\nRetrieving Data with SQL\\nIf you have not done so already, click on Tools→Open SQL Editor in the top menu,\\nand make sure the rexon_metals database is open, as mentioned in the previous\\nchapter. Y our SQLiteStudio workspace should look something like Figure 4-1. Notice\\nthat the SQL workspace is now divided into two panes, a SQL Editor pane and a \\nQuery Results pane.\\n19'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 35, 'page_label': '20', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 4-1. The SQL workspace\\nThe SQL Editor pane is where you will write your SQL, and the Query Results pane\\nwill display the results of your SQL.\\nLet’s write our first SQL statement. The most common SQL operation is a SELECT\\nstatement, which pulls data from a table and then displays the results. Click on the\\nSQL Editor pane and write the following statement:\\nSELECT * FROM CUSTOMER;\\nClick the blue triangle button or hit F9 to execute the SQL.\\nY ou just ran your first query, and the results should be displayed in the bottom pane\\n(Figure 4-2).\\n20 | Chapter 4: SELECT'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 36, 'page_label': '21', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 4-2. Running a SELECT query\\nLet’s break down exactly what happened. A SELECT statement allows you to choose\\nwhich columns to pull from a table. So the first part of the SQL shown here should be\\nread as “Select all columns, ” where * is a placeholder to specify all columns:\\nSELECT * FROM CUSTOMER;\\nAnd you are getting these columns from the CUSTOMER table:\\nSELECT * FROM CUSTOMER;\\nWhen you execute this SELECT statement, it brings back all the columns from the\\nCUSTOMER table and displays them to you (Figure 4-3).\\nRetrieving Data with SQL | 21'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 37, 'page_label': '22', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 4-3. Selecting all records from the CUSTOMER table\\nY ou do not have to pull all columns in a SELECT statement. Y ou can also pick and\\nchoose only the columns you are interested in. The following query will only pull the\\nCUSTOMER_ID and NAME columns:\\nSELECT CUSTOMER_ID, NAME FROM CUSTOMER;\\nAnd the output will only display those two columns (Figure 4-4).\\nFigure 4-4. Selecting only two columns from a table\\nA single SQL statement can optionally end with a semicolon ( ;), as\\nshown in the previous examples. However, the semicolon is neces‐\\nsary to run multiple SQL statements at once, which is helpful when\\nwriting data, as covered in Chapter 10.\\nBeing able to pick and choose columns may not seem interesting at the moment, but\\nit allows us to hone in on what we are interested in. Reducing scope to just certain\\ncolumns will assist with GROUP BY aggregation tasks as well, as we’ll see in Chapter 6.\\n22 | Chapter 4: SELECT'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 38, 'page_label': '23', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Expressions in SELECT Statements\\nThe SELECT statement can do far more than simply select columns. Y ou can also do\\ncalculations on one or more columns and include them in your query result.\\nLet’s work with another table called PRODUCT. First, do a SELECT all to see the data\\n(Figure 4-5):\\nSELECT * FROM PRODUCT;\\nFigure 4-5. The PRODUCT table\\nSuppose we wanted to generate a calculated column called TAXED_PRICE that is 7%\\nhigher than PRICE. We could use a SELECT query to dynamically calculate this for us\\n(Figure 4-6):\\nSELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE,\\nPRICE * 1.07 AS TAXED_PRICE\\nFROM PRODUCT;\\nExpressions in SELECT Statements | 23'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 39, 'page_label': '24', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 4-6. Using expressions to calculate a TAXED_PRICE column\\nNotice in the SELECT statement that we can spread our SQL across\\nmultiple lines to make it more legible. The software will ignore\\nextraneous whitespace and separate lines, so we can use them to\\nmake our SQL easier to read.\\nNotice how the TAXED_PRICE column was dynamically calculated in the SELECT query.\\nThis column is not stored in the table, but rather calculated and displayed to us every\\ntime we run this query. This is a powerful feature of SQL, which allows us to keep the\\nstored data simple and use queries to layer calculations on top of it.\\nLet’s take a look at our TAXED_PRICE column and break down how it was created. We\\nfirst see the PRICE is multiplied by 1.07 to calculate the taxed amount. We generate\\nthis TAXED_PRICE value for every record:\\nSELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE,\\nPRICE * 1.07 AS TAXED_PRICE\\nFROM PRODUCT\\nNotice too that we gave this calculated value a name using an AS statement (this is'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 39, 'page_label': '24', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE,\\nPRICE * 1.07 AS TAXED_PRICE\\nFROM PRODUCT\\nNotice too that we gave this calculated value a name using an AS statement (this is\\nknown as an alias):\\n24 | Chapter 4: SELECT'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 40, 'page_label': '25', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE,\\nPRICE * 1.07 AS TAXED_PRICE\\nFROM PRODUCT\\nWe can use aliases to give names to expressions. We can also use aliases to apply a\\nnew name to an existing column within the query. For example, we can alias the\\nPRICE column to UNTAXED_PRICE (Figure 4-7 ). This does not actually change the\\nname of the column in the table, but it gives it a new name within the scope of our\\nSELECT statement:\\nSELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE AS UNTAXED_PRICE,\\nPRICE * 1.07 AS TAXED_PRICE\\nFROM PRODUCT\\nFigure 4-7. Aliasing the PRICE column to UNTAXED_PRICE\\nWhen giving names to anything in SQL (whether it is an alias, a\\ncolumn name, a table name, or any other entity), always use an\\nunderscore (_) as a placeholder for spaces. Y ou will run into errors\\notherwise.\\nIf we were to distribute the results of this SQL statement as a report to our workplace,\\nwe would probably want to touch up the rounding on the TAXED_PRICE. Having more'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 40, 'page_label': '25', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='otherwise.\\nIf we were to distribute the results of this SQL statement as a report to our workplace,\\nwe would probably want to touch up the rounding on the TAXED_PRICE. Having more\\nthan two decimal places may not be desirable. Every database platform has built-in\\nExpressions in SELECT Statements | 25'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 41, 'page_label': '26', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='functions to assist with these kinds of operations, and SQLite provides a round()\\nfunction that accepts two arguments in parentheses separated by a comma: the num‐\\nber to be rounded, and the number of decimal places to round to. To round the\\nTAXED_PRICE to two decimal places, we can pass the multiplication expression PRICE\\n* 1.07 as the first argument, and a 2 as the second:\\nSELECT\\nPRODUCT_ID,\\nDESCRIPTION,\\nPRICE,\\nround(PRICE * 1.07, 2) AS TAXED_PRICE\\nFROM PRODUCT;\\nRun the statement and you will notice it rounds the TAXED_PRICE, which displays\\nmuch more nicely with two decimal places (Figure 4-8).\\nFigure 4-8. Using the round() function to limit decimal places for TAXED_PRICE\\nHere is a short summary of the mathematical operators you  can use in SQL (we will\\nsee these used throughout the book):\\nOperator Description Example\\n+ Adds two numbers STOCK + NEW_SHIPMENT\\n- Subtracts two numbers STOCK - DEFECTS\\n* Multiplies two numbers PRICE * 1.07\\n/ Divides two numbers STOCK / PALLET_SIZE'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 41, 'page_label': '26', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Operator Description Example\\n+ Adds two numbers STOCK + NEW_SHIPMENT\\n- Subtracts two numbers STOCK - DEFECTS\\n* Multiplies two numbers PRICE * 1.07\\n/ Divides two numbers STOCK / PALLET_SIZE\\n% Divides two numbers, but returns the remainder STOCK % PALLET_SIZE\\n26 | Chapter 4: SELECT'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 42, 'page_label': '27', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Text Concatenation\\nExpressions do not have to work only with numbers. Y ou can also use expressions\\nwith text and other data types. A helpful operator to use with text is concatenation,\\nwhich merges two or more pieces of data together.    The concatenate operator is\\nspecified by a double pipe ( ||), and you put the data values to concatenate on both\\nsides of it.\\nFor instance, you can concatenate the CITY and STATE fields from the CUSTOMER table\\nas well as put a comma and space between them to create a LOCATION value\\n(Figure 4-9):\\nSELECT NAME,\\nCITY || ', ' || STATE AS LOCATION\\nFROM CUSTOMER;\\nFigure 4-9. Concatenating CITY and STATE\\nY ou can even concatenate several fields into a single SHIP_ADDRESS value\\n(Figure 4-10):\\nSELECT NAME,\\nSTREET_ADDRESS || ' ' || CITY || ', ' || STATE || ' ' || ZIP AS SHIP_ADDRESS\\nFROM CUSTOMER;\\nFigure 4-10. Concatenating several fields to create a SHIP_ADDRESS\\nText Concatenation | 27\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 43, 'page_label': '28', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Concatenation should work with any data type (numbers, dates, etc.) and treat it as\\ntext when merging. The ZIP field shown here is a number, but it was implicitly con‐\\nverted to text during concatenation.\\nMore text operations will be covered in the next chapter, but concatenation is defi‐\\nnitely an important one.\\nMany database platforms use double pipes ( ||) to concatenate, but\\nMySQL and some others require using a CONCAT() function.\\nSummary\\nIn this chapter, we covered how to use the SELECT statement, the most common SQL\\noperation. It retrieves and transforms data from a table without affecting the table\\nitself. We also learned how to select columns and write expressions. Within expres‐\\nsions, we can use operators and functions to do tasks such as rounding, math, and\\nconcatenation.\\nIn the next chapter, we will learn about the WHERE statement, which will allow us to\\nfilter records based on criteria we specify.\\n28 | Chapter 4: SELECT'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 44, 'page_label': '29', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 5\\nWHERE\\nOver the next few chapters, we will be adding more functionalities to the SELECT\\nstatement. A very common task when working with data is filtering for records based\\non criteria, which can be done with a WHERE statement.\\nWe will be learning more functions and using them in the WHERE clause, but we can\\nalso use them in SELECT statements, as discussed in the previous chapter. For the most\\npart, expressions and functions can be used in any part of a SQL statement.\\nFiltering Records\\nWe are going to open another database called weather_stations. Add this database\\nto your database navigator (refer to Chapter 3  if you’ve forgotten how to do this).\\nDouble-click on the database and you will see there is a single table called STA\\nTION_DATA. This contains weather-related sample data gathered from various weather\\nstations.\\nExecute a SELECT on all columns to see the data inside:\\nSELECT * FROM station_data;'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 44, 'page_label': '29', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='TION_DATA. This contains weather-related sample data gathered from various weather\\nstations.\\nExecute a SELECT on all columns to see the data inside:\\nSELECT * FROM station_data;\\nThere is a lot of data here: about 28,000 records ( Figure 5-1 ). We are not going to\\nglean a lot of interesting information by scrolling through these records one by one.\\nWe will need to learn some more SQL features to morph this data into something\\nmeaningful. We will start by learning the WHERE statement, which we can use to filter\\ndown records based on a criterion.\\n29'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 45, 'page_label': '30', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 5-1. The weather_stations database\\nTable names and columns can be defined in uppercase or lower‐\\ncase. SQL commands such as SELECT, FROM, and WHERE can be\\nuppercase or lowercase as well.\\nUsing WHERE on Numbers\\nLet’s say we are interested in station_data records for only the year 2010. Using a\\nWHERE is pretty straightforward for a simple criterion like this.  With this query, you\\nshould only get back records where the year field equals 2010 (Figure 5-2):\\nSELECT * FROM station_data\\nWHERE year = 2010;\\n30 | Chapter 5: WHERE'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 46, 'page_label': '31', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 5-2. Records for the year 2010\\nConversely, you can use != or <> to get everything but 2010. For example:\\nSELECT * FROM station_data\\nWHERE year != 2010\\nOr:\\nSELECT * FROM station_data\\nWHERE year <> 2010\\nThese two syntaxes do the same thing. SQLite and most platforms now support both.\\nHowever, Microsoft Access and IBM DB2 only support <>.\\nWe can also qualify inclusive ranges using a BETWEEN statement, as shown here\\n(“inclusive” means that 2005 and 2010 are included in the range):\\nSELECT * FROM station_data\\nWHERE year BETWEEN 2005 and 2010\\nAND, OR, and IN Statements\\nA BETWEEN can alternatively be expressed using greater than or equal to and less than\\nor equal to expressions and an AND statement. It is a little more verbose, but it demon‐\\nstrates we can use two conditions with an AND. In this case, the year must be greater\\nthan or equal to 2005 and less than or equal to 2010:\\nSELECT * FROM station_data\\nWHERE year >= 2005 AND year <= 2010'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 46, 'page_label': '31', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='than or equal to 2005 and less than or equal to 2010:\\nSELECT * FROM station_data\\nWHERE year >= 2005 AND year <= 2010\\nIf we wanted everything between 2005 and 2010 exclusively—i.e., not including those\\ntwo years—we would just get rid of the = characters. Only 2006, 2007, 2008, and 2009\\nwould then qualify:\\nAND, OR, and IN Statements | 31'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 47, 'page_label': '32', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT * FROM station_data\\nWHERE year > 2005 AND year < 2010\\nWe also have the option of using OR. In an OR statement, at least one of the criteria\\nmust be true for the record to qualify. If we wanted only records with months 3, 6, 9,\\nor 12, we could use an OR to accomplish that:\\nSELECT * FROM station_data\\nWHERE MONTH = 3\\nOR MONTH = 6\\nOR MONTH = 9\\nOR MONTH = 12\\nThis is a little verbose. A more efficient way to accomplish this is using an IN state‐\\nment to provide valid list of values:\\nSELECT * FROM station_data\\nWHERE MONTH IN (3,6,9,12)\\nIf we wanted everything except 3, 6, 9, and 12, we could use NOT IN:\\nSELECT * FROM station_data\\nWHERE MONTH NOT IN (3,6,9,12)\\nY ou can use other math expressions in your WHERE statements too. Earlier, we were\\ntrying to filter on months 3, 6, 9, and 12. If you wanted only months divisible by 3\\n(the “quarter” months), you could use the modulus operator (%). The modulus is simi‐'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 47, 'page_label': '32', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='trying to filter on months 3, 6, 9, and 12. If you wanted only months divisible by 3\\n(the “quarter” months), you could use the modulus operator (%). The modulus is simi‐\\nlar to the division operator (/), but it returns the remainder instead of the quotient. A\\nremainder of 0 means there is no remainder at all, so you can leverage this logic by\\nlooking for a remainder of 0 with modulus 3.\\nIn English, this means “give me all months where dividing by 3 gives me a remainder\\nof 0”:\\nSELECT * FROM station_data\\nWHERE MONTH % 3 = 0\\nOracle does not support the modulus operator. It instead uses the\\nMOD() function.\\nUsing WHERE on Text\\nWe’ve covered several examples of how to qualify number fields in WHERE statements. \\nThe rules for qualifying text fields follow the same structure, although there are sub‐\\ntle differences. Y ou can use =, AND, OR, and IN statements with text. However, when\\nusing text, you must wrap literals (or text values you specify) in single quotes. For'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 47, 'page_label': '32', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='tle differences. Y ou can use =, AND, OR, and IN statements with text. However, when\\nusing text, you must wrap literals (or text values you specify) in single quotes. For\\nexample, if you wanted to filter for a specific report_code, you could run this query:\\n32 | Chapter 5: WHERE'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 48, 'page_label': '33', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"SELECT * FROM station_data\\nWHERE report_code = '513A63'\\nNotice that because the report_code field is text (not a number), we need to put sin‐\\ngle quotes around '513A63' to qualify it. If we do not do this, the SQL software will\\nget confused and think 513A63 is a column rather than a text value. This will cause an\\nerror and the query will fail.\\nThis single-quote rule applies to all text operations, including this IN operation:\\nSELECT * FROM station_data\\nWHERE report_code IN ('513A63','1F8A7B','EF616A')\\nThere are other helpful text operations and functions you can use in WHERE and\\nSELECT statements. For example, the length() function will count the number of\\ncharacters for a given value. So, if we were assigned quality control and needed to\\nensure every report_code was six characters in length, we would want to make sure\\nno records come back when running this query:\\nSELECT * FROM station_data\\nWHERE length(report_code) != 6\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 48, 'page_label': '33', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"ensure every report_code was six characters in length, we would want to make sure\\nno records come back when running this query:\\nSELECT * FROM station_data\\nWHERE length(report_code) != 6\\nAnother common operation is to use wildcards with a LIKE expression, where % is\\nany number of characters and _ is any single character. Any other character is inter‐\\npreted literally. So, if you wanted to find all report codes that start with the letter “ A, ”\\nyou would run this statement to find “ A ” followed by any characters:\\nSELECT * FROM station_data\\nWHERE report_code LIKE 'A%'\\nIf you wanted to find all report codes that have a “B” as the first character and a “C”\\nas the third character, you would specify an underscore ( _) for the second position,\\nand follow with any number of characters after the “C”:\\nSELECT * FROM station_data\\nWHERE report_code LIKE 'B_C%'\\nDo not be confused by the % being used for two different purposes.\\nEarlier we used it to perform a modulus operation, but in a LIKE\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 48, 'page_label': '33', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"SELECT * FROM station_data\\nWHERE report_code LIKE 'B_C%'\\nDo not be confused by the % being used for two different purposes.\\nEarlier we used it to perform a modulus operation, but in a LIKE\\nstatement it is a wildcard in a text pattern. Like some other symbols\\nand characters in SQL, the context in which it is used defines its\\nfunctionality.\\nThere are many other handy text functions, such as INSTR, SUBSTR, and REPLACE. In\\nthe interest of brevity, we will stop covering text functions here, but you can refer to\\n“ APPENDIX A6 – Common Core Functions” on page 103 and “ APPENDIX A8 –\\nDate and Time Functions” on page 104 for more coverage on these functions.\\nUsing WHERE on Text | 33\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 49, 'page_label': '34', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Text functions such as LIKE, SUBSTR, and INSTR can start to become\\ntedious and verbose when qualifying complex text patterns. I\\nhighly recommend researching regular expressions when you find\\nyourself experiencing this. They are not beginner material, but they\\nare handy once you hit that intermediate need.\\nUsing WHERE on Booleans\\nBooleans are true/false values. In the database world, typically false is expressed as 0\\nand true is expressed as 1. Some database platforms (like MySQL) allow you to\\nimplicitly use the words true and false to qualify, as shown here:\\nSELECT * FROM station_data\\nWHERE tornado = true AND hail = true;\\nSQLite, however, does not support this. It expects you to explicitly use 1 for true and\\n0 for false. If you wanted all records where there was tornado and hail, you would run\\nthis statement:\\nSELECT * FROM station_data\\nWHERE tornado = 1 AND hail = 1;\\nIf you are looking for just true values, you do not even have to use the = 1 expression.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 49, 'page_label': '34', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='this statement:\\nSELECT * FROM station_data\\nWHERE tornado = 1 AND hail = 1;\\nIf you are looking for just true values, you do not even have to use the = 1 expression.\\nBecause the fields are already Boolean (behind the scenes, every WHERE condition\\nboils down to a Boolean expression), they inherently qualify by themselves. Hence,\\nyou can achieve the same results by running the following query:\\nSELECT * FROM station_data\\nWHERE tornado AND hail;\\nHowever, qualifying for false conditions needs to be explicit. To get all records with\\nno tornado but with hail, run this query:\\nSELECT * FROM station_data\\nWHERE tornado = 0 AND hail = 1;\\nY ou can also use the NOT keyword to qualify tornado as false:\\nSELECT * FROM station_data\\nWHERE NOT tornado AND hail;\\nHandling NULL\\nY ou may have noticed that some columns, such as station_pressure and\\nsnow_depth, have null values (Figure 5-3). A null is a value that has no value. It is the'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 49, 'page_label': '34', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Handling NULL\\nY ou may have noticed that some columns, such as station_pressure and\\nsnow_depth, have null values (Figure 5-3). A null is a value that has no value. It is the\\ncomplete absence of any content. It is a vacuous state. In layman’s terms, it is blank.\\n34 | Chapter 5: WHERE'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 50, 'page_label': '35', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 5-3. The station_data table has NULL values\\nNull values cannot be determined with an =. Y ou need to use the IS NULL or IS NOT\\nNULL statements to identify null values. So, to get all records with no recorded\\nsnow_depth, you could run this query:\\nSELECT * FROM station_data\\nWHERE snow_depth IS NULL;\\nOften, null values are not desirable to have. The station_number column should be\\ndesigned so it never allows nulls, or else we could have orphan data that belongs to no\\nstation. It might make sense to have null values for snow_depth or precipitation,\\nthough, not because it was a sunny day (in this case, it is better to record the values as\\n0), but rather because some stations might not have the necessary instruments to take\\nthose measurements. It might be misleading to set those values to 0 (which implies\\ndata was recorded), so those measurements should be left null.\\nThis shows that nulls can be ambiguous and it can be difficult to determine their'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 50, 'page_label': '35', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"data was recorded), so those measurements should be left null.\\nThis shows that nulls can be ambiguous and it can be difficult to determine their\\nbusiness meaning. It is important that nullable columns (columns that are allowed to\\nhave null values) have documented what a null value means from a business perspec‐\\ntive. Otherwise, nulls should be banned from those table columns.\\nDo not confuse nulls with empty text, which is two single quotes\\nwith nothing in them (i.e., ''). This also applies to whitespace text\\n(i.e., ' '). These will be treated as values and never will be consid‐\\nered null. A null is definitely not the same as 0 either, because 0 is a\\nvalue, whereas null is an absence of a value.\\nNulls can be very annoying to handle when composing WHERE statements. If you\\nwanted to query all records where precipitation is less than 0.5, you could write\\nthis statement:\\nSELECT * FROM station_data\\nWHERE precipitation <= 0.5;\\nHandling NULL | 35\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 51, 'page_label': '36', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='But have you considered the null values? What if for this query you wanted nulls to\\nbe included? Because null is not 0 or any number, it will not qualify. Nulls rarely qual‐\\nify with anything and almost always get filtered out in a WHERE unless you explicitly\\nhandle them. So you have to use an OR to include nulls:\\nSELECT * FROM station_data\\nWHERE precipitation IS NULL OR precipitation <= 0.5;\\nA more elegant way of handling null values is to use the coalesce() function, which\\nwill turn a possibly null value into a specified default value if it is null. Otherwise, it\\nwill leave the value as is. The first argument is the possibly null value, and the second\\nis the value to use if it is null. So if we wanted all nulls to be treated as 0 within our\\ncondition, we could coalesce() the precipitation field to convert null to 0:\\nSELECT * FROM station_data\\nWHERE coalesce(precipitation, 0) <= 0.5;\\nLike any function, a coalesce() can be used in the SELECT statement too, and not just'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 51, 'page_label': '36', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT * FROM station_data\\nWHERE coalesce(precipitation, 0) <= 0.5;\\nLike any function, a coalesce() can be used in the SELECT statement too, and not just\\nthe WHERE. This is helpful if you want to pretty up a report and not have null values\\ndisplayed, but rather have some placeholder—for example, 0, “N/A ” or “None”—\\nwhich is more meaningful to most people:\\nSELECT report_code, coalesce(precipitation, 0) as rainfall\\nFROM station_data;\\nGrouping Conditions\\nWhen you start chaining AND and OR together, it is good to group them deliberately.\\nY ou need to make sure that you organize each set of conditions between each OR in a\\nway that groups related conditions. Say you were looking for sleet or snow condi‐\\ntions. For sleet to happen, there must be rain and a temperature less than or equal to\\n32 degrees Fahrenheit. Y ou can test for that sleet condition or a snow depth greater\\nthan 0, as shown here:\\nSELECT * FROM station_data\\nWHERE rain = 1 AND temperature <= 32\\nOR snow_depth > 0;'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 51, 'page_label': '36', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='32 degrees Fahrenheit. Y ou can test for that sleet condition or a snow depth greater\\nthan 0, as shown here:\\nSELECT * FROM station_data\\nWHERE rain = 1 AND temperature <= 32\\nOR snow_depth > 0;\\nBut there is one possible problem here. While this technically works, there is a degree\\nof ambiguity that we were lucky SQLite interpreted correctly. The reason is due to the\\nunclear question of “What conditions belong to the AND and what conditions belong\\nto the OR?” The SQL interpreter could derail quickly and incorrectly interpret that we\\nare looking for rain AND another condition where either the temperature is below 32\\nOR the snow depth is greater than 0. The semantics are not clear, and in more compli‐\\ncated SQL this could confuse not only people but also the machine.\\nThis is why it is better to explicitly group conditions in parentheses:\\n36 | Chapter 5: WHERE'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 52, 'page_label': '37', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT * FROM station_data\\nWHERE (rain = 1 AND temperature <= 32)\\nOR snow_depth > 0\\nHere, we group up the sleet expression within parentheses so it is calculated as a sin‐\\ngle unit, and temperature is not mixed up with the OR operator and accidentally\\nmangled with the snow_depth. Grouping with parentheses in WHERE statements not\\nonly makes the semantics clearer, but also the execution safer. This is much like the\\norder of operations (PEMDAS) you probably remember from your middle school\\nmath days. Anything in parentheses gets calculated first. When you start writing\\ncomplicated WHERE conditions, this practice becomes even more critical.\\nSummary\\nIn this chapter, we learned to effectively filter out records in a SELECT statement using\\na WHERE clause. We also leveraged new functions and expression operators that can be\\nused in almost any part of the SQL statement. Finally, we covered how to deliberately\\nand safely chain multiple conditions together in a single WHERE statement.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 52, 'page_label': '37', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='used in almost any part of the SQL statement. Finally, we covered how to deliberately\\nand safely chain multiple conditions together in a single WHERE statement.\\nHopefully SQL is already proving to be useful. Y ou can quickly and easily filter data\\non very explicit conditions, in a way that is difficult to achieve in everyday tools like\\nExcel.\\nDespite all we have covered, we have only just gotten started. The next chapter will\\ncover aggregating data, which will add even more value to your SQL repertoire. It is\\none thing to narrow down your records and filter on specific criteria. It is another to\\ncrunch down millions of records into a few that summarize the data.\\nWe covered a few functions in this chapter, but there are dozens\\nmore that you can research and use as needed. We will learn a few\\nmore throughout this book. “ APPENDIX A6 – Common Core\\nFunctions” on page 103 and “ APPENDIX A8 – Date and Time\\nFunctions” on page 104 cover several more of these functions, and'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 52, 'page_label': '37', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='more throughout this book. “ APPENDIX A6 – Common Core\\nFunctions” on page 103 and “ APPENDIX A8 – Date and Time\\nFunctions” on page 104 cover several more of these functions, and\\nyou can always see a full list of SQLite functions at https://\\nwww.sqlite.org/lang_corefunc.html.\\nSummary | 37'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 54, 'page_label': '39', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 6\\nGROUP BY and ORDER BY\\nAggregating data (also referred to as rolling up, summarizing, or grouping data) is\\ncreating some sort of total from a number of records. Sum, min, max, count, and\\naverage are common aggregate operations. In SQL you can group these totals on any\\nspecified columns, allowing you to control the scope of these aggregations easily.\\nGrouping Records\\nFirst, perform the simplest aggregation: count the number of records in a table. Open\\nthe SQL editor and get a count of records for station data:\\nSELECT COUNT(*) AS record_count FROM station_data;\\nThe COUNT(*) means to count the records. We can also use this in combination with\\nother SQL operations, like WHERE. To count the number of records where a tornado\\nwas present, input the following:\\nSELECT COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1;\\nWe identified 3,000 records with tornadoes present. But what if we wanted to separate\\nthe count by year (Figure 6-1)? We can do that too with this query:'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 54, 'page_label': '39', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='WHERE tornado = 1;\\nWe identified 3,000 records with tornadoes present. But what if we wanted to separate\\nthe count by year (Figure 6-1)? We can do that too with this query:\\nSELECT year, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year;\\n39'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 55, 'page_label': '40', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 6-1. Getting a tornado count by year\\nThis data suddenly becomes more meaningful. We now see the tornado sighting\\ncount by year. Let’s break down this query to see how this happened.\\nFirst, we select the year, then we select the COUNT(*) from the records, and we filter\\nonly for records where tornado is true:\\nSELECT year, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year;\\nHowever, we also specify that we are grouping on year. This is what effectively allows\\nus to count the number of records by year. The last line, highlighted in bold, performs\\nthis grouping:\\nSELECT year, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year;\\nWe can slice this data on more than one field. If we wanted a count by year and\\nmonth, we could group on the month field as well (Figure 6-2):\\nSELECT year, month, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year, month\\n40 | Chapter 6: GROUP BY and ORDER BY'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 56, 'page_label': '41', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 6-2. Tornado count by year and month\\nAlternatively, we can use ordinal positions instead of specifying the columns in the\\nGROUP BY. The ordinal positions correspond to each item’s numeric position in the\\nSELECT statement. So, instead of writing GROUP BY year, month , we could instead\\nmake it GROUP BY 1, 2  (which is especially helpful if our SELECT has long column\\nnames or expressions, and we do not want to rewrite them in the GROUP BY):\\nSELECT year, month, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY 1, 2\\nNote that not all platforms support ordinal positions. With Oracle and SQL Server,\\nfor example, you will have to rewrite the entire column name or expression in the\\nGROUP BY. \\nOrdering Records\\nNotice that the month column is not in a natural sort we would expect. This is a good\\ntime to  bring up the ORDER BY operator, which you can put at the end of a SQL state‐\\nment (after any WHERE and GROUP BY). If you wanted to sort by year, and then month,'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 56, 'page_label': '41', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='time to  bring up the ORDER BY operator, which you can put at the end of a SQL state‐\\nment (after any WHERE and GROUP BY). If you wanted to sort by year, and then month,\\nyou could just add this command:\\nOrdering Records | 41'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 57, 'page_label': '42', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT year, month, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year, month\\nORDER BY year, month\\nHowever, you are probably more interested in recent data and would like it at the top.\\nBy default, sorting is done with the ASC operator, which orders the data in ascending\\norder. If you want to sort in descending order instead, apply the DESC operator to the\\nordering of year to make more recent records appear at the top of the results:\\nSELECT year, month, COUNT(*) AS record_count FROM station_data\\nWHERE tornado = 1\\nGROUP BY year, month\\nORDER BY year DESC, month\\nAggregate Functions\\nWe already used the COUNT(*) function to count records. But there are other aggrega‐\\ntion functions, including SUM(), MIN(), MAX(), and AVG(). We can use aggregation\\nfunctions on a specific column to perform some sort of calculation on it.\\nBut first let’s look at another way to use COUNT(). The COUNT() function can be used'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 57, 'page_label': '42', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='functions on a specific column to perform some sort of calculation on it.\\nBut first let’s look at another way to use COUNT(). The COUNT() function can be used\\nfor a purpose other than simply counting records. If you specify a column instead of\\nan asterisk, it will count the number of non-null values in that column. For instance,\\nwe can take a count of snow_depth recordings, which will count the number of non-\\nnull values (Figure 6-3):\\nSELECT COUNT(snow_depth) as recorded_snow_depth_count\\nFROM STATION_DATA\\nFigure 6-3. Count of non-null snow depth recordings\\nTaking a count of non-null values in a column can be useful, so take note that\\nCOUNT() can fulfill that purpose as well when applied to a column.\\n42 | Chapter 6: GROUP BY and ORDER BY'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 58, 'page_label': '43', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Aggregate functions such as COUNT(), SUM(), AVG(), MIN(), and\\nMAX() will never include null values in their calculations.  Only\\nnon- null values will be considered.\\nLet’s move on to other aggregation tasks. If you wanted to find the average tempera‐\\nture for each month since 2000, you could filter for years 2000 and later, group by\\nmonth, and perform an average on temp (Figure 6-4):\\nSELECT month, AVG(temp) as avg_temp\\nFROM station_data\\nWHERE year >= 2000\\nGROUP BY month\\nFigure 6-4. Average temperature by month since the year 2000\\nAs always, you can use functions on the aggregated values and perform tasks such as\\nrounding to make them look nicer (Figure 6-5):\\nSELECT month, round(AVG(temp),2) as avg_temp\\nFROM station_data\\nWHERE year >= 2000\\nGROUP BY month\\nAggregate Functions | 43'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 59, 'page_label': '44', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 6-5. Rounding the average temperature by month\\nSUM() is another common aggregate operation. To find the sum of snow depth by\\nyear since 2000, run this query:\\nSELECT year, SUM(snow_depth) as total_snow\\nFROM station_data\\nWHERE year >= 2000\\nGROUP BY year\\nThere is no limitation on how many aggregate operations you can use in a single\\nquery. Here we find the total_snow and total_precipitation for each year since\\n2000 in a single query, as well as the max_precipitation:\\nSELECT year,\\nSUM(snow_depth) as total_snow,\\nSUM(precipitation) as total_precipitation,\\nMAX(precipitation) as max_precipitation\\nFROM station_data\\nWHERE year >= 2000\\nGROUP BY year\\nIt may not be apparent yet, but you can achieve some very specific aggregations by\\nleveraging the WHERE. If you wanted the total precipitation by year only when a tor‐\\nnado was present, you would just have to filter on tornado being true. This will only\\ninclude tornado-related precipitation in the totals:\\n44 | Chapter 6: GROUP BY and ORDER BY'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 60, 'page_label': '45', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT year,\\nSUM(precipitation) as tornado_precipitation\\nFROM station_data\\nWHERE tornado = 1\\nGROUP BY year\\nThe HAVING Statement\\nSuppose you wanted to filter out records based on an aggregated value.  While your\\nfirst instinct might be to use a WHERE statement, this actually will not work because the\\nWHERE filters records, and does not filter aggregations. For example, if you try to use a\\nWHERE to filter results where total_precipitation is greater than 30, this will error\\nout:\\nSELECT year,\\nSUM(precipitation) as total_precipitation\\nFROM station_data\\nWHERE total_precipitation > 30\\nGROUP BY year\\nWhy does this not work? Y ou cannot filter on aggregated fields using WHERE. Y ou have\\nto use the HAVING keyword to accomplish this. The way aggregation works is that the\\nsoftware processes record by record, finding which ones it wants to keep based on the\\nWHERE condition. After that, it crunches the records down on the GROUP BY and per‐'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 60, 'page_label': '45', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='software processes record by record, finding which ones it wants to keep based on the\\nWHERE condition. After that, it crunches the records down on the GROUP BY and per‐\\nforms any aggregate functions, such as SUM(). If we wanted to filter on the SUM()\\nvalue, we would need the filter to take place after it is calculated. This is where HAVING\\ncan be applied:\\nSELECT year,\\nSUM(precipitation) as total_precipitation\\nFROM station_data\\nGROUP BY year\\nHAVING total_precipitation > 30\\nHAVING is the aggregated equivalent to WHERE. The WHERE keyword filters individual\\nrecords, but HAVING filters aggregations.\\nNote that some platforms, including Oracle, do not support aliases in the HAVING\\nstatement (just like the GROUP BY). This means you must specify the aggregate func‐\\ntion again in the HAVING statement. If you were running the preceding query on an\\nOracle database, you would have to write it like this:\\nSELECT year,\\nSUM(precipitation) as total_precipitation\\nFROM station_data\\nGROUP BY year'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 60, 'page_label': '45', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Oracle database, you would have to write it like this:\\nSELECT year,\\nSUM(precipitation) as total_precipitation\\nFROM station_data\\nGROUP BY year\\nHAVING SUM(precipitation) > 30\\nThe HAVING Statement | 45'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 61, 'page_label': '46', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Getting Distinct Records\\nIt is not uncommon to want a set of distinct results from a query. We know there are\\n28,000 records in our station_data table. But suppose we want to get a distinct list\\nof the station_number values? If we run this query, we will get duplicates:\\nSELECT station_number FROM station_data\\nIf we want a distinct list of station numbers without any duplicates, we can use the\\nDISTINCT keyword:\\nSELECT DISTINCT station_number FROM station_data\\nY ou can also get distinct results for more than one column. If you need the distinct\\nstation_number and year sets, just include both of those columns in the SELECT\\nstatement:\\nSELECT DISTINCT station_number, year FROM station_data\\nSummary\\nIn this chapter, we learned how to aggregate and sort data using GROUP BY and ORDER\\nBY. We also leveraged the SUM(), MAX(), MIN(), AVG(), and COUNT() aggregate func‐\\ntions to crunch thousands of records into a few meaningful totaled records. Because'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 61, 'page_label': '46', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='BY. We also leveraged the SUM(), MAX(), MIN(), AVG(), and COUNT() aggregate func‐\\ntions to crunch thousands of records into a few meaningful totaled records. Because\\nwe cannot use WHERE to filter aggregated fields, we used the HAVING keyword to\\naccomplish that. We also leveraged the DISTINCT operator to get distinct results in\\nour queries and eliminate duplicates.\\nI hope by now you see the flexibility SQL offers to quickly develop meaningful\\nreports based on thousands or millions of records. Before moving on, I would recom‐\\nmend experimenting with everything you’ve learned so far and trying out the SELECT,\\nWHERE, and GROUP BY in your queries. Ask yourself business questions such as “Has\\nthe temperature been getting warmer every January for the past 20 years?” or “How\\nmany times has hail been present versus not present during a tornado?” Try to create\\nSQL queries on the weather data to answer these questions.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 61, 'page_label': '46', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='many times has hail been present versus not present during a tornado?” Try to create\\nSQL queries on the weather data to answer these questions.\\nGet comfortable with what you have learned so far, but do not fret about memorizing\\nevery SQL functionality. That will come with time as you repeatedly use and practice\\nSQL. Y ou’ll gain more knowledge in the coming chapters, and it’s OK to refer to Goo‐\\ngle or this guide if you forget how to compose things.\\nY ou can get the short, complete list of SQLite aggregate functions\\nin “ APPENDIX A7 – Aggregate Functions” on page 104 or at\\nhttps://www.sqlite.org/lang_aggfunc.html.\\n46 | Chapter 6: GROUP BY and ORDER BY'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 62, 'page_label': '47', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"CHAPTER 7\\nCASE Statements\\nWe are almost ready to learn the truly defining feature of SQL, the JOIN operator. But\\nbefore we do that, we should spend a short chapter covering a very handy operator\\ncalled CASE. This command allows us to swap a column value for another value based\\non one or more conditions.\\nThe CASE Statement\\nA CASE statement allows us to map one or more conditions to a corresponding value\\nfor each condition. Y ou start a CASE statement with the word CASE and conclude it\\nwith an END. Between those keywords, you specify each condition with a WHEN [condi\\ntion] THEN [value], where the [condition] and the corresponding [value] are\\nsupplied by you. After specifying the condition–value pairs, you can have a catch-all\\nvalue to default to if none of the conditions were met, which is specified in the ELSE.\\nFor example, we could categorize wind_speed into wind_severity categories\\n(Figure 7-1), where any speed greater than 40 is 'HIGH', 30 to 40 is 'MODERATE', and\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 62, 'page_label': '47', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"For example, we could categorize wind_speed into wind_severity categories\\n(Figure 7-1), where any speed greater than 40 is 'HIGH', 30 to 40 is 'MODERATE', and\\nanything less is 'LOW':\\nSELECT report_code, year, month, day, wind_speed,\\nCASE\\n    WHEN wind_speed >= 40 THEN 'HIGH'\\n    WHEN wind_speed >= 30 AND wind_speed < 40 THEN 'MODERATE'\\n    ELSE 'LOW'\\nEND as wind_severity\\nFROM station_data\\n47\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 63, 'page_label': '48', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Figure 7-1. Categorizing wind severity into HIGH, MODERATE, and LOW\\nWe can actually omit the AND wind_speed < 40 condition. Here is why: the machine\\nreads a CASE statement from top to bottom, and the first condition it finds true is the\\none it uses (and it will stop evaluating subsequent conditions). So if we have a record\\nwith a wind_speed of 43, we can be certain it will be evaluated as 'HIGH'. Although it\\nis greater than 30, it will not be assigned 'MODERATE' because it will not get to that\\npoint. Knowing this allows us to create a slightly more efficient query:\\nSELECT report_code, year, month, day, wind_speed,\\nCASE\\n    WHEN wind_speed >= 40 THEN 'HIGH'\\n    WHEN wind_speed >= 30 THEN 'MODERATE'\\n    ELSE 'LOW'\\nEND as wind_severity\\nFROM station_data\\nGrouping CASE Statements\\nWhen you create CASE statements and group them, you can create some very power‐\\nful transformations. Converting values based on one or more conditions before\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 63, 'page_label': '48', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Grouping CASE Statements\\nWhen you create CASE statements and group them, you can create some very power‐\\nful transformations. Converting values based on one or more conditions before\\naggregating them gives us even more possibilities to slice data in interesting ways.\\nElaborating on our previous example, we can group on year and wind_severity and\\nget a count of records for each one as shown here (also notice we use GROUP BY with\\n48 | Chapter 7: CASE Statements'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 64, 'page_label': '49', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"ordinal positions so we do not have to rewrite the wind_severity case expression in\\nthe GROUP BY):\\nSELECT year,\\nCASE\\n    WHEN wind_speed >= 40 THEN 'HIGH'\\n    WHEN wind_speed >= 30 THEN 'MODERATE'\\n    ELSE 'LOW'\\nEND as wind_severity,\\nCOUNT(*) as record_count\\nFROM station_data\\nGROUP BY 1, 2\\nThe “Zero/Null” CASE Trick\\nY ou can do some clever tricks with the CASE statement. One simple but helpful pat‐\\ntern is the “zero/null” CASE trick. This allows you to apply different “filters” for differ‐\\nent aggregate values, all in a single SELECT query. Y ou could never accomplish this in\\na WHERE because the WHERE applies a filter to everything. But you can use a CASE to\\ncreate a different filter condition for each aggregate value.\\nSay you wanted to aggregate precipitation into two sums, tornado_precipitation\\nand non_tornado_precipitation, and GROUP BY year and month. The logic is pri‐\\nmarily dependent on two fields: precipitation and tornado. But how exactly do you\\ncode this?\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 64, 'page_label': '49', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='and non_tornado_precipitation, and GROUP BY year and month. The logic is pri‐\\nmarily dependent on two fields: precipitation and tornado. But how exactly do you\\ncode this?\\nIf you give it some thought, you will realize you cannot do this with a WHERE state‐\\nment unless you do two separate queries (one for tornado being true and the other\\nfalse):\\nTornado precipitation\\nSELECT year, month,\\nSUM(precipitation) as tornado_precipitation\\nFROM station_data\\nWHERE tornado = 1\\nGROUP BY year, month\\nNon-tornado precipitation\\nSELECT year, month,\\nSUM(precipitation) as non_tornado_precipitation\\nFROM station_data\\nWHERE tornado = 0\\nGROUP BY year, month\\nThe “Zero/Null” CASE Trick | 49'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 65, 'page_label': '50', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='But it is possible to do this in a single query using a CASE statement. Y ou can move the\\ntornado conditions from the WHERE to a CASE, and make the value 0 if the condition is\\nfalse. Then you can SUM those CASE statements (Figure 7-2):\\nSELECT year, month,\\nSUM(CASE WHEN tornado = 1 THEN precipitation ELSE 0 END) as tornado_precipitation,\\nSUM(CASE WHEN tornado = 0 THEN precipitation ELSE 0 END) as \\n    non_tornado_precipitation\\n    \\nFROM station_data\\nGROUP BY year, month\\nFigure 7-2. Getting tornado and non-tornado precipitation by year and month\\nThe CASE statement can do an impressive amount of work, especially in complex\\naggregation tasks. By leveraging a condition to make a value 0 if the condition is not\\nmet, we effectively ignore that value and exclude it from the SUM (because adding 0\\nhas no impact).\\nY ou could also do this with MIN or MAX operations, and use a null instead of 0 to make\\nsure values with certain conditions are never considered. Y ou can find the maximum'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 65, 'page_label': '50', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='has no impact).\\nY ou could also do this with MIN or MAX operations, and use a null instead of 0 to make\\nsure values with certain conditions are never considered. Y ou can find the maximum\\nprecipitation when tornadoes were present and when they were not ( Figure 7-3) as\\nfollows:\\n50 | Chapter 7: CASE Statements'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 66, 'page_label': '51', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT year,\\nMAX(CASE WHEN tornado = 0 THEN precipitation ELSE NULL END) as\\n    max_non_tornado_precipitation,\\nMAX(CASE WHEN tornado = 1 THEN precipitation ELSE NULL END) as\\n    max_tornado_precipitation\\nFROM station_data\\nGROUP BY year\\nFigure 7-3. Maximum tornado and non-tornado precipitations by year\\nJust like with the WHERE statement, you can use any Boolean expressions in a CASE\\nstatement, including functions and AND, OR, and NOT statements. The following query\\nwill find the average temperatures by month when rain/hail was present versus not\\npresent after the year 2000:\\nSELECT month,\\nAVG(CASE WHEN rain OR hail THEN temperature ELSE null END)\\nAS avg_precipitation_temp,\\nAVG(CASE WHEN NOT (rain OR hail) THEN temperature ELSE null END)\\nAS avg_non_precipitation_temp\\nFROM STATION_DATA\\nWHERE year > 2000\\nGROUP BY month\\nThe “Zero/Null” CASE Trick | 51'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 67, 'page_label': '52', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='The zero/null CASE trick is a great use of the CASE statement. It offers many possibili‐\\nties to perform several aggregations with different criteria and therefore is worth\\nknowing.\\nSummary\\nWe dedicated a chapter to learning about CASE statements because they offer a lot of\\nflexibility. We can swap values in a column with another set of values based on condi‐\\ntions we provide. When we aggregate CASE statements, we bring more possibilities to\\nslice data in interesting ways and pack more information into a single query.\\nHopefully by now you have a solid foundation and are ready to learn the defining\\npart of SQL: the JOIN. Take a break. Down a few espressos. After you learn the JOIN,\\nyou can truly call yourself a SQL developer.\\n52 | Chapter 7: CASE Statements'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 68, 'page_label': '53', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 8\\nJOIN\\nStitching Tables Together\\nJoining is the defining functionality of SQL and sets it apart from other data technol‐\\nogies. Be sure you are somewhat comfortable with the material we’ve covered so far,\\nand take your time practicing and reviewing before moving on.\\nLet’s rewind back to the beginning of this book, when we were discussing relational\\ndatabases. Remember how “normalized” databases often have tables with fields that\\npoint to other tables? For example, consider this CUSTOMER_ORDER table, which has a\\nCUSTOMER_ID field (Figure 8-1).\\nFigure 8-1. The CUSTOMER_ORDER table has a CUSTOMER_ID field\\nThis CUSTOMER_ID field gives us a key to look up in the table CUSTOMER. Knowing this,\\nit should be no surprise that the CUSTOMER table also has a CUSTOMER_ID field\\n(Figure 8-2).\\n53'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 69, 'page_label': '54', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-2. The CUSTOMER table has a CUSTOMER_ID key field that can be used to\\nget customer information\\nWe can retrieve customer information for an order from this table, very much like a\\nVLOOKUP in Excel.\\nThis is an example of a relationship between the CUSTOMER_ORDER table and the\\nCUSTOMER table. We can say  that CUSTOMER is a parent to CUSTOMER_ORDER. Because\\nCUSTOMER_ORDER depends on CUSTOMER for information, it is a child of CUSTOMER. Con‐\\nversely, CUSTOMER cannot be a child of CUSTOMER_ORDER because it does not rely on it\\nfor any information.  The diagram in Figure 8-3 shows this relationship; the arrow\\nshows that CUSTOMER supplies customer information to CUSTOMER_ORDER via the\\nCUSTOMER_ID.\\nFigure 8-3. CUSTOMER is the parent to CUSTOMER_ORDER, because\\nCUSTOMER_ORDER depends on it for CUSTOMER information\\nThe other aspect to consider in a relationship is how many records in the child can be\\ntied to a single record of the parent. Take the CUSTOMER and CUSTOMER_ORDER tables'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 69, 'page_label': '54', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='The other aspect to consider in a relationship is how many records in the child can be\\ntied to a single record of the parent. Take the CUSTOMER and CUSTOMER_ORDER tables\\nand you will  see it is a one-to-many relationship, where a single customer record can\\nline up with multiple orders. Let’s take a look at Figure 8-4 to see a specific example:\\nthe customer “Re-Barre Construction” with CUSTOMER_ID 3 is tied to three orders.\\n54 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 70, 'page_label': '55', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-4. A one-to-many relationship between CUSTOMER and\\nCUSTOMER_ORDER\\nOne-to-many is the most common type of relationship because it accommodates\\nmost business needs, such as a single customer having multiple orders. Business data\\nin a well-designed database should strive for a one-to-many pattern. Less common\\nare the one-to-one and many-to-many relationships (sometimes referred to as a Carte‐\\nsian product). These are worth researching later, but in the interest of focusing the\\nscope of this book, we will steer clear of them.\\nINNER JOIN\\nUnderstanding table relationships, we can consider that it might be nice to stitch two\\ntables together, so we can see CUSTOMER and CUSTOMER_ORDER information alongside\\neach other. Otherwise, we will have to manually perform tons of lookups with\\nCUSTOMER_ID, which can be quite tedious. We can avoid that with JOIN operators, and\\nwe will start by learning the INNER JOIN.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 70, 'page_label': '55', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CUSTOMER_ID, which can be quite tedious. We can avoid that with JOIN operators, and\\nwe will start by learning the INNER JOIN.\\nThe INNER JOIN allows us to merge two tables together. But if we are going to merge\\ntables, we need to define a commonality between the two so records from both tables\\nline up. We need to define one or more fields they have in common and join on them.\\nIf we are going to query the CUSTOMER_ORDER table and join it to CUSTOMER to bring in\\ncustomer information, we need to define the commonality on CUSTOMER_ID.\\nOpen up the rexon_metals database and open a new SQL editor window. We are\\ngoing to execute our first INNER JOIN:\\nINNER JOIN | 55'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 71, 'page_label': '56', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT ORDER_ID,\\nCUSTOMER.CUSTOMER_ID,\\nORDER_DATE,\\nSHIP_DATE,\\nNAME,\\nSTREET_ADDRESS,\\nCITY,\\nSTATE,\\nZIP,\\nPRODUCT_ID,\\nORDER_QTY\\nFROM CUSTOMER INNER JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nThe first thing you may notice is we were able to query fields from both CUSTOMER\\nand CUSTOMER_ORDER. It is almost like we took those two tables and temporarily\\nmerged them into a single table, which we queried off of. In effect, that is exactly what\\nwe did!\\nLet’s break down how this was accomplished. First, we select the fields we want from\\nthe CUSTOMER and CUSTOMER_ORDER tables:\\nSELECT CUSTOMER.CUSTOMER_ID,\\nNAME,\\nSTREET_ADDRESS,\\nCITY,\\nSTATE,\\nZIP,\\nORDER_DATE,\\nSHIP_DATE,\\nORDER_ID,\\nPRODUCT_ID,\\nORDER_QTY\\nFROM CUSTOMER INNER JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nIn this case, we want to show customer address information for each order. Also\\nnotice that because CUSTOMER_ID is in both tables, we had to explicitly choose one'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 71, 'page_label': '56', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='In this case, we want to show customer address information for each order. Also\\nnotice that because CUSTOMER_ID is in both tables, we had to explicitly choose one\\n(although it should not matter which). In this case, we chose the CUSTOMER_ID in\\nCUSTOMER using an explicit syntax, CUSTOMER.CUSTOMER_ID.\\nFinally, the important part that temporarily merges two tables into one.  The FROM\\nstatement is where we execute our INNER JOIN. We specify that we are pulling from\\nCUSTOMER and inner joining it with CUSTOMER_ORDER, and that the commonality is on\\nthe CUSTOMER_ID fields (which have to be equal to line up):\\n56 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 72, 'page_label': '57', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SELECT CUSTOMER.CUSTOMER_ID,\\nNAME,\\nSTREET_ADDRESS,\\nCITY,\\nSTATE,\\nZIP,\\nORDER_DATE,\\nSHIP_DATE,\\nORDER_ID,\\nPRODUCT_ID,\\nORDER_QTY\\nFROM CUSTOMER INNER JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nIf you have worked with Excel, think of this as a VLOOKUP on steroids, where\\ninstead of looking up CUSTOMER_ID and getting one value from another table, we are\\ngetting the entire matching record. This enables us to select any number of fields\\nfrom the other table.\\nNow take a look at the results ( Figure 8-5 ). Thanks to the INNER JOIN, this query\\ngives us a view that includes the customer details with each order.\\nFigure 8-5. CUSTOMER inner joined with CUSTOMER_ORDER\\nJoins truly give us the best of both worlds. We store data efficiently through normal‐\\nization, but can use joins to merge tables together on common fields to create more\\ndescriptive views of the data.\\nThere is one behavior with INNER JOIN to be aware of. Take a moment to look at the'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 72, 'page_label': '57', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='ization, but can use joins to merge tables together on common fields to create more\\ndescriptive views of the data.\\nThere is one behavior with INNER JOIN to be aware of. Take a moment to look at the\\nresults of the preceding query. We can see that we have three “Re-Barre Construction”\\norders, as well as an order from “LITE Industrial” and another from “Marsh Lane\\nMetal Works. ” But are we missing anybody?\\nIf you go look at the CUSTOMER table, you will see there are five customers. Our INNER\\nJOIN query captured only three. “Rex Tooling Inc” and “Prairie Construction” are\\nnowhere to be found in our query results. So what exactly happened? There are no\\norders for Rex Tooling Inc and Prairie Construction, and because of this the INNER\\nJOIN excluded them from the query. It will only show records that inclusively exist in\\nboth tables (Figure 8-6).\\nINNER JOIN | 57'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 73, 'page_label': '58', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-6. A visualized inner join between CUSTOMER and CUSTOMER_ORDER\\n(note the two customers getting omitted, as they have no orders to join to)\\nWith an INNER JOIN, any records that do not have a common joined value in both\\ntables will be excluded. If we want to include all records from the CUSTOMER table, we\\ncan accomplish this with a LEFT JOIN.\\nLEFT JOIN\\nThose two customers, Rex Tooling Inc and Prairie Construction, were excluded from\\nthe INNER JOIN on CUSTOMER_ID because they had no orders to join on. But suppose\\nwe did want to include them anyway. Often, we may want to join tables and see, for\\nexample, all customers, even if they had no orders.\\nIf you are comfortable with the INNER JOIN, the left outer join is not much different.\\nBut there is one very subtle difference. Modify your query from before and replace\\nthe INNER JOIN with LEFT JOIN, the keywords for a left outer join. As shown in\\nFigure 8-7, the table specified on the “left” side of the LEFT JOIN operator (CUSTOMER)'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 73, 'page_label': '58', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='the INNER JOIN with LEFT JOIN, the keywords for a left outer join. As shown in\\nFigure 8-7, the table specified on the “left” side of the LEFT JOIN operator (CUSTOMER)\\nwill have all its records included, even if they do not have any child records in the\\n“right” table (CUSTOMER_ORDER).\\n58 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 74, 'page_label': '59', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-7. The LEFT JOIN will include all records on the “left” table, even if they have\\nnothing to join to on the “right” table (which will be null)\\nRunning this, we have similar results to what we got from the INNER JOIN query ear‐\\nlier, but we have two additional records for the customers that have no orders\\n(Figure 8-8 ). For those two customers, notice all the fields that come from CUS\\nTOMER_ORDER are null, because there were no orders to join to. Instead of omitting\\nthem like the INNER JOIN did, the LEFT JOIN just made them null (Figure 8-9).\\nFigure 8-8. CUSTOMER left joined with CUSTOMER_ORDER (note the null\\nCUSTOMER_ORDER fields mean no orders were found for those two customers)\\nLEFT JOIN | 59'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 75, 'page_label': '60', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-9. CUSTOMER left joined with CUSTOMER_ORDER (note that “Rex Tooling”\\nand “Prairie Construction” joined to NULL, as they have no orders to join to)\\nIt is also common to use LEFT JOIN to check for “orphaned” child records that have\\nno parent, or conversely a parent that has no children (e.g., orders that have no cus‐\\ntomers, or customers that have no orders). Y ou can use a WHERE statement to check\\nfor null values that are a result of the LEFT JOIN. Modifying our previous example, we\\ncan filter for customers that have no orders by filtering any field from the right table\\nthat is null:\\nSELECT\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME\\nFROM CUSTOMER LEFT JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nWHERE ORDER_ID IS NULL\\nSure enough, you will only see Rex Tooling Inc and Prairie Construction listed, as\\nthey have no orders.\\n60 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 76, 'page_label': '61', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Other JOIN Types\\nThere is a RIGHT JOIN operator, which performs a right outer join that is almost iden‐\\ntical to the left outer join. It flips the direction of the join and includes all records\\nfrom the right table. However, the RIGHT JOIN is rarely used and should be avoided.\\nY ou should stick to convention and prefer left outer joins with LEFT JOIN, and put\\nthe “all records” table on the left side of the join operator.\\nThere also is a full outer join operator called OUTER JOIN that includes all records\\nfrom both tables. It does a LEFT JOIN and a RIGHT JOIN simultaneously, and can have\\nnull records in both tables. It can be helpful to find orphaned records in both direc‐\\ntions simultaneously in a single query, but it also is seldom used.\\nRIGHT JOIN and OUTER JOIN are not supported in SQLite due to\\ntheir highly niche nature. But most database solutions feature\\nthem.\\nJoining Multiple Tables\\nRelational databases can be fairly complex in terms of relationships between tables. A'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 76, 'page_label': '61', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='their highly niche nature. But most database solutions feature\\nthem.\\nJoining Multiple Tables\\nRelational databases can be fairly complex in terms of relationships between tables. A\\ngiven table can be the child of more than one parent table, and a table can be the par‐\\nent to one table but a child to another. So how does this all work?\\nWe have observed the relationship between CUSTOMER and CUSTOMER_ORDER. But there\\nis another table we can include that will make our orders more meaningful: the\\nPRODUCT table. Notice that the CUSTOMER_ORDER table has a PRODUCT_ID column,\\nwhich corresponds to a product in the PRODUCT table.\\nWe can supply not only CUSTOMER information to the CUSTOMER_ORDER table, but also\\nPRODUCT information using PRODUCT_ID (Figure 8-10).\\nOther JOIN Types | 61'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 77, 'page_label': '62', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-10. Joining multiple tables\\nWe can use these two relationships to execute a query that displays orders with cus‐\\ntomer information and product information simultaneously. All we do is define the\\ntwo joins between CUSTOMER_ORDER and CUSTOMER, and CUSTOMER_ORDER and PRODUCT\\n(Figure 8-11 ). If you start to get confused, just compare the following query to the\\ndiagram in Figure 8-10 , and you will see the joins are constructed strictly on these\\nrelationships:\\nSELECT\\nORDER_ID,\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME,\\nSTREET_ADDRESS,\\nCITY,\\nSTATE,\\nZIP,\\nORDER_DATE,\\nPRODUCT_ID,\\nDESCRIPTION,\\nORDER_QTY\\nFROM CUSTOMER\\nINNER JOIN CUSTOMER_ORDER\\nON CUSTOMER_ORDER.CUSTOMER_ID = CUSTOMER.CUSTOMER_ID\\nINNER JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\n62 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 78, 'page_label': '63', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-11. Joining ORDER, CUSTOMER, and PRODUCT fields together\\nThese orders are much more descriptive now that we’ve leveraged CUSTOMER_ID and\\nPRODUCT_ID to bring in customer and product information. As a matter of fact, now\\nthat we’ve merged these three tables, we can use fields from all three tables to create\\nexpressions. If we want to find the revenue for each order, we can multiply ORDER_QTY\\nand PRICE, even though those fields exist in two separate tables:\\nSELECT\\nORDER_ID,\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME,\\nSTREET_ADDRESS,\\nCITY,\\nSTATE,\\nZIP,\\nORDER_DATE,\\nPRODUCT_ID,\\nDESCRIPTION,\\nORDER_QTY,\\nORDER_QTY * PRICE as REVENUE\\nFROM CUSTOMER\\nINNER JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nINNER JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\nNow we have the revenue for each order, even though the needed columns came from\\ntwo separate tables.\\nGrouping JOINs'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 78, 'page_label': '63', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='INNER JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\nNow we have the revenue for each order, even though the needed columns came from\\ntwo separate tables.\\nGrouping JOINs\\nLet’s keep going with this example. We have the orders with their revenue, thanks to\\nthe join we built. But suppose we want to find the total revenue by customer? We still\\nneed to use all three tables and merge them together with our current join setup,\\nbecause we need the revenue we just calculated. But also we need to do a GROUP BY.\\nThis is legitimate and perfectly doable. Because we want to aggregate by customer, we\\nneed to group on CUSTOMER_ID and CUSTOMER_NAME. Then we need to SUM the\\nGrouping JOINs | 63'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 79, 'page_label': '64', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='ORDER_QTY * PRICE expression to get total revenue (Figure 8-12). To focus our GROUP\\nBY scope, we omit all other fields:\\nSELECT\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME,\\nsum(ORDER_QTY * PRICE) as TOTAL_REVENUE\\nFROM CUSTOMER_ORDER\\nINNER JOIN CUSTOMER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nINNER JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\nGROUP BY 1,2\\nFigure 8-12. Calculating TOTAL_REVENUE by joining and aggregating three tables\\nBecause we may want to see all customers, including ones that have no orders, we can\\nuse LEFT JOIN instead of INNER JOIN for all our join operations (Figure 8-13):\\nSELECT\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME,\\nsum(ORDER_QTY * PRICE) as TOTAL_REVENUE\\nFROM CUSTOMER\\nLEFT JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nLEFT JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\nGROUP BY 1,2\\n64 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 80, 'page_label': '65', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 8-13. Using a LEFT JOIN to include all customers and their TOTAL_REVENUE\\nWe have to LEFT JOIN both table pairs, because mixing LEFT JOIN\\nand INNER JOIN would cause the INNER JOIN to win, resulting in\\nthe two customers without orders getting excluded. This is because\\nnull values cannot be inner joined on and will always get filtered\\nout. A LEFT JOIN tolerates null values.\\nRex Tooling Inc and Prairie Construction are now present even though they have no\\norders. But we may want the values to default to 0 instead of null if there are no sales. \\nWe can accomplish this simply with the coalesce() function we learned about in\\nChapter 5 to turn nulls into zeros (Figure 8-14):\\nSELECT\\nCUSTOMER.CUSTOMER_ID,\\nNAME AS CUSTOMER_NAME,\\ncoalesce(sum(ORDER_QTY * PRICE), 0) as TOTAL_REVENUE\\nFROM CUSTOMER\\nLEFT JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nLEFT JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\nGROUP BY 1,2'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 80, 'page_label': '65', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='FROM CUSTOMER\\nLEFT JOIN CUSTOMER_ORDER\\nON CUSTOMER.CUSTOMER_ID = CUSTOMER_ORDER.CUSTOMER_ID\\nLEFT JOIN PRODUCT\\nON CUSTOMER_ORDER.PRODUCT_ID = PRODUCT.PRODUCT_ID\\nGROUP BY 1,2\\nFigure 8-14. Coalescing null TOTAL_REVENUE values to 0\\nGrouping JOINs | 65'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 81, 'page_label': '66', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Summary\\nJoins are the most challenging topic in SQL, but they are also the most rewarding.\\nJoins allow us to take data scattered across multiple tables and stitch it together into\\nsomething more meaningful and descriptive. We can take two or more tables and\\njoin them together into a larger table that has more context. In the next chapter, we\\nwill learn more about joins and how they are naturally defined by table relationships.\\n66 | Chapter 8: JOIN'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 82, 'page_label': '67', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 9\\nDatabase Design\\nPlanning a Database\\nSo far in this book, we have only learned how to be consumers of data with the\\nSELECT statement.  We have done analysis operations that read data and transform it\\nin interesting ways, but none of this physically changes the data in the tables. A\\nSELECT statement is a read-only operation. Sometimes, though, we will want to\\nCREATE new tables, as well as INSERT, UPDATE, and DELETE records.\\nWhen you create your own tables to support your business, it should not be done\\nlightly. Y ou need to plan carefully because bad database design is sure to cause regrets\\ndown the road. There are critical questions that should drive your design:\\nDesign questions\\n• What are the business requirements?\\n• What tables will I need to fulfill those requirements?\\n• What columns will each table contain?\\n• How will the tables be normalized?\\n• What will their parent/child relationships be?\\nIt might be a good idea to draft a diagram showing the tables and how they are'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 82, 'page_label': '67', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='• How will the tables be normalized?\\n• What will their parent/child relationships be?\\nIt might be a good idea to draft a diagram showing the tables and how they are\\nrelated. But design is not the only factor to consider. Populating data should be\\npart of the planning process too. If the data is not maintainable and kept up to\\ndate, then the design has already failed. This factor is often overlooked and can\\neasily cause a database project to fail.\\n67'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 83, 'page_label': '68', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Data questions\\n• How much data will be populated into these tables?\\n• Who/what will populate the data into these tables?\\n• Where will the data come from?\\n• Do we need processes to automatically populate the tables?\\nData inception has to happen somewhere. Depending on the nature of the data, it\\ncan be created within your organization or received from an external party. If you\\nneed to store a high volume of data that updates regularly, chances are a human\\ncannot do this task manually. Y ou will need a process written in Java, Python, or\\nanother coding language to do that.\\nAlthough security and administration are beyond the scope of this book, central‐\\nized databases usually are concerned with these areas. Administrating privileges\\nand security is a full-time job in itself and often done by database administrators\\n(DBAs). For centralized databases, security factors should be considered.\\nSecurity questions\\n• Who should have access to this database?'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 83, 'page_label': '68', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='(DBAs). For centralized databases, security factors should be considered.\\nSecurity questions\\n• Who should have access to this database?\\n• Who should have access to which tables? Read-only access? Write access?\\n• Is this database critical to business operations?\\n• What backup plans do we have in the event of disaster/failure?\\n• Should changes to tables be logged?\\n• If the database is used for websites or web applications, is it secure?\\nSecurity is often a tough topic to address. Excessive security creates bureaucracy\\nand obstructs nimbleness, but insufficient security will invite calamity. Like with\\nany complex issue, a balance between the two extremes has to be found. But\\nsecurity should become a high priority when the database is used for a website.\\nConnecting anything to the Web makes it more vulnerable to leaks and malicious\\nattacks.\\nOne of the most common malicious hacks is SQL injection. If a\\nweb developer has failed to implement security measures in a'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 83, 'page_label': '68', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='attacks.\\nOne of the most common malicious hacks is SQL injection. If a\\nweb developer has failed to implement security measures in a\\nwebsite, you can type a carefully crafted SELECT statement\\nright inside a web browser, and get the query results displayed\\nright back to you! 130 million credit card numbers were stolen\\nthis way in 2009.\\nSQLite has few security or administrative features, as these features would be\\noverkill in a lightweight database. If your SQLite databases need to be secured,\\n68 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 84, 'page_label': '69', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='protect the database files the same way you would any other file. Either hide\\nthem, copy them to a backup, or distribute copies to your coworkers so they do\\nnot have access to the “master copy. ”\\nWith all these considerations in mind, let’s design our first database.\\nThe SurgeTech Conference\\nY ou are a staff member for the SurgeTech conference, a gathering of tech startup\\ncompanies seeking publicity and investors. The organizer has tasked you with creat‐\\ning a database to manage the attendees, companies, presentations, rooms, and pre‐\\nsentation attendance. How should this database be designed?\\nFirst, review the different entities and start thinking about how they will be structured\\ninto tables. This may seem like a large number of business asks to capture, but any\\ncomplex problem can be broken down into simple components.\\nATTENDEE\\nThe attendees are registered guests (including some VIPs) who are checking out the'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 84, 'page_label': '69', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='complex problem can be broken down into simple components.\\nATTENDEE\\nThe attendees are registered guests (including some VIPs) who are checking out the\\ntech startups. Each attendee’s ID, name, phone number, email, and VIP status will\\nneed to be tracked.\\nTaking all this information, we may design the ATTENDEE table with these columns:\\nCOMPANY\\nThe startup companies need to be tracked as well. The company ID, company name,\\ncompany description, and primary contact (who should be listed as an attendee) for\\neach must be tracked:\\nThe SurgeTech Conference | 69'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 85, 'page_label': '70', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='PRESENTATION\\nSome companies will schedule to do a presentation for a specific slot of time (with a\\nstart time and end time). The company leading the presentation as well as a room\\nnumber must also be booked for each presentation slot:\\nROOM\\nThere will be rooms available for the presentations, each with a room ID number, a\\nfloor number, and a seating capacity:\\nPRESENTATION_ATTENDANCE\\nIf attendees are interested in hearing a company’s presentation, they can acquire a\\nticket (with a ticket ID) and be allowed in. This will help keep track of who attended\\nwhat presentations. To implement this, the PRESENTATION_ATTENDANCE table will track\\nthe ticket IDs and pair the presentations with the attendees through their respective\\nIDs to show who was where:\\nPrimary and Foreign Keys\\nY ou should always strive to have a primary key on any table. A primary key is a spe‐\\ncial field (or combination of fields) that provides a unique identity to each record. A'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 85, 'page_label': '70', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Primary and Foreign Keys\\nY ou should always strive to have a primary key on any table. A primary key is a spe‐\\ncial field (or combination of fields) that provides a unique identity to each record. A\\nprimary key often defines a relationship and is frequently joined on. The ATTENDEE\\n70 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 86, 'page_label': '71', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='table has an ATTENDEE_ID field as its primary key, COMPANY has COMPANY_ID, and so\\non. While you do not need to designate a field as a primary key to join on it, it allows\\nthe database software to execute queries much more efficiently. It also acts as a con‐\\nstraint to ensure data integrity. No duplicates are allowed on the primary key, which\\nmeans you cannot have two ATTENDEE records both with an ATTENDEE_ID of 2. The\\ndatabase will forbid this from happening and throw an error.\\nTo focus our scope in this book, we will not compose a primary key\\noff more than one field. But be aware that multiple fields can act as\\na primary key, and you can never have duplicate combinations of\\nthose fields. For example, if you specified your primary key on the\\nfields REPORT_ID and APPROVER_ID, you can never have two records\\nwith the same combination of REPORT_ID and APPROVER_ID.\\nDo not confuse the primary key with a foreign key. The primary key exists in the par‐'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 86, 'page_label': '71', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='with the same combination of REPORT_ID and APPROVER_ID.\\nDo not confuse the primary key with a foreign key. The primary key exists in the par‐\\nent table, but the foreign key exists in the child table. The foreign key in a child table\\npoints to the primary key in its parent table. For example, the ATTENDEE_ID in the\\nATTENDEE table is a primary key, but the ATTENDEE_ID in the\\nPRESENTATION_ATTENDANCE table is a foreign key. The two are joined together for a\\none-to-many relationship. Unlike a primary key, a foreign key does not enforce\\nuniqueness, as it is the “many” in a “one-to-many” relationship.\\nThe primary key and foreign key do not have to share the same field name. The\\nBOOKED_COMPANY_ID in the PRESENTATION table is a foreign key pointing to the\\nCOMPANY_ID in its parent table COMPANY. The field name can be different on the for‐\\neign key to make it more descriptive of its usage. In this case, BOOKED_COMPANY_ID is'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 86, 'page_label': '71', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='COMPANY_ID in its parent table COMPANY. The field name can be different on the for‐\\neign key to make it more descriptive of its usage. In this case, BOOKED_COMPANY_ID is\\nmore descriptive than just COMPANY_ID. The semantics are subjective but still legiti‐\\nmate as long as the business wording is clear.\\nThe Schema\\nApplying our knowledge of primary keys and foreign keys, we can establish the rela‐\\ntionships between these five tables and draw a database schema as shown in\\nFigure 9-1. A database schema is a diagram showing tables, their columns, and their\\nrelationships. All the primary keys and foreign keys are connected by arrows. The\\nnon-tipped side of the arrow ties to a primary key, while the tipped side points to a\\nforeign key. These arrows visualize how each parent table supplies data to a child\\ntable.\\nThe Schema | 71'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 87, 'page_label': '72', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-1. The database schema for the SurgeTech conference, with all tables and rela‐\\ntionships\\nIt can be overwhelming to look at these tables and relationships all at once. But all\\ncomplex structures can be broken down into simple pieces. Chances are you will\\nnever write a SELECT query that uses all the tables, and you probably will only SELECT\\nfrom two (maybe three) tables. Therefore, the secret to observing a schema is to focus\\nonly on two or three tables and their relationships at a time. While you analyze your\\ndrafted design, you can ensure the tables are efficiently normalized and primary/\\nforeign keys are used effectively (Figure 9-2).\\n72 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 88, 'page_label': '73', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-2. Focusing on just two tables and their relationships (here we can easily see the\\nPRIMARY_CONTACT_ATTENDEE_ID supplies name and contact information from\\nthe ATTENDEE table)\\nIf you can successfully visualize different SELECT queries and joins you would typi‐\\ncally use on the data, the database schema is probably sound.\\nCreating a New Database\\nWith a well-planned design, it is now time to actually create this database. We are\\ngoing to use SQLiteStudio’s tools to create the tables and components. But along the\\nway, SQLiteStudio will show us the SQL it uses to create and modify our tables.\\nFirst, navigate to Database→Add a Database (Figure 9-3).\\nCreating a New Database | 73'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 89, 'page_label': '74', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-3. Adding a database\\nClick the green “plus” button circled in Figure 9-4 to create a new database.\\nFigure 9-4. Creating a database\\n74 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 90, 'page_label': '75', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Browse to the folder you would like to save the database to. In the “File name” field,\\nprovide a name for the database file. It usually is good practice to end the name with\\nthe file extension .db. In this case, we might name it surgetech_conference.db\\n(Figure 9-5).\\nFigure 9-5. Selecting a location to create a database\\nClick Save, then OK. Y ou should now see the new database in your navigator\\n(Figure 9-6).\\nFigure 9-6. Our new surgetech_conference database\\nThis database is empty, so next we will add some tables to it.\\nCreating a New Database | 75'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 91, 'page_label': '76', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CREATE TABLE\\nWhen we create a table in SQL, we use a CREATE TABLE statement. However, I am an\\nadvocate for using tools that make tasks easier. We are going to use SQLiteStudio’s\\nvisual tools to create the table, and when we are done it will generate and display the\\nCREATE TABLE statement it built for us.\\nRight-click on the Tables item in the navigator and click Create a table, as shown in\\nFigure 9-7.\\nFigure 9-7. Creating a table\\nY ou will then come to the table Structure tab. Here we add, modify, and remove col‐\\numns from our table (Figure 9-8).\\n76 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 92, 'page_label': '77', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-8. The table Structure tab, which we can use to add, modify, and remove col‐\\numns from a table\\nWe can also define various constraints to ensure data entered into the columns con‐\\nforms to rules we specify. We also supply a name for this table in the “Table name”\\nfield. Type in COMPANY for this table name. Also note there is a button to save your\\nedits and another to add a new column.\\nClick the Add Column button, and you will see a dialog to define a new column and\\nits attributes. Name this column COMPANY_ID and make its data type “INTEGER, ”\\nas shown in Figure 9-9.\\nCREATE TABLE | 77'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 93, 'page_label': '78', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-9. Defining a new COMPANY_ID column that holds integers; it also is config‐\\nured to be the primary key and will automatically populate a value via “ Autoincrement”\\nfor each inserted record\\nThis is the COMPANY_ID field, and we need to define this as the primary key for the\\nCOMPANY table. Typically, the easiest way to assign key values is to do it sequentially for\\neach new record. The first record will have a COMPANY_ID of 1, then the second record\\nwill have 2, then 3, and so on. When we INSERT records in the next chapter, this is a\\npain to do manually. But we can configure SQLite to automatically assign an ID for\\neach record we insert. Simply check Primary Key, then click Configure, then select\\nAutoincrement and click Apply (Figure 9-9).\\nFinally, click OK in the Column window and you will see our first column defined\\n(Figure 9-10).\\n78 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 94, 'page_label': '79', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-10. Our first column is defined; notice the key symbol indicating this column is\\nthe primary key\\nWe have now defined our first column, and because it was the primary key column, it\\ntook some extra work. The rest of the columns will be a little easier to set up.\\nClick on the Add Column button again to create another column (Figure 9-11). Label\\nthis column NAME and make it a VARCHAR type, which is for text that can be of\\nvarying lengths. Specify the maximum number of characters to be 30. Because we\\nlikely never want this field to be null, check the “Not NULL ” constraint. If any records\\nare added or modified with NAME set to null, then the database will reject the edits.\\nFigure 9-11. Creating a “NAME” column with type VARCHAR, a max character length\\nof 30, and a “Not NULL ” constraint\\nCREATE TABLE | 79'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 95, 'page_label': '80', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Click OK and then create two more columns, DESCRIPTION and PRIMARY_CON‐\\nTACT_ATTENDEE_ID, with the configurations shown in Figure 9-12. Note that PRI‐\\nMARY_CONTACT_ATTENDEE_ID should be a foreign key, but we have not defined\\nthat yet. We will come back to configure this after we have created its parent, the\\nATTENDEE table.\\nFigure 9-12. Creating the remaining two columns\\nFinally, click the Save Table button. Y ou will be presented with a CREATE TABLE state‐\\nment that SQLiteStudio has built for you, and will execute on your approval\\n(Figure 9-13).\\n80 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 96, 'page_label': '81', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-13. Click the Save Table button in the top toolbar, and SQLiteStudio will\\npresent the CREATE TABLE statement it is about to execute based on our inputted defi‐\\nnitions\\nHow cool is that? SQLiteStudio wrote SQL for you based on all the table definitions\\nyou built. Before you click OK, let’s take a quick look at the CREATE TABLE statement\\nto see how it works:\\nCREATE TABLE COMPANY (\\n    COMPANY_ID INTEGER PRIMARY KEY AUTOINCREMENT,\\n    NAME VARCHAR(30) NOT NULL,\\n    DESCRIPTION VARCHAR(60),\\n    PRIMARY_CONTACT_ID INTEGER NOT NULL\\n);\\nIf you inspect the SQL query, you will see the CREATE TABLE statement declares a new\\ntable named COMPANY. After that, everything in parentheses defines the table columns.\\nEach table column is defined by a name, followed by its type, and then any con‐\\nstraints or rules such as PRIMARY KEY, AUTOINCREMENT, or NOT NULL.\\nY ou could literally copy this statement and execute it in the SQL editor, but just click'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 96, 'page_label': '81', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='straints or rules such as PRIMARY KEY, AUTOINCREMENT, or NOT NULL.\\nY ou could literally copy this statement and execute it in the SQL editor, but just click\\nOK and it will execute the statement for you. After that, you should see your new\\ntable in the navigator (Figure 9-14).\\nCREATE TABLE | 81'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 97, 'page_label': '82', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-14. The COMPANY table in the navigator\\nThe AUTOINCREMENT constraint in SQLite is actually not necessary.\\nWe use it here for practice because it is necessary for other plat‐\\nforms, including MySQL. In SQLite, making a column of type\\nINTEGER a primary key will automatically make it handle its own\\nID assignment. As a matter of fact, it is actually more efficient in\\nSQLite to not use AUTOINCREMENT and let the primary key implicitly\\ndo it.\\nCreate the remaining four tables in the same manner. The needed CREATE TABLE\\nstatements are shown here (you can choose to build the tables using the Structure tab\\nor just execute the CREATE TABLE statements verbatim in the SQL editor):\\nCREATE TABLE ROOM (\\n    ROOM_ID       INTEGER PRIMARY KEY AUTOINCREMENT,\\n    FLOOR_NUMBER  INTEGER NOT NULL,\\n    SEAT_CAPACITY INTEGER NOT NULL\\n);\\nCREATE TABLE PRESENTATION (\\n    PRESENTATION_ID   INTEGER PRIMARY KEY AUTOINCREMENT,\\n    BOOKED_COMPANY_ID INTEGER NOT NULL,\\n    BOOKED_ROOM_ID    INTEGER NOT NULL,'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 97, 'page_label': '82', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=');\\nCREATE TABLE PRESENTATION (\\n    PRESENTATION_ID   INTEGER PRIMARY KEY AUTOINCREMENT,\\n    BOOKED_COMPANY_ID INTEGER NOT NULL,\\n    BOOKED_ROOM_ID    INTEGER NOT NULL,\\n    START_TIME        TIME,\\n    END_TIME          TIME\\n);\\n82 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 98, 'page_label': '83', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CREATE TABLE ATTENDEE (\\n    ATTENDEE_ID INTEGER      PRIMARY KEY AUTOINCREMENT,\\n    FIRST_NAME  VARCHAR (30) NOT NULL,\\n    LAST_NAME   VARCHAR (30) NOT NULL,\\n    PHONE       INTEGER,\\n    EMAIL       VARCHAR (30),\\n    VIP         BOOLEAN      DEFAULT (0)\\n);\\nCREATE TABLE PRESENTATION_ATTENDANCE (\\n    TICKET_ID       INTEGER PRIMARY KEY AUTOINCREMENT,\\n    PRESENTATION_ID INTEGER,\\n    ATTENDEE_ID     INTEGER\\n);\\nNote that the ATTENDEE table has a VIP field which is a Boolean (true/false) value.  By\\ndefault, if a record does not specify a value for a column, the value will default to null.\\nIt might be a good idea to default this particular field to false (0) if a value is never\\nprovided. The preceding SQL snippet reflects this, but you can also accomplish this\\nin the column builder as shown in Figure 9-15.\\nFigure 9-15. Setting a default value for a column\\nBy now, you should have all five of your tables created with all constraints defined,\\nexcept the foreign keys (Figure 9-16).'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 98, 'page_label': '83', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-15. Setting a default value for a column\\nBy now, you should have all five of your tables created with all constraints defined,\\nexcept the foreign keys (Figure 9-16).\\nCREATE TABLE | 83'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 99, 'page_label': '84', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-16. All tables have been built\\nMost database solutions enforce values in a column only by the\\nspecified data type. SQLite does not. In SQLite, you can put a TEXT\\nvalue in an INTEGER column. Other database solutions will disallow\\nthis. While this seems counterintuitive, the creators of SQLite made\\nit this way for technical reasons beyond the scope of this book.\\nSetting the Foreign Keys\\nThere is one last task remaining to make our tables airtight.  We have defined the pri‐\\nmary keys but not the foreign keys. Remember that the foreign key in a child table is\\ntied to the primary key of a parent table. Logically, we should never have a foreign key\\nvalue that does not have a corresponding primary key value.\\nFor example, we should never have a PRESENTATION record with a\\nBOOKED_COMPANY_ID value that does not exist in the COMPANY table’s COMPANY_ID col‐\\numn. If there is a BOOKED_COMPANY_ID value of 5, there had better be a record in'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 99, 'page_label': '84', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='BOOKED_COMPANY_ID value that does not exist in the COMPANY table’s COMPANY_ID col‐\\numn. If there is a BOOKED_COMPANY_ID value of 5, there had better be a record in\\nCOMPANY with a COMPANY_ID of 5 as well. Otherwise, it is an orphaned record. We can\\nenforce this by setting up foreign key constraints.\\nOpen up the PRESENTATION table and double-click the BOOKED_COMPANY_ID column to\\nmodify it (Figure 9-17). Check Foreign Key and then click Configure. Set the foreign\\ntable to CUSTOMER and the foreign column to CUSTOMER_ID. This will constrain\\nBOOKED_COMPANY_ID to only the values in the CUSTOMER_ID column in the CUSTOMER\\ntable. Click Apply, then OK.\\n84 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 100, 'page_label': '85', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-17. Making BOOKED_COMPANY_ID a foreign key to COMPANY_ID in the\\nCOMPANY table\\nClick the Commit Changes button on the Structure tab, and a series of SQL state‐\\nments will be generated to implement the foreign key. Y ou can look at the SQL if you\\nare curious, but it will only make you appreciate all the work that SQLiteStudio has\\ndone for you. Then click OK to commit the change.\\nSetting the Foreign Keys | 85'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 101, 'page_label': '86', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Using foreign keys keeps data tight and prevents deviant data from undermining the\\nrelationships. We should define foreign key constraints for all relationships in this\\ndatabase so no orphan records ever occur.\\nAt this point, you can create foreign keys for all of the following parent–child rela‐\\ntionships by repeating the same procedure:\\nCreate foreign key for [Table].[Field] Off parent primary key [Table].[Field]\\nPRESENTATION.BOOKED_ROOM_ID ROOM.ROOM_ID\\nPRESENTATION_ATTENDANCE.PRESENTATION_ID PRESENTATION.PRESENTATION_ID\\nPRESENTATION_ATTENDANCE.ATTENDEE_ID ATTENDEE.ATTENDEE_ID\\nCOMPANY.PRIMARY_CONTACT_ATTENDEE_ID ATTENDEE.ATTENDEE_ID\\nNow we have ensured every child record has a parent record, and no orphans will\\never be allowed into the database.\\nIf you ever use SQLite outside SQLiteStudio, note that the foreign\\nkey constraint enforcement might have to be turned on first.\\nSQLiteStudio has it enabled by default, but other SQLite environ‐\\nments may not.\\nCreating Views'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 101, 'page_label': '86', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='key constraint enforcement might have to be turned on first.\\nSQLiteStudio has it enabled by default, but other SQLite environ‐\\nments may not.\\nCreating Views\\nIt is not uncommon to store frequently used SELECT queries in a database.  When you\\nsave a query in a database, it is called a view. A view behaves much like a table. Y ou\\ncan run SELECT statements against it and join it to other views and tables. But the data\\nis completely derived from a SELECT query you specify, so in many cases you cannot\\nmodify the data (nor would it make sense to).\\nSuppose we run a SELECT query very often to give us a more descriptive view of the\\nPRESENTATION table, which pulls in the booked company and booked room informa‐\\ntion:\\nSELECT\\nCOMPANY.NAME as BOOKED_COMPANY,\\nROOM.ROOM_ID as ROOM_NUMBER,\\nROOM.FLOOR_NUMBER as FLOOR,\\nROOM.SEAT_CAPACITY as SEATS,\\nSTART_TIME,\\nEND_TIME\\nFROM PRESENTATION\\n \\n \\n \\n86 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 102, 'page_label': '87', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='INNER JOIN COMPANY\\nON PRESENTATION.BOOKED_COMPANY_ID = COMPANY.COMPANY_ID\\nINNER JOIN ROOM\\nON PRESENTATION.BOOKED_ROOM_ID = ROOM.ROOM_ID\\nNow suppose we want to store this query in the database so it can easily be called. We\\ncan do that by right-clicking the Views item in the navigator, then clicking Create a\\nview (Figure 9-18).\\nFigure 9-18. Creating a view\\nY ou will then be taken to a view designer window ( Figure 9-19 ). Navigate to the\\nQuery tab. Here you will paste your SELECT statement. In the “View name” field,\\nname this view PRESENTATION_VW (with “VW” an abbreviation for “VIEW”),\\nand click the green checkmark to save it. Before it executes the SQL query to create\\nthe view, SQLiteStudio will present it for review. As you can observe, the SQL syntax\\nto create a view is fairly simple. It is CREATE VIEW [view_name] AS [a SELECT\\nquery].\\nCreating Views | 87'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 103, 'page_label': '88', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure 9-19. Creating a view off a SELECT query\\nWhen you click OK, you should now see the view in your navigator under “Views”\\n(Figure 9-20 ). Double-click on it and in the Query tab you will see the query it is\\nusing, and the Data tab will have the query results.\\nFigure 9-20. Although there is no data yet, the SELECT query has been saved as a view\\ncalled PRESENTATION_VW\\nThe Data tab will be blank, until the queried tables are populated with data.\\nNote also that we can query from a view just like it was a table (and apply filters, do\\njoin operations, and do anything else you could do in a SELECT with a table):\\nSELECT * FROM PRESENTATION_VW\\nWHERE SEAT_CAPACITY >= 30\\n88 | Chapter 9: Database Design'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 104, 'page_label': '89', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Summary\\nIn this chapter, we dived into creating our own databases and learned how to design\\nthem efficiently. We studied table relationships, which help us clearly define how\\ntables are joined. We also explored some of the various column constraints (including\\nPRIMARY KEY, FOREIGN KEY, NOT NULL, AUTOINCREMENT, and DEFAULT) to keep data\\nconsistent and ensure it follows rules we define.\\nIn the next chapter, we will actually populate and modify data in this database. We\\nwill witness our design at work and appreciate the time we put into planning it. A\\ngood design with well-defined constraints will make a resilient database.\\nOne topic this chapter did not cover is indexes. Indexes are useful\\nfor tables with a large number of records but have performance\\nissues with SELECT statements. “ APPENDIX B2 – Improving Per‐\\nformance with Indexes” on page 108 discusses indexes and when\\nand when not to use them. \\nSummary | 89'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 106, 'page_label': '91', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 10\\nManaging Data\\nIn the previous chapter, we learned not only how to create a database but how to do it\\nwell. Well-considered table design, column constraints, and relationships will really\\nshine once we start putting data into the tables. With our strong table design and the\\nwell-thought-out relationships between them, we will be able to join efficiently and\\neasily. When a piece of data needs to be changed (e.g., a customer address), we only\\nneed to change it in one place rather than several. When a bad piece of data comes in,\\nhopefully we have set enough sensible constraints to prevent it from entering the\\ndatabase.\\nIn this chapter, we will learn how to INSERT, DELETE, and UPDATE records. Fortunately,\\nwriting operations like these is much simpler than writing SELECT statements.\\nThese SQL write operations do not have to be done by a human. Software (written in\\nJava, Python, or other coding languages) will often generate and execute SQL state‐'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 106, 'page_label': '91', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='These SQL write operations do not have to be done by a human. Software (written in\\nJava, Python, or other coding languages) will often generate and execute SQL state‐\\nments to read and write data the same way a human would, but much more effi‐\\nciently. Although this is beyond the scope of this book, coding languages will be\\ntouched on in the next chapter if that is pertinent to you.\\nINSERT\\nIn a relational database, data only exists if the database first receives records. The\\nINSERT statement does just that and inserts a record into the database. Y ou can pick\\nwhat fields to populate in the record, and the rest of the fields will be null or use a\\ndefault value.\\nFirst, we will INSERT an ATTENDEE record into the SurgeTech database we created in\\nthe last chapter. An INSERT to add yourself to the database should look something like\\nthis. Execute the following statement with your name:\\n91'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 107, 'page_label': '92', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"INSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME)\\nVALUES ('Thomas','Nield')\\nLet’s break this statement down:\\nINSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME)\\nVALUES ('Thomas','Nield')\\nTo start, we declare that we are inserting a record into the ATTENDEE table, and the\\nfields we are choosing to populate are FIRST_NAME and LAST_NAME:\\nINSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME)\\nVALUES ('Thomas','Nield')\\nThen we specify the values for each of these fields. Note that we specify the values in\\nthe same order we declared the fields in: 'Thomas' corresponds to FIRST_NAME, and\\n'Nield' to LAST_NAME.\\nRun SELECT * FROM ATTENDEE  to see if our INSERT made it in. Sure enough, the\\nrecord now exists (Figure 10-1).\\nFigure 10-1. Our newly inserted record in the ATTENDEE table\\nThere are a number of observations to make here. We did not populate all the col‐\\numns in our INSERT, but due to the rules we created in the previous chapter, some of\\nthe columns were assigned a default value.\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 107, 'page_label': '92', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='umns in our INSERT, but due to the rules we created in the previous chapter, some of\\nthe columns were assigned a default value.\\nThe ATTENDEE_ID gave itself a value of 1 due to our PRIMARY KEY and AUTOINCREMENT\\nrule.  If we were to INSERT another record, it would automatically be assigned an\\nATTENDEE_ID of 2, then 3, and so on. On an INSERT, you should avoid populating the\\nATTENDEE_ID field yourself and let it assign its own ID.\\nAgain, the AUTOINCREMENT constraint in SQLite is actually not nec‐\\nessary. It is needed for MySQL and other platforms, though, hence\\nwhy we are doing it for practice. In SQLite, simply making a col‐\\numn of type INTEGER a primary key will automatically assign IDs to\\nnew records.\\nPHONE and EMAIL were not specified in our INSERT, so they were left null. If either of\\nthese columns had a NOT NULL constraint and no default value policy, our INSERT\\n92 | Chapter 10: Managing Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 108, 'page_label': '93', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"would have failed. But in our design, we have allowed these two fields to be null in\\ncase our attendees prefer to be off the grid.\\nThe VIP status was not specified in our INSERT either, but we gave this field a default\\nvalue of false (0). So instead of making it null, SQLite resorted to using the default\\nvalue we specified.\\nHopefully by now, you are already appreciating that the design is working efficiently.\\nBecause of the policies we set, the columns resorted to default values when they were\\nnot provided with any.\\nMultiple INSERTs\\nIf you have a lot of records to INSERT, you do not have to do it one at a time. Y ou can\\nspecify multiple records in a single INSERT command. Simply repeat the clause fol‐\\nlowing VALUES and separate each entry with a comma:\\nINSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME, PHONE, EMAIL, VIP)\\nVALUES\\n('Jon', 'Skeeter',4802185842,'john.skeeter@rex.net', 1),\\n('Sam','Scala',2156783401,'sam.scala@gmail.com', 0),\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 108, 'page_label': '93', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"INSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME, PHONE, EMAIL, VIP)\\nVALUES\\n('Jon', 'Skeeter',4802185842,'john.skeeter@rex.net', 1),\\n('Sam','Scala',2156783401,'sam.scala@gmail.com', 0),\\n('Brittany','Fisher',5932857296,'brittany.fisher@outlook.com', 0)\\nDoing multiple inserts in this manner is far more efficient, especially if you have\\nthousands of records. If a process written in Java or Python is populating a table, it\\nshould use this syntax to insert large amounts of records rather than executing one\\nINSERT at a time. Otherwise, the process can run very slowly.\\nY ou can also INSERT records using the results from a SELECT query. This is helpful if\\nyou need to migrate data from one table to another. Just make sure the SELECT fields\\nline up with the INSERT fields and have the same order and data types:\\nINSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME, PHONE, EMAIL)\\nSELECT FIRST_NAME, LAST_NAME, PHONE, EMAIL\\nFROM SOME_OTHER_TABLE\\nTesting the Foreign Keys\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 108, 'page_label': '93', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"INSERT INTO ATTENDEE (FIRST_NAME, LAST_NAME, PHONE, EMAIL)\\nSELECT FIRST_NAME, LAST_NAME, PHONE, EMAIL\\nFROM SOME_OTHER_TABLE\\nTesting the Foreign Keys\\nLet’s take an opportunity to witness another policy of our design at work: the foreign\\nkeys.\\nRight now, we should only have four attendees with ATTENDEE_ID assignments of 1, 2,\\n3, and 4. But test this functionality by inserting a COMPANY record with a PRI\\nMARY_CONTACT_ID value of 5:\\nINSERT INTO COMPANY (NAME, DESCRIPTION, PRIMARY_CONTACT_ID)\\nVALUES ('RexApp Solutions', 'A mobile app delivery service', 5)\\nINSERT | 93\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 109, 'page_label': '94', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"This query should have failed and an error should be displayed at the bottom of the\\nwindow (Figure 10-2).\\nFigure 10-2. The foreign key constraint successfully raised an error\\nThis is good because it means our foreign key constraint has worked: it kept an\\norphan record out. The INSERT needs to have a PRIMARY_CONTACT_ID that is existent.\\nSo, if we change it to 3 (Sam Scala), the INSERT should now work correctly:\\nINSERT INTO COMPANY (NAME, DESCRIPTION, PRIMARY_CONTACT_ID)\\nVALUES ('RexApp Solutions', 'A mobile app delivery service', 3)\\nDELETE\\nThe DELETE statement is dangerously simple. It deletes all records in a table:\\nDELETE FROM ATTENDEE\\nHowever, you can conditionally delete records with a WHERE statement. If we wanted\\nto remove all records that have no contact information, we could filter to records\\nwhere PHONE and EMAIL are null:\\nDELETE FROM ATTENDEE\\nWHERE PHONE IS NULL\\nAND EMAIL IS NULL\\nBecause it is perilously easy to make mistakes with a DELETE statement, it is a good\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 109, 'page_label': '94', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='where PHONE and EMAIL are null:\\nDELETE FROM ATTENDEE\\nWHERE PHONE IS NULL\\nAND EMAIL IS NULL\\nBecause it is perilously easy to make mistakes with a DELETE statement, it is a good\\npractice to replace the DELETE with a SELECT * first. Executing that query gives us a\\npreview of what records will be deleted:\\nSELECT * FROM ATTENDEE\\nWHERE PHONE IS NULL\\nAND EMAIL IS NULL\\nTRUNCATE TABLE\\nIn the previous section, we looked at a means to delete all records from a table:\\nDELETE FROM ATTENDEE\\nAlthough not used in SQLite, on some database platforms (like MySQL) the preferred\\nway to delete all records from a table is to use TRUNCATE TABLE:\\nTRUNCATE TABLE ATTENDEE\\n94 | Chapter 10: Managing Data'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 110, 'page_label': '95', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Using this command will allow the database engine to reset the autoincrements for\\nany primary keys as well as any other constraint behaviors. It also allows it to make\\nsome optimizations behind the scenes to reset the table.\\nWhile SQLite does not support TRUNCATE TABLE, it does allow some similar optimiza‐\\ntions when you run a DELETE without a WHERE.\\nUPDATE\\nFinally, we come to the UPDATE command. The UPDATE modifies existing records. If\\nwe wanted to update the EMAIL values for all records to be uppercase, we could do\\nthat with this statement using the UPPER() function:\\nUPDATE ATTENDEE SET EMAIL = UPPER(EMAIL)\\nWe can also update multiple fields at once. Just separate each expression after the SET\\nkeyword with a comma. To update both the FIRST_NAME and LAST_NAME fields to\\nuppercase, run this command:\\nUPDATE ATTENDEE SET FIRST_NAME = UPPER(FIRST_NAME),\\nLAST_NAME = UPPER(LAST_NAME)\\nLike with DELETE, we can use a WHERE to conditionally apply updates to records. Exe‐'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 110, 'page_label': '95', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='uppercase, run this command:\\nUPDATE ATTENDEE SET FIRST_NAME = UPPER(FIRST_NAME),\\nLAST_NAME = UPPER(LAST_NAME)\\nLike with DELETE, we can use a WHERE to conditionally apply updates to records. Exe‐\\ncute the following query to set the VIP field to true where the ATTENDEE_ID is 3 or 4:\\nUPDATE ATTENDEE SET VIP = 1\\nWHERE ATTENDEE_ID IN (3,4)\\nDROP TABLE\\nThere may be times where you want to remove a table altogether from the database.\\nJust type DROP TABLE followed by the name of the table you want to delete (this is a\\ndangerous statement as well because it deletes the table permanently, so be careful\\nand certain about what you are doing):\\nDROP TABLE MY_UNWANTED_TABLE\\nSummary\\nAt this point, you have the tools you need to go out and create your own database and\\nmanage its data. Y ou may have questions on how to do all of this efficiently or pro‐\\nvide practical means for users to input and update data, because chances are you can‐'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 110, 'page_label': '95', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='manage its data. Y ou may have questions on how to do all of this efficiently or pro‐\\nvide practical means for users to input and update data, because chances are you can‐\\nnot teach all of them SQL and they will want a graphical user interface. Or maybe you\\nwant to know how to pump large amounts of data into a database automatically or\\nsynchronize it with another data source. This will be lightly addressed in the final\\nchapter.\\nUPDATE | 95'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 112, 'page_label': '97', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CHAPTER 11\\nGoing Forward\\nIf you have gotten to this chapter and covered all the material to this point, congrats!\\nY ou have learned an adaptable and marketable skillset that can be applied to the fields\\nof business, information technology, and engineering. With consistent practice and\\nusage, you should feel comfortable positioning yourself professionally with SQL and\\ndatabase design.\\nWhen it comes to understanding and using technology, everyone has a different level\\nof ambition. Y ou may very well have read this book and see this material as not cen‐\\ntral to your career goals, but rather complementary. This may be the case if you are an\\ninterdepartmental manager of sorts and just want to understand IT a little better. Y ou\\nmay feel this book has given all the insight you need, which is perfectly acceptable.\\nOnly you can define your career goals and prioritize what maximizes your value!\\nBut perhaps SQL has really clicked with you, and you have questions on how you can'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 112, 'page_label': '97', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Only you can define your career goals and prioritize what maximizes your value!\\nBut perhaps SQL has really clicked with you, and you have questions on how you can\\ndo more. Maybe you want to have a deeper knowledge of SQL and business intelli‐\\ngence. Or maybe you want to create software and graphical user interfaces that inter‐\\nact with a database. After reading this material, you may feel you are not done and\\nhave unanswered questions on how to create full business solutions. If this is you, I\\nencourage you to keep researching and pursuing whatever knowledge you need to\\nfulfill your goals.\\nIf you want to learn more detailed functionalities in SQL, there are plenty of books\\nand resources available. Y ou can expand your SQL knowledge with more advanced\\nfunctionalities like subqueries and triggers. If you want a deep understanding of\\nSQLite, check out Using SQLite by Jay A. Kreibich (O’Reilly). Y ou can also pursue'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 112, 'page_label': '97', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='functionalities like subqueries and triggers. If you want a deep understanding of\\nSQLite, check out Using SQLite by Jay A. Kreibich (O’Reilly). Y ou can also pursue\\nlearning another database platform like MySQL in greater detail. But whatever you\\ndo, do not hesitate to use the Internet as a resource. It has an infinite number of tuto‐\\nrials, guides, walkthroughs, and documentation to help you expand your knowledge.\\n97'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 113, 'page_label': '98', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='While SQL alone can increase your opportunities, you can become very adaptable by\\nlearning another relevant technology or two. If you are eager to expand your skillsets,\\nthen consider learning and integrating another topic. This will open dozens of career\\npaths. Integrating a few technology skillsets can make you even more marketable\\nthan just specializing in one skillset.\\nPython is a great programming language that is accessible to beginners, and yet is\\npowerful enough to be used by professional programmers and security testing hack‐\\ners alike. Y ou can use Python to process data into databases or write applications and\\nprocesses that support a business. It is an adaptable programming language and pairs\\nwell with database development. A great resource to get started with Python is Al\\nSweigart’s popular book Automate the Boring Stuff with Python (No Starch Press).\\nR can be used for business intelligence programming. R is a statistics programming'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 113, 'page_label': '98', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Sweigart’s popular book Automate the Boring Stuff with Python (No Starch Press).\\nR can be used for business intelligence programming. R is a statistics programming\\nlanguage used to perform deep machine learning and analysis on data. I have noticed\\nit is preferred by the business/science/math crowd because it focuses on those areas\\nvery well. It has endless functionalities to analyze anything from DNA sequences to\\nbusiness market trends. It does a great job applying classic statistical models like lin‐\\near regression. I have not used many resources on R, but I have heard Coursera offers\\nsome great online courses on it.\\nPython’s adaptability is catching up with R, as it now features libraries for data min‐\\ning. Both technologies are free and open source. When a SELECT statement does not\\nprovide enough functionality for a specific question you have about your data,\\nPython and R are powerful tools to glean that information.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 113, 'page_label': '98', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='provide enough functionality for a specific question you have about your data,\\nPython and R are powerful tools to glean that information.\\nIf you are interested in full-blown software development and not just casual scripting,\\nlanguages such as Java, Apple’s Swift, or Microsoft’s C# are great to pick up. With\\nthese languages you can make commercial-grade business software solutions and\\nmobile apps. Full-blown programming can take many hours to master, and it is chal‐\\nlenging. But if you become good at it, the work and opportunities are endless. If you\\ndecide to pursue Java, Herbert Schildt’s Java: A Beginner’s Guide (McGraw Hill Profes‐\\nsional) is a good book to start with.\\nThese definitely are not the only career paths you can take. Technology needs are so\\nniche nowadays, you may adapt yourself into a role that never has been done before.\\nJust be sure to focus on learning material that is pertinent to your immediate needs.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 113, 'page_label': '98', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='niche nowadays, you may adapt yourself into a role that never has been done before.\\nJust be sure to focus on learning material that is pertinent to your immediate needs.\\nOn top of books, there are also great websites like Pluralsight and W3Schools to gain\\nfoundational knowledge in whatever topics you choose to pursue. And never under‐\\nestimate Google when you have a specific question. Chances are if you have a ques‐\\ntion or problem, someone else has probably had it too and the answer was posted\\nonline.\\nIf you cannot find an answer to your question, there is a great Q&A site called Stack\\nOverflow filled with  professionals and enthusiasts in all areas of technology. Y ou can\\n98 | Chapter 11: Going Forward'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 114, 'page_label': '99', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='post a well-defined, researched question and get answers from other members for\\nfree. The most productive answerers often work for Google, Oracle, Microsoft, and\\nother big technology companies. Some of them have written books or are Silicon Val‐\\nley celebrities in the technology community. These people provide expertise simply\\nbecause they are passionate about what they do.\\nFinally, you are more than welcome to email me if you have any questions, concerns,\\nor comments. If you have feedback on this book or have more general questions, I\\ncan try my best to help with that. Please email me at tmnield@outlook.com—I look\\nforward to hearing from you.\\nHappy querying!\\nGoing Forward | 99'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 116, 'page_label': '101', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"APPENDIX A\\nOperators and Functions\\nThis appendix covers common operations and functions used in SQLite as a conve‐\\nnient reference. While this book does not cover all the functionalities of SQLite, the\\nfunctionalities likely to have an immediate need can be found here. The primary goal\\nis to showcase universal SQL concepts that apply to most platforms, not to teach the\\nnuances of the SQLite platform in detail.\\nA comprehensive coverage of SQLite’s features can be found at https://www.sqlite.org/\\ndocs.html.\\nAppendix A1 – Literal Expression Queries\\nY ou can test operators and functions easily without querying any tables at all. Y ou\\nsimply SELECT an expression of literals as in the following query, which will calculate\\na single value of 12:\\nSELECT 5 + 7\\nAny functions and literals, including text strings, can be tested in this manner as well.\\nThis query will check if the word 'TONY' is in the string 'TONY STARK', and it should\\nreturn 1:\\nSELECT INSTR('TONY STARK', 'TONY')\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 116, 'page_label': '101', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"This query will check if the word 'TONY' is in the string 'TONY STARK', and it should\\nreturn 1:\\nSELECT INSTR('TONY STARK', 'TONY')\\nThis is a great way to test operators and functions without using any tables. This\\nappendix will show many examples with this approach, and you can use it for your\\nown experimentation.\\n101\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 117, 'page_label': '102', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Appendix A2 – Mathematical Operators\\nSQLite has a small set of basic math operators. More advanced tasks are usually done\\nwith functions, but here are the five core mathematical operators.\\nAssume x = 7 and y = 3\\nOperator Description Example Result\\n+ Adds two numbers x + y 10\\n- Subtracts two numbers x - y 4\\n* Multiplies two numbers x * y 21\\n/ Divides two numbers x / y 2\\n% Divides two numbers, but returns the remainder x % y 1\\nAppendix A3 – Comparison Operators\\nComparison operators yield a true (1) or false (0) value based on a comparative evalu‐\\nation.\\nAssume x = 5 and y = 10\\nOperator Description Example Result\\n= and == Checks if two values are equal x = y 0 (false)\\n!= and <> Checks if two values are not equal x != y 1 (true)\\n> Checks if value on left is greater than value on right x > y 0 (false)\\n< Checks if value on left is less than value on right x < y 1 (true)\\n>= Checks if value on left is greater than or equal to value on right x >= y 0 (false)'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 117, 'page_label': '102', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='< Checks if value on left is less than value on right x < y 1 (true)\\n>= Checks if value on left is greater than or equal to value on right x >= y 0 (false)\\n<= Checks if value on left is less than or equal to value on right x <= y 1 (true)\\nAPPENDIX A4 – Logical Operators\\nLogical operators allow you combine Boolean expressions as well as perform more\\nconditional operations.\\nAssume x = true (1) and y = false (0)\\nAssume a = 4 and b = 10\\nOperator Description Example Result\\nAND Checks for all Boolean expressions to be true x AND y 0 (false)\\nOR Checks for any Boolean expression to be true x OR y 1 (true)\\nBETWEEN Checks if a value inclusively falls inside a range a BETWEEN 1 and b 1 (true)\\n102 | Appendix A: Operators and Functions'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 118, 'page_label': '103', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Operator Description Example Result\\nIN Checks if a value is in a list of values a IN (1,5,6,7) 0 (false)\\nNOT Negates and flips a Boolean expression’s value a NOT IN (1,5,6,7) 1 (true)\\nIS NULL Checks if a value is null a IS NULL 0 (false)\\nIS NOT NULL Checks if a value is not null a IS NOT NULL 1 (true)\\nAPPENDIX A5 – Text Operators\\nText has a limited set of operators, as most text processing tasks are done with func‐\\ntions. There are a few, though. Keep in mind also that regular expressions are beyond\\nthe scope of this book, but they are worth studying if you ever work heavily with text\\npatterns.\\nAssume city = ‘Dallas’ and state = ‘TX’\\nOperator Description Example Result\\n|| Concatenates one or more values together into text city || ', ' || state Dallas, TX\\nLIKE Allows wildcards _ and % to look for text patterns state LIKE 'D_l%' 1 (true)\\nREGEXP Matches a text pattern using a regular expression state REGEXP '[A-Z]{2}' 1 (true)\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 118, 'page_label': '103', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"LIKE Allows wildcards _ and % to look for text patterns state LIKE 'D_l%' 1 (true)\\nREGEXP Matches a text pattern using a regular expression state REGEXP '[A-Z]{2}' 1 (true)\\nA special note to programmers: REGEXP is not implemented out of\\nthe box for SQLite, so you may have to compile or implement it\\nwhen using SQLite for your app or program.\\nAPPENDIX A6 – Common Core Functions\\nSQLite has many core functions built in. While this is not a comprehensive list, these\\nare the most commonly used ones. A full list of functions and their documentation\\ncan be found at http://www.sqlite.org/lang_corefunc.html.\\nAssume x = –5, y = 2, and z is NULL\\nOperator Description Example Result\\nabs() Calculates the absolute value of a number abs(x) 5\\ncoalesce() Converts a possible null value into a default value if it is null coalesce(z, y) 2\\ninstr() Checks if a text string contains another text string; if so it\\nreturns the index for the found position, and otherwise it\\nreturns 0\\ninstr('HTX','TX') 2\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 118, 'page_label': '103', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"instr() Checks if a text string contains another text string; if so it\\nreturns the index for the found position, and otherwise it\\nreturns 0\\ninstr('HTX','TX') 2\\nlength() Provides the number of characters in a string length('Dallas') 6\\ntrim() Removes extraneous spaces on both sides of a string trim(' TX ') TX\\nOperators and Functions | 103\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 119, 'page_label': '104', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Operator Description Example Result\\nltrim() Removes extraneous spaces on the left side of a string ltrim(' TX') TX\\nrtrim() Removes extraneous spaces on the right side of a string rtrim('LA ') LA\\nrandom() Returns a pseudorandom number from the range\\n–9223372036854775808 to +9223372036854775807\\nrandom() 7328249\\nround() Rounds a decimal to a specified number of decimal places round(182.245, 2) 182.24\\nreplace() Replaces a substring of text in a string with another string replace('Tom \\nNield','Tom',\\n'Thomas')\\nThomas\\nNield\\nsubstr() Extracts a range of characters from a string with their numeric\\npositions\\nsubstr('DOG',2,3) OG\\nlower() Turns all letters in a string to lowercase lower('DoG') dog\\nupper() Turns all letters in a string to uppercase upper('DoG') DOG\\nAPPENDIX A7 – Aggregate Functions\\nSQLite has a set of aggregate functions you can use with a GROUP BY statement to get a\\nscoped aggregation in some form.\\nX = a column you specify the aggregation on\\nFunction Description\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 119, 'page_label': '104', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='SQLite has a set of aggregate functions you can use with a GROUP BY statement to get a\\nscoped aggregation in some form.\\nX = a column you specify the aggregation on\\nFunction Description\\navg(X) Calculates the average of all values in that column (omits null values).\\ncount(X) Counts the number of non-null values in that column.\\ncount(*) Counts the number of records.\\nmax(X) Calculates the maximum value in that column (omits null values).\\nmin(X) Calculates the minimum value in that column (omits null values).\\nsum(X) Calculates the sum of the values in that column (omits null values).\\ngroup_concat(X) Concatenates all non-null values in that column. You can also provide a second argument specifying\\na separator, like commas.\\nAPPENDIX A8 – Date and Time Functions\\nFunctionality for dates and times in SQL varies greatly between database platforms.\\nTherefore, this book does not cover this topic outside this appendix.  Y ou will need to'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 119, 'page_label': '104', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Functionality for dates and times in SQL varies greatly between database platforms.\\nTherefore, this book does not cover this topic outside this appendix.  Y ou will need to\\nlearn the date/time syntax for your specific database platform. Some platforms, such\\nas MySQL, make working with date/time values very intuitive, while others can be\\nless intuitive.\\n104 | Appendix A: Operators and Functions'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 120, 'page_label': '105', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"For SQLite, date and time functions cannot be comprehensively covered here as that\\nwould be beyond the scope of this book. But the most common date and time tasks\\nwill be covered in this section. Full documentation of SQLite date and time handling\\ncan be found at the SQLite website (http://www.sqlite.org/lang_datefunc.html).\\nDate Functions\\nWhen working with dates, it is simplest to store them in the string format YYYY-\\nMM-DD as most database platforms inherently understand this format (technically\\ncalled the ISO8601 format). A four-digit year comes first, following by a two-digit\\nmonth, and a two-digit day, each separated by a dash (e.g., 2015-06-17). If you format\\nyour date strings like this, you will never have to do any explicit conversions.\\nWhen running a query, any string in the 'YYYY-MM-DD' date format will be inter‐\\npreted as a date. This means you can do chronological tasks like comparing one date\\nto another date:\\nSELECT '2015-05-14' > '2015-01-12'\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 120, 'page_label': '105', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"preted as a date. This means you can do chronological tasks like comparing one date\\nto another date:\\nSELECT '2015-05-14' > '2015-01-12'\\nIf you do not use this ISO8601 format, SQLite will compare them as text strings and\\ncheck if the first text comes before the second text alphabetically. This obviously is\\nnot desirable as you want dates to be evaluated, compared, and treated as dates.\\nConveniently, you can get today’s date by passing a 'now' string to the DATE() func‐\\ntion:\\nSELECT DATE('now')\\nSQLite also allows you to pass any number of modifier arguments to the DATE() func‐\\ntion. For instance, you can get yesterday’s date by passing '-1 day' as an argument:\\nSELECT DATE('now','-1 day')\\nY ou can pass also pass a date string to the DATE() function, and add any number of\\nmodifiers to transform the date. This example will add three months and subtract one\\nday from 2015-12-07:\\nSELECT DATE('2015-12-07','+3 month','-1 day')\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 120, 'page_label': '105', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"modifiers to transform the date. This example will add three months and subtract one\\nday from 2015-12-07:\\nSELECT DATE('2015-12-07','+3 month','-1 day')\\nThere are several advanced date transformations you can perform. Refer to the\\nSQLite date functions link at the beginning of this section to get a comprehensive\\noverview of these functionalities.\\nTime Functions\\nTime also has a typical format, which is HH:MM:SS (this also is ISO8601 standard).\\nHH is a two-digit military format of hours, MM is a two-digit format of minutes, and\\nSS is a two-digit format of seconds . The separator is a colon. If you format times like\\nOperators and Functions | 105\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 121, 'page_label': '106', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"this, the strings will always be interpreted as time values. This string represents a time\\nvalue of 4:31 PM and 15 seconds:\\nSELECT '16:31:15'\\nY ou can omit the seconds value if you are not concerned with it. SQLite will infer the\\nseconds value to be 00:\\nSELECT '16:31'\\nJust like with dates, you can do all kinds of operations with times, like comparing one\\ntime value to another:\\nSELECT '16:31' < '08:31'\\nThe 'now' string also works with the TIME() function to get the current time:\\nSELECT TIME('now')\\nAlso like with dates, you can use the TIME() function to perform transformations\\nsuch as adding or subtracting hours, minutes, and seconds:\\nSELECT TIME('16:31','+1 minute')\\nDate/Time Functions\\nY ou can have a date that also has a time value. Reasonably, the standard string format\\nis the date and time formats concatenated together, separated by a space: ‘YYYY-\\nMM-DD HH:MM:SS’ . SQLite will recognize a string in this format to be a date/time\\nvalue:\\nSELECT '2015-12-13 16:04:11'\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 121, 'page_label': '106', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"is the date and time formats concatenated together, separated by a space: ‘YYYY-\\nMM-DD HH:MM:SS’ . SQLite will recognize a string in this format to be a date/time\\nvalue:\\nSELECT '2015-12-13 16:04:11'\\nAll the rules from the DATE() and TIME() functions can apply to the DATETIME()\\nfunction. Y ou can use 'now', transformations, and any other chronological opera‐\\ntions we have learned. For instance, you can subtract a day from a date/time value\\nand add three hours:\\nSELECT DATETIME('2015-12-13 16:04:11','-1 day','+3 hour')\\nThere are several other functions and features for working with dates and time in\\nSQLite, including converting dates into alternative formats and compensating for\\ntimes zones. There is also support for Unix time and the Julian day number system.\\nAs said earlier, go to http://www.sqlite.org/lang_datefunc.html to get a comprehensive\\nlist of these functionalities.\\n106 | Appendix A: Operators and Functions\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 122, 'page_label': '107', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='APPENDIX B\\nSupplementary Topics\\nThere are some database and SQL tasks that, at the time of writing, did not fit into the\\nmission of this book. The intention was to get your feet wet with SQL and not inun‐\\ndate you with every functionality available. But there are some topics that arguably do\\nnot fit into the core mission of this book, yet do not deserve to be omitted either.\\nThese are given mention here to help you progress in your proficiency.\\nAPPENDIX B1 – Further Topics of Interest\\nThis is a beginner’s book on SQL. Therefore, the scope and focus is limited to founda‐\\ntional topics. However, if you finished this book and are interested in expanding your\\ndatabase and SQL repertoire, here are some suggested topics you can explore and\\nresearch:\\nTopic Description\\nUNION and UNION \\nALL\\nAppend the results of two or more queries into a single result set.\\nSubqueries Query off other queries just like they were tables.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 122, 'page_label': '107', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='research:\\nTopic Description\\nUNION and UNION \\nALL\\nAppend the results of two or more queries into a single result set.\\nSubqueries Query off other queries just like they were tables.\\nIndexes Improve the SELECT performance of a table with large amounts of data (addressed briefly in\\n“APPENDIX B2 – Improving Performance with Indexes” on page 108 ).\\nTransactions Perform multiple UPDATE/DELETE/INSERT statements as one fail-safe batch (addressed briefly in\\n“Appendix B3 – Transactions” on page 109 ).\\nTriggers React to UPDATE/DELETE/INSERT statements and perform tasks like logging and advanced data\\nvalidation.\\nRegular expressions Use a universal syntax to match advanced text patterns easily—basically, LIKE wildcards on\\nsteroids.\\nDatabase\\nadministration\\nFine-tune production databases for large corporate environments.\\n107'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 123, 'page_label': '108', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Y ou will probably encounter dozens of other topics, especially as you explore the\\nnuances of different database platforms. But these should provide plenty of leads to\\nexpand your database knowledge beyond this book.\\nAPPENDIX B2 – Improving Performance with Indexes\\nAs your database grows, the performance can start to slow down with SELECT queries.\\nThe machine has to process each record to find ones that match your WHERE condi‐\\ntion, and obviously having more records will slow this process down.\\nA common way to improve performance significantly is to use indexes, a mechanism\\nthat enables faster lookups in a way very similar to an index in a book. An index\\nspeeds up SELECT performance, but it slows down INSERT, UPDATE, and DELETE state‐\\nments. It will also make the database file larger. These are factors you have to balance\\nin your decision to use them. Y ou should not think about creating indexes when you'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 123, 'page_label': '108', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='ments. It will also make the database file larger. These are factors you have to balance\\nin your decision to use them. Y ou should not think about creating indexes when you\\nfirst design a database. Do it later, when you find you are having performance issues.\\nY ou specify an index on one or more columns, and you want these columns to be the\\nones you frequently qualify on. For example, if you frequently query the PRODUCT\\ntable and use a WHERE statement on the price column, you can apply an index on that\\ncolumn as shown here:\\nCREATE INDEX price_index ON PRODUCT(price);\\nWe name the index price_index and we apply it on the PRODUCT table, and in paren‐\\ntheses we specify it on the price column. SQLite will keep a map of which records\\nhave which price values. This will significantly speed up performance when we qual‐\\nify on price. But obviously, when we modify records it has to update this index, so\\nthis overhead will slow down INSERT, UPDATE, and DELETE operations.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 123, 'page_label': '108', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='ify on price. But obviously, when we modify records it has to update this index, so\\nthis overhead will slow down INSERT, UPDATE, and DELETE operations.\\nY ou will notice in the SQLiteStudio database navigator that the table contains all the\\nindex objects you have created (see Figure B-1).\\n108 | Appendix B: Supplementary Topics'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 124, 'page_label': '109', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Figure B-1. The price_index was added to the PRODUCT table’s indexes\\nY ou can also create a UNIQUE index for a column that never has duplicate values, and\\nSQLite will make special optimizations for that case:\\nCREATE UNIQUE INDEX name_index ON CUSTOMER(name);\\nIn addition, you can use composite indexes if two or more fields are frequently quali‐\\nfied together, but that is beyond the scope of this book.\\nTo remove an index, just run a DROP INDEX statement on the index’s name:\\nDROP INDEX price_index;\\nAgain, indexes should only be used for very large tables that have noticeable perfor‐\\nmance issues with SELECT statements. Y ou should avoid using indexes on small tables\\nas the overhead will actually slow performance down (meaning this example was\\ndemonstrational, not something you should actually do to the PRODUCT table). Y ou\\nshould also avoid using indexes on tables that update heavily and frequently.\\nAppendix B3 – Transactions'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 124, 'page_label': '109', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='demonstrational, not something you should actually do to the PRODUCT table). Y ou\\nshould also avoid using indexes on tables that update heavily and frequently.\\nAppendix B3 – Transactions\\nThere may be situations where you will want to execute multiple INSERT, UPDATE, or\\nDELETE statements as a batch, but you want all of them to complete successfully, and if\\none fails you want all of them to fail. This is known as atomicity, which means these\\nactions must all happen successfully or none of them happen at all.\\nSupplementary Topics | 109'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 125, 'page_label': '110', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='A good example where this kind of behavior is needed is financial transactions, like\\nbank account transfers or payment services like PayPal. When you take money from\\none account and put it in another, you have to make sure both operations happen\\nsuccessfully.\\nTake these two INSERT statements that move $187.56 from one account and put it in\\nanother:\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (1563,-187.56);\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (3067,187.56);\\nWhat happens if the first INSERT succeeds but the second one fails? Well, that $187.56\\nhas effectively disappeared. Y ou have two upset customers and a possible auditing\\nmess. So how do you ensure that in the event of failure, that money returns back to\\nthe customer giving it and everything is restored to the way it was?\\nThe answer is to leverage a transaction. With a transaction you can make this transfer\\natomic and do a ROLLBACK if anything fails and a COMMIT if everything succeeds.'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 125, 'page_label': '110', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='The answer is to leverage a transaction. With a transaction you can make this transfer\\natomic and do a ROLLBACK if anything fails and a COMMIT if everything succeeds.\\nFirst, call the BEGIN or BEGIN TRANSACTION command (these are the same command):\\nBEGIN TRANSACTION;\\nNow any INSERT, UPDATE, and DELETE statements will be recorded so they can be\\nundone if necessary. Perform the two INSERTs. The actions will be performed while\\nbeing recorded in “transaction mode”:\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (1563,-187.56);\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (3067,187.56);\\nIf everything goes well and no errors occurred, you can call COMMIT or its alias, END\\nTRANSACTION, to finalize the INSERTs. The transfer has then happened successfully.\\nNow let’s start another transaction so we can do another transfer:\\nBEGIN TRANSACTION;\\nHowever, this time we are going to break it. Say we do another transfer between these\\ntwo accounts:'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 125, 'page_label': '110', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='Now let’s start another transaction so we can do another transfer:\\nBEGIN TRANSACTION;\\nHowever, this time we are going to break it. Say we do another transfer between these\\ntwo accounts:\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (1563,-121.36);\\nINSERT INTO ACCOUNT_ACTIVITY (ACCOUNT_ID,AMOUNT) VALUES (121.36);\\nThe first statement will succeed, but the second SQL statement was messed up and\\nwill error out. It is missing the ACCOUNT_ID value, and now we have $121.36 in limbo.\\nFortunately, we are in “transaction mode. ” We can basically hit a rewind button and\\ncall ROLLBACK. This will undo everything since the last COMMIT or BEGIN TRANSACTION\\n110 | Appendix B: Supplementary Topics'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 126, 'page_label': '111', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='we called. The first INSERT will be undone and the $121.36 will be back in the 1563\\naccount.\\nIn the event that a database connection goes down, a bad SQL statement is composed,\\nor a validation rule train-wrecks a series of updates, transactions are a way to ensure\\ndata does not get corrupted in your database. Y ou will especially want to use them\\nwith automated processes or large jobs requiring you INSERT, UPDATE, and DELETE a\\nhigh volume of fragile records.\\nSupplementary Topics | 111'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 128, 'page_label': '113', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content=\"Index\\nSymbols\\n!= (inequality) operator, 31, 102\\n% (percent sign)\\nmodulus operator, 32\\nwildcard character, 33\\n' ' (quotation marks, single), enclosing literals,\\n32\\n() (parentheses), grouping with, 36\\n; (semicolon), ending SQL statements, 22\\n< (less than) operator, 31, 102\\n<= (less than or equal to) operator, 31, 36, 102\\n<> (inequality) operator, 31, 102\\n= (equals) sign\\n= (equals) operator, 30, 32\\n= and == operators, 102\\n> (greater than) operator, 31, 36, 102\\n>= (greater than or equal to) operator, 31, 102\\n_ (underscore)\\nin SQL names, 25\\nwildcard character, 33\\n|| (concatenation) operator, 27, 103\\nA\\nAccess, 5\\naggregate functions, 42-45, 104\\nnull values and, 43\\naggregating data, 39\\nfiltering different aggregate values using\\nzero/null CASE trick, 49\\nfiltering out records based on aggregated\\nvalue, 45\\naliases, 24\\nin HAVING statement, 45\\nAND statement, 31\\nAS statement, 24\\nASC operator, 42\\natomicity, 109\\nAUTOINCREMENT constraint, 78, 92\\nAVG() function, 43\\nB\\nBEGIN TRANSACTION command, 110\"),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 128, 'page_label': '113', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='value, 45\\naliases, 24\\nin HAVING statement, 45\\nAND statement, 31\\nAS statement, 24\\nASC operator, 42\\natomicity, 109\\nAUTOINCREMENT constraint, 78, 92\\nAVG() function, 43\\nB\\nBEGIN TRANSACTION command, 110\\nBETWEEN statement, 31\\nboolean expressions\\nin CASE statements, 51\\nlogical operators with, 102\\nBoolean values, 83\\nC\\nCASE statement, 47-52\\ngrouping, 48\\nzero/null trick, 49-52\\ncase, SQL commands, 30\\ncentralized databases, 6\\nchild relationships, 54\\nclients, 6\\ncoalesce() function, 65\\ncolumns\\naliasing, 25\\nchoosing for SELECT statement, 21\\nCOMMIT statement, 110\\ncomparison operators, 102\\nconcatenation operator (||), 27\\nconstraints, 77\\nforeign key, setting up, 84\\nCOUNT() function, 39\\ncounting non-null values in a column, 42\\nCREATE INDEX statement, 108\\n113'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 129, 'page_label': '114', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='CREATE TABLE statement, 76-84\\nCREATE VIEW AS statement, 87\\nD\\ndata questions (database design), 67\\ndata types, 16\\nDatabase Navigator (SQLiteStudio), 10\\ndatabase schemas, 71\\ndatabases, 3-7\\ncentralized, 6\\nchoosing a database solution, 5\\ncreating a new database, 73-89\\nCREATE TABLE, 76-84\\nselecting a location, 75\\nsetting foreign keys, 84\\nviews, 86\\ndefined, 3\\ndesigning, 67-73\\nexample, SurgeTech conference, 69-70\\nplanning a database, 67\\nprimary and foreign keys, 70\\nschema, 71\\nlightweight, 5\\nrelational databases, exploring, 3\\nseparate tables, reasons for, 4\\ndate and time functions, 104\\nDELETE statement, 94\\nDESC operator, 42\\ndesign questions (databases), 67\\nDISTINCT operator, 46\\nDROP INDEX statement, 109\\nDROP TABLE statement, 95\\nduplicates, eliminating from results, 46\\nE\\neditors (SQL)\\nSQLiteStudio, 9\\nELSE clauses in CASE statements, 47\\nEND keyword, ending CASE statements, 47\\nEND TRANSACTION command, 110\\nexpressions\\nin SELECT statements, 23\\nusing with data types other than numbers,\\n27\\nF'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 129, 'page_label': '114', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='ELSE clauses in CASE statements, 47\\nEND keyword, ending CASE statements, 47\\nEND TRANSACTION command, 110\\nexpressions\\nin SELECT statements, 23\\nusing with data types other than numbers,\\n27\\nF\\nforeign keys, 71\\nsetting, 84\\ntesting, 93\\nFROM statement, INNER JOIN in, 56\\nfunctions\\naggregate, 42-45, 45, 104\\nbuilt-in functions in SQLite, 25\\ncommon core functions in SQLite, 103\\ndate and time, 104\\nG\\nGROUP BY operator, 39-41\\naliases and, 45\\ngrouping JOINs, 63\\nORDER BY operator and, 41\\nusing ordinal positions, 41, 49\\ngrouping records, 39\\nH\\nHAVING statement, 45\\nI\\nIN statement, 32\\nusing on text, 33\\nindexes, 108\\nINNER JOIN operator, 55\\nLEFT JOIN and, 65\\nrecords excluded in, 57\\nINSERT statement, 91\\nmultiple INSERTs with one command, 93\\ntesting foreign keys, 93\\ntransaction mode, 110\\nINTEGER type, 16\\nIS NULL operator, 60\\nJ\\nJava, learning more about, 98\\njoins, 53-66\\ngrouping, 63\\nINNER JOIN, 55\\njoining multiple tables, 61\\nleft outer join, 58\\nRIGHT JOIN and OUTER JOIN, 61\\nL\\nLEFT JOIN operator, 58, 64'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 129, 'page_label': '114', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='J\\nJava, learning more about, 98\\njoins, 53-66\\ngrouping, 63\\nINNER JOIN, 55\\njoining multiple tables, 61\\nleft outer join, 58\\nRIGHT JOIN and OUTER JOIN, 61\\nL\\nLEFT JOIN operator, 58, 64\\nnull values, inclusion of, 65\\nleft outer joins, 58\\nlightweight databases, 5\\nliterals, 32\\n114 | Index'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 130, 'page_label': '115', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='literal expression queries, 101\\nlogical operators, 102\\nM\\nmathematical operators, 26, 102\\nMAX() function, 44, 50\\nMicrosoft Access, 5\\nMIN() function, 50\\nmodulus operator (%), 32\\nMySQL, 7\\nN\\nnames in SQL\\nunderscore(_) in, 25\\nusing aliases, 24\\nnon-null values, counting in a column, 42\\nnormalization, 4\\nNOT IN statement, 32\\nNot NULL constraint, 79\\nnull values\\naggregate functions and, 43\\nand Boolean values in tables, 83\\nconverting to zeros with coalesce(), 65\\nin LEFT JOINs versus INNER JOINs, 65\\nresulting from LEFT JOIN, checking for, 60\\nzero/null CASE trick, 49-52\\nO\\none-to-many relationships, 54\\noperators\\ncomparison, 102\\nconcatenation, 27\\nlogical, 102\\nmathematical, 26, 102\\ntext, 103\\nOR statement, 32\\nORDER BY operator, 41\\norder of operations, 37\\nordinal positions, 41, 49\\nOUTER JOIN operator, 61\\nP\\nparent-child relationships, 54\\nperformance, improving with indexes, 108\\nPRIMARY KEY constraint, 81, 92\\nprimary keys, 70\\ndefining in SQLiteStudio, 78\\nin SQLite, 82'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 130, 'page_label': '115', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='OUTER JOIN operator, 61\\nP\\nparent-child relationships, 54\\nperformance, improving with indexes, 108\\nPRIMARY KEY constraint, 81, 92\\nprimary keys, 70\\ndefining in SQLiteStudio, 78\\nin SQLite, 82\\nprogramming languages, resources for, 98\\nPython, learning more about, 98\\nQ\\nQuery Results pane (SQLiteEditor), 19\\nR\\nR language, learning more about, 98\\nrelational database management system\\n(RDBMS), 3\\nrelational databases, 3\\nrelationships between tables, 54\\nand joining multiple tables, 61\\nin the database schema, 71\\nRIGHT JOIN operator, 61\\nROLLBACK command, 110\\nrolling up data, 39\\nround() function, 26, 43\\nS\\nschemas (database), 71\\nsecurity questions (database design), 68\\nSELECT statement, 19-28\\nDISTINCT operator, 46\\nexecuting before DELETEs, 94\\nexpressions in, 23\\ninserting records using results from, 93\\nliteral expression queries, 101\\nspecifying columns for, 21\\nspreading across multiple lines, 24\\nstoring frequently used SELECT queries in a\\ndatabase, 86\\ntext concatenation in, 27'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 130, 'page_label': '115', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='literal expression queries, 101\\nspecifying columns for, 21\\nspreading across multiple lines, 24\\nstoring frequently used SELECT queries in a\\ndatabase, 86\\ntext concatenation in, 27\\nwriting and executing in SQLiteStudio, 20\\nsemicolon (;), ending SQL statements, 22\\nservers, 6\\nSET keyword, 95\\nsoftware development, 98\\nspaces in SQL names, underscore(_) as place‐\\nholder, 25\\nSQL\\ndatabase solutions, 5\\nmarketability of, 1\\nresources for further learning, 97\\nuses of, 2\\nSQL injection, 68\\nSQL Work Area (SQLiteStudio), 10\\nSQLite, 5, 9-17\\nIndex | 115'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 131, 'page_label': '116', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='advantages and limitations of, 9\\naggregate functions, 104\\ncommon core functions, 103\\ndate and time functions, 105\\nfunctions in, 25\\noperators, 101\\nsecurity, 68\\nSQLiteStudio, 9\\ncreating tables, 76-84\\nimporting and navigating databases, 10\\nSQL Editor and Query Results panes, 19\\nSUM() function, 44\\nfiltering on value of, 45\\nsumming CASE statements, 50\\nT\\ntables\\nCREATE TABLE statement, 76-84\\nexamining in SQLiteStudio, 14\\nin relational databases, 3\\nin the database schema, 71\\njoining, 53\\n(see also joins)\\nrelationships between, 54\\nseparation in relational databases, 4\\ntext\\nconcatenating, 27\\noperators for, 103\\nusing WHERE on, 32\\nTEXT type, 16\\nTHEN keyword, 47\\ntime functions, 105\\ntopics of interest, 107\\ntransactions, 109\\nTRUNCATE TABLE statement, 94\\nU\\nunique index, creating, 109\\nUPDATE statement, 95\\nUPPER() function, 95\\nV\\nV ALUES keyword, 91\\nin multiple INSERTs, 93\\nV ARCHAR type, 79\\nviews, creating, 86\\nW\\nwebsites for further learning, 98\\nWHEN keyword, 47\\nWHERE statement, 29-37\\nAND operator, 31'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 131, 'page_label': '116', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='UPPER() function, 95\\nV\\nV ALUES keyword, 91\\nin multiple INSERTs, 93\\nV ARCHAR type, 79\\nviews, creating, 86\\nW\\nwebsites for further learning, 98\\nWHEN keyword, 47\\nWHERE statement, 29-37\\nAND operator, 31\\nchecking for null values from LEFT JOIN,\\n60\\nconditionally applying updates with, 95\\nconditionally deleting records with, 94\\nfiltering aggregations and, 45, 49\\nfiltering records, 29\\ngrouping conditions with parentheses, 37\\nIN operator, 32\\nOR operator, 32\\nORDER BY operator, 41\\nusing on numbers, 30\\nusing on text, 32\\nwhitespace in SQL, 24\\nZ\\nZero/Null CASE trick, 49-52\\n116 | Index'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 132, 'page_label': '117', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='About the Author\\nThomas Nield has a business analyst background and works in revenue management\\nat Southwest Airlines. Early in his career, he became fascinated with technology and\\nbought dozens of books to master programming in Java, C#, and database design. He\\nis passionate about sharing what he learns and enabling others with new skillsets,\\neven if they do not work in IT. He enjoys making technical content relatable and rele‐\\nvant to those unfamiliar with or intimidated by it.\\nColophon\\nThe animal on the cover of Getting Started with SQL  is a Natterjack toad ( Epidalea\\ncalamita). It is part of the Bufonidae family and can be found in sand dune systems,\\nheathlands, and coastlines with low coverings of grass throughout western European.\\nAn identifying feature of natterjack toads is the yellow line that runs down the middle\\nof their backs. Adults range in length from 50–70 mm, with females being larger than'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 132, 'page_label': '117', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='An identifying feature of natterjack toads is the yellow line that runs down the middle\\nof their backs. Adults range in length from 50–70 mm, with females being larger than\\nmales. Overall coloring is either brown, cream, or green, and they are covered in\\nwarts, like their toad brethren. Another distinguishing feature is its shorter hind legs,\\nwhich make it more of a runner than a hopper or walker, as other toads are.\\nLike more common toads, the natterjack diet consists of many invertebrate insects\\nsuch as beetles, spiders, and worms. They are nocturnal hunters, so meals are swal‐\\nlowed whole at night. Those in sand dune systems are also known to eat small crusta‐\\nceans, such as freshwater shrimp. Natterjacks release toxins from their skin, which\\nmake them unlikely to be made into meals themselves, but birds such as grey herons,\\nas well as grass snakes are able to consume them without issue.\\nAs with hunting, the natterjack’s mating rituals are nocturnal. Males have a dinstinct,'),\n",
       " Document(metadata={'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-02-09T20:28:58+00:00', 'author': 'Thomas Nield', 'moddate': '2016-02-09T15:39:30-05:00', 'title': 'Getting Started with SQL', 'trapped': '/False', 'source': '../data/Getting Started with SQL.pdf', 'total_pages': 133, 'page': 132, 'page_label': '117', 'source_file': 'Getting Started with SQL.pdf', 'file_type': 'pdf'}, page_content='as well as grass snakes are able to consume them without issue.\\nAs with hunting, the natterjack’s mating rituals are nocturnal. Males have a dinstinct,\\nloud mating call that signals females to head to nearby warm, shallow waters (because\\nnatterjacks are terrible swimmers). This usually occurs between the months of April\\nand July, with females spawning 1,500–7,500 eggs. The eggs turn into tadpoles about\\na week after fertilization, which then turn into toadlets 3–8 weeks later.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com.\\nThe cover image is from Johnson’s Natural History. The cover fonts are URW Type‐\\nwriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is\\nAdobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe92ea",
   "metadata": {},
   "source": [
    "### embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ae3031",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Settings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543614c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x26010000ec0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c9e3b",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c276d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2604e956cf0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5d2c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='comments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The\\ndata transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciﬁc training strate gies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='example generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-\\ning initial retrieval-focused pretraining followed by ful l-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more div erse data is crucial for\\nadvancing retrieval model performance, and that leveragin g LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Our model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-\\ntor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhan ced agent systems\\n∗ https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on ﬁne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiﬁcant performance improvements[ 4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='the limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ”signal states” repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='the objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe ﬁnal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='dient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance\\nof up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='enhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neﬃcient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiﬁcation. Our framework comprises two core components : 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc require ments of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extractio n from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s charact eristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and g eneralization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the ﬁrst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='capability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pa ir classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='the advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—suc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]—combined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model’s dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='followed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='we also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='for improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Embedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='to identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constrainin g tokens’ ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='categories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted in to a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ”Question/Query/User” portion is converted into que ries, while the\\n”Answer/Response/Assistant” portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='mantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical va lue of 1, while\\n”no”/”false” are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='tion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diﬀeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by us ing the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diﬀerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to\\nto enhance model training eﬃciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='additional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = − 1\\nn\\n∑\\ni\\nlog esim(qi,d +\\ni )/τ\\nesim(qi,d +\\ni )/τ + ∑\\nj esim(qi,d −\\nj )/τ + ∑\\nj̸=i esim(qi,q j )/τ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n∑\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) − sim(xi, x j)\\nτ )) (2)\\n3.3.3 CLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiﬁcation datasets with limited categories may lead to false neg atives from items\\nof diﬀerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='sample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = − 1\\nn\\n∑\\ni\\nlog esim(ti,t +\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/τ +\\n∑\\nn\\nMASK(ti, t −\\ni,n ) ·esim(ti,t −\\ni,n )/τ +\\n∑\\nj̸=i\\nMASK(ti, t j ) ·esim(ti,t j )/τ +\\n∑\\nj̸=i\\n∑\\nn\\nMASK(ti, t −\\nj,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='portance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃcu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='which represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must\\naccurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantic ally equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='semantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcat ions yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='tailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='enable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lα\\ni∑ m\\nj=1 lα\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed’s[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusive ly uses retrieval-\\noriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='datasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for η,\\nwhile non-retrieval data constitutes 1 − η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scalin g factor α , a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Sret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ ηRET ·lα\\ni\\nSret\\nif di ∈ RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='passage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface’s sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Classiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-\\nproximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='conﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations f or all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bﬂoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='boards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='both task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding\\nsecured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/c lassiﬁcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1\\nSeed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='NV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99\\nxiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Qwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transform ation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we d eveloped a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='strategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eﬀective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’9 4: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine le arning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Rangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-s hot learners.”\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”F ine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th Int ernational ACM\\nSIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Raghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large languag e models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jaspe r and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv :2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embedd ings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ ıc Magne, and Nils Reimers . ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embed ding: Gen-\\neral text embedding with more and better negative samples.” arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–id f measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='mation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGI R’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with tran sfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='multi-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Through Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embeddin g.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer norma lization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ”Glu variants improve transformer.” arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Wang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapor e. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Generative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345–2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unn atural in-\\nstructions: Tuning language models with (almost) no human labor.” ar Xiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Schiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='models, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis\\nplatform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='linguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation .” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='ing comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and\\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558–3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='explainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association fo r Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriﬁcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='should not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='ity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='reason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='manuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='Laureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='tory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The Faerie Queene’, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='signiﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork it wasn’t an epic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conﬂicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\emneddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'emneddings.pdf', 'file_type': 'pdf'}, page_content='wary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer’s\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 1\\nObject Detection with Deep Learning: A Review\\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\\nAbstract—Due to object detection’s close relationship with\\nvideo analysis and image understanding, it has attracted much\\nresearch attention in recent years. Traditional object detection\\nmethods are built on handcrafted features and shallow trainable\\narchitectures. Their performance easily stagnates by constructing\\ncomplex ensembles which combine multiple low-level image\\nfeatures with high-level context from object detectors and scene\\nclassiﬁers. With the rapid development in deep learning, more\\npowerful tools, which are able to learn semantic, high-level,\\ndeeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='deeper features, are introduced to address the problems existing\\nin traditional architectures. These models behave differently\\nin network architecture, training strategy and optimization\\nfunction, etc. In this paper, we provide a review on deep\\nlearning based object detection frameworks. Our review begins\\nwith a brief introduction on the history of deep learning and\\nits representative tool, namely Convolutional Neural Network\\n(CNN). Then we focus on typical generic object detection\\narchitectures along with some modiﬁcations and useful tricks\\nto improve detection performance further. As distinct speciﬁc\\ndetection tasks exhibit different characteristics, we also brieﬂy\\nsurvey several speciﬁc tasks, including salient object detection,\\nface detection and pedestrian detection. Experimental analyses\\nare also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='are also provided to compare various methods and draw some\\nmeaningful conclusions. Finally, several promising directions and\\ntasks are provided to serve as guidelines for future work in\\nboth object detection and relevant neural network based learning\\nsystems.\\nIndex Terms—deep learning, object detection, neural network\\nI. I NTRODUCTION\\nT\\nO gain a complete image understanding, we should not\\nonly concentrate on classifying different images, but\\nalso try to precisely estimate the concepts and locations of\\nobjects contained in each image. This task is referred as object\\ndetection [1][S1], which usually consists of different subtasks\\nsuch as face detection [2][S2], pedestrian detection [3][S2]\\nand skeleton detection [4][S3]. As one of the fundamental\\ncomputer vision problems, object detection is able to provide\\nvaluable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiﬁcation [5], [6], human behavior analysis [7][S4],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='valuable information for semantic understanding of images\\nand videos, and is related to many applications, including\\nimage classiﬁcation [5], [6], human behavior analysis [7][S4],\\nface recognition [8][S5] and autonomous driving [9], [10].\\nMeanwhile, Inheriting from neural networks and related learn-\\ning systems, the progress in these ﬁelds will develop neural\\nnetwork algorithms, and will also have great impacts on object\\ndetection techniques which can be considered as learning\\nsystems. [11]–[14][S6]. However, due to large variations in\\nviewpoints, poses, occlusions and lighting conditions, it’s difﬁ-\\ncult to perfectly accomplish object detection with an additional\\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\\nComputer Science and Information Engineering, Hefei University of Technol-\\nogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ogy, China. Xindong Wu is with the School of Computing and Informatics,\\nUniversity of Louisiana at Lafayette, USA.\\nManuscript received August xx, 2017; revised xx xx, 2017.\\nobject localization task. So much attention has been attracted\\nto this ﬁeld in recent years [15]–[18].\\nThe problem deﬁnition of object detection is to determine\\nwhere objects are located in a given image (object localization)\\nand which category each object belongs to (object classiﬁca-\\ntion). So the pipeline of traditional object detection models\\ncan be mainly divided into three stages: informative region\\nselection, feature extraction and classiﬁcation.\\nInformative region selection. As different objects may appear\\nin any positions of the image and have different aspect ratios\\nor sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan ﬁnd out all possible positions of the objects, its short-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='or sizes, it is a natural choice to scan the whole image with a\\nmulti-scale sliding window. Although this exhaustive strategy\\ncan ﬁnd out all possible positions of the objects, its short-\\ncomings are also obvious. Due to a large number of candidate\\nwindows, it is computationally expensive and produces too\\nmany redundant windows. However, if only a ﬁxed number of\\nsliding window templates are applied, unsatisfactory regions\\nmay be produced.\\nFeature extraction. To recognize different objects, we need\\nto extract visual features which can provide a semantic and\\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\\nfeatures are the representative ones. This is due to the fact\\nthat these features can produce representations associated with\\ncomplex cells in human brain [19]. However, due to the diver-\\nsity of appearances, illumination conditions and backgrounds,\\nit’s difﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='sity of appearances, illumination conditions and backgrounds,\\nit’s difﬁcult to manually design a robust feature descriptor to\\nperfectly describe all kinds of objects.\\nClassiﬁcation. Besides, a classiﬁer is needed to distinguish\\na target object from all the other categories and to make the\\nrepresentations more hierarchical, semantic and informative\\nfor visual recognition. Usually, the Supported Vector Machine\\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\\n(DPM) [24] are good choices. Among these classiﬁers, the\\nDPM is a ﬂexible model by combining object parts with\\ndeformation cost to handle severe deformations. In DPM, with\\nthe aid of a graphical model, carefully designed low-level\\nfeatures and kinematically inspired part decompositions are\\ncombined. And discriminative learning of graphical models\\nallows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='allows for building high-precision part-based models for a\\nvariety of object classes.\\nBased on these discriminant local feature descriptors and\\nshallow learnable architectures, state of the art results have\\nbeen obtained on PASCAL VOC object detection competition\\n[25] and real-time embedded systems have been obtained with\\na low burden on hardware. However, small gains are obtained\\nduring 2010-2012 by only building ensemble systems and\\nemploying minor variants of successful methods [15]. This fact\\nis due to the following reasons: 1) The generation of candidate\\nbounding boxes with a sliding window strategy is redundant,\\ninefﬁcient and inaccurate. 2) The semantic gap cannot be\\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 2\\nPedestrian \\ndetection\\nSalient object \\ndetection \\nFace\\ndetection \\nGeneric object \\ndetection\\nObject \\ndetection\\nBounding box \\nregression\\nLocal contrast \\nSegmentation\\nMulti-featureBoosting forest\\nMulti-scale\\nadaption\\nFig. 1. The application domains of object detection.\\nbridged by the combination of manually engineered low-level\\ndescriptors and discriminatively-trained shallow models.\\nThanks to the emergency of Deep Neural Networks (DNNs)\\n[6][S7], a more signiﬁcant gain is obtained with the introduc-\\ntion of Regions with CNN features (R-CNN) [15]. DNNs, or\\nthe most representative CNNs, act in a quite different way from\\ntraditional approaches. They have deeper architectures with the\\ncapacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='capacity to learn more complex features than the shallow ones.\\nAlso the expressivity and robust training algorithms allow to\\nlearn informative object representations without the need to\\ndesign features manually [26].\\nSince the proposal of R-CNN, a great deal of improved\\nmodels have been suggested, including Fast R-CNN which\\njointly optimizes classiﬁcation and bounding box regression\\ntasks [16], Faster R-CNN which takes an additional sub-\\nnetwork to generate region proposals [18] and YOLO which\\naccomplishes object detection via a ﬁxed-grid regression [17].\\nAll of them bring different degrees of detection performance\\nimprovements over the primary R-CNN and make real-time\\nand accurate object detection become more achievable.\\nIn this paper, a systematic review is provided to summarise\\nrepresentative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='representative models and their different characteristics in\\nseveral application domains, including generic object detec-\\ntion [15], [16], [18], salient object detection [27], [28], face\\ndetection [29]–[31] and pedestrian detection [32], [33]. Their\\nrelationships are depicted in Figure 1. Based on basic CNN ar-\\nchitectures, generic object detection is achieved with bounding\\nbox regression, while salient object detection is accomplished\\nwith local contrast enhancement and pixel-level segmentation.\\nFace detection and pedestrian detection are closely related\\nto generic object detection and mainly accomplished with\\nmulti-scale adaption and multi-feature fusion/boosting forest,\\nrespectively. The dotted lines indicate that the corresponding\\ndomains are associated with each other under certain con-\\nditions. It should be noticed that the covered domains are\\ndiversiﬁed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ditions. It should be noticed that the covered domains are\\ndiversiﬁed. Pedestrian and face images have regular structures,\\nwhile general objects and scene images have more complex\\nvariations in geometric structures and layouts. Therefore,\\ndifferent deep models are required by various images.\\nThere has been a relevant pioneer effort [34] which mainly\\nfocuses on relevant software tools to implement deep learning\\ntechniques for image classiﬁcation and object detection, but\\npays little attention on detailing speciﬁc algorithms. Different\\nfrom it, our work not only reviews deep learning based object\\ndetection models and algorithms covering different applica-\\ntion domains in detail, but also provides their corresponding\\nexperimental comparisons and meaningful analyses.\\nThe rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The rest of this paper is organized as follows. In Section\\n2, a brief introduction on the history of deep learning and the\\nbasic architecture of CNN is provided. Generic object detec-\\ntion architectures are presented in Section 3. Then reviews\\nof CNN applied in several speciﬁc tasks, including salient\\nobject detection, face detection and pedestrian detection, are\\nexhibited in Section 4-6, respectively. Several promising future\\ndirections are proposed in Section 7. At last, some concluding\\nremarks are presented in Section 8.\\nII. A B RIEF OVERVIEW OF DEEP LEARNING\\nPrior to overview on deep learning based object detection\\napproaches, we provide a review on the history of deep\\nlearning along with an introduction on the basic architecture\\nand advantages of CNN.\\nA. The History: Birth, Decline and Prosperity\\nDeep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deep models can be referred to as neural networks with\\ndeep structures. The history of neural networks can date back\\nto 1940s [35], and the original intention was to simulate the\\nhuman brain system to solve general learning problems in a\\nprincipled way. It was popular in 1980s and 1990s with the\\nproposal of back-propagation algorithm by Hinton et al. [36].\\nHowever, due to the overﬁtting of training, lack of large scale\\ntraining data, limited computation power and insigniﬁcance\\nin performance compared with other machine learning tools,\\nneural networks fell out of fashion in early 2000s.\\nDeep learning has become popular since 2006 [37][S7] with\\na break through in speech recognition [38]. The recovery of\\ndeep learning can be attributed to the following factors.\\n•The emergence of large scale annotated training data, such\\nas ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n•Fast development of high performance parallel computing\\nsystems, such as GPU clusters;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='as ImageNet [39], to fully exhibit its very large learning\\ncapacity;\\n•Fast development of high performance parallel computing\\nsystems, such as GPU clusters;\\n•Signiﬁcant advances in the design of network structures\\nand training strategies. With unsupervised and layerwise\\npre-training guided by Auto-Encoder (AE) [40] or Re-\\nstricted Boltzmann Machine (RBM) [41], a good initializa-\\ntion is provided. With dropout and data augmentation, the\\noverﬁtting problem in training has been relieved [6], [42].\\nWith batch normalization (BN), the training of very deep\\nneural networks becomes quite efﬁcient [43]. Meanwhile,\\nvarious network structures, such as AlexNet [6], Overfeat\\n[44], GoogLeNet [45], VGG [46] and ResNet [47], have\\nbeen extensively studied to improve the performance.\\nWhat prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton’s group, whose continuous efforts have demonstrated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='What prompts deep learning to have a huge impact on the\\nentire academic community? It may owe to the contribution of\\nHinton’s group, whose continuous efforts have demonstrated\\nthat deep learning would bring a revolutionary breakthrough\\non grand challenges rather than just obvious improvements on\\nsmall datasets. Their success results from training a large CNN\\non 1.2 million labeled images together with a few techniques\\n[6] (e.g., ReLU operation [48] and ‘dropout’ regularization).\\nB. Architecture and Advantages of CNN\\nCNN is the most representative model of deep learning [26].\\nA typical CNN architecture, which is referred to as VGG16,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 3\\ncan be found in Fig. S1. Each layer of CNN is known as a\\nfeature map. The feature map of the input layer is a 3D matrix\\nof pixel intensities for different color channels (e.g. RGB). The\\nfeature map of any internal layer is an induced multi-channel\\nimage, whose ‘pixel’ can be viewed as a speciﬁc feature. Every\\nneuron is connected with a small portion of adjacent neurons\\nfrom the previous layer (receptive ﬁeld). Different types of\\ntransformations [6], [49], [50] can be conducted on feature\\nmaps, such as ﬁltering and pooling. Filtering (convolution)\\noperation convolutes a ﬁlter matrix (learned weights) with\\nthe values of a receptive ﬁeld of neurons and takes a non-\\nlinear function (such as sigmoid [51], ReLU) to obtain ﬁnal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='linear function (such as sigmoid [51], ReLU) to obtain ﬁnal\\nresponses. Pooling operation, such as max pooling, average\\npooling, L2-pooling and local contrast normalization [52],\\nsummaries the responses of a receptive ﬁeld into one value\\nto produce more robust feature descriptions.\\nWith an interleave between convolution and pooling, an\\ninitial feature hierarchy is constructed, which can be ﬁne-tuned\\nin a supervised manner by adding several fully connected (FC)\\nlayers to adapt to different visual tasks. According to the tasks\\ninvolved, the ﬁnal layer with different activation functions [6]\\nis added to get a speciﬁc conditional probability for each\\noutput neuron. And the whole network can be optimized on\\nan objective function (e.g. mean squared error or cross-entropy\\nloss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='loss) via the stochastic gradient descent (SGD) method. The\\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\\nfully connected layers, 3 max-pooling layers and a softmax\\nclassiﬁcation layer. The conv feature maps are produced by\\nconvoluting 3*3 ﬁlter windows, and feature map resolutions\\nare reduced with 2 stride max-pooling layers. An arbitrary test\\nimage of the same size as training samples can be processed\\nwith the trained network. Re-scaling or cropping operations\\nmay be needed if different sizes are provided [6].\\nThe advantages of CNN against traditional methods can be\\nsummarised as follows.\\n•Hierarchical feature representation, which is the multi-\\nlevel representations from pixel to high-level semantic fea-\\ntures learned by a hierarchical multi-stage structure [15],\\n[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n• Compared with traditional shallow models, a deeper'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[53], can be learned from data automatically and hidden\\nfactors of input data can be disentangled through multi-level\\nnonlinear mappings.\\n• Compared with traditional shallow models, a deeper\\narchitecture provides an exponentially increased expressive\\ncapability.\\n• The architecture of CNN provides an opportunity to\\njointly optimize several related tasks together (e.g. Fast R-\\nCNN combines classiﬁcation and bounding box regression\\ninto a multi-task leaning manner).\\n• Beneﬁtting from the large learning capacity of deep\\nCNNs, some classical computer vision challenges can be\\nrecast as high-dimensional data transform problems and\\nsolved from a different viewpoint.\\nDue to these advantages, CNN has been widely applied\\ninto many research ﬁelds, such as image super-resolution\\nreconstruction [54], [55], image classiﬁcation [5], [56], im-\\nage retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]–[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='age retrieval [57], [58], face recognition [8][S5], pedestrian\\ndetection [59]–[61] and video analysis [62], [63].\\nIII. G ENERIC OBJECT DETECTION\\nGeneric object detection aims at locating and classifying\\nexisting objects in any one image, and labeling them with\\nrectangular bounding boxes to show the conﬁdences of exis-\\ntence. The frameworks of generic object detection methods\\ncan mainly be categorized into two types (see Figure 2).\\nOne follows traditional object detection pipeline, generating\\nregion proposals at ﬁrst and then classifying each proposal into\\ndifferent object categories. The other regards object detection\\nas a regression or classiﬁcation problem, adopting a uniﬁed\\nframework to achieve ﬁnal results (categories and locations)\\ndirectly. The region proposal based methods mainly include\\nR-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiﬁes R-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\\nwhich are correlated with each other (e.g. SPP-net modiﬁes R-\\nCNN with a SPP layer). The regression /classiﬁcation based\\nmethods mainly includes MultiBox [68], AttentionNet [69],\\nG-CNN [70], YOLO [17], SSD [71], YOLOv2 [72], DSSD\\n[73] and DSOD [74]. The correlations between these two\\npipelines are bridged by the anchors introduced in Faster R-\\nCNN. Details of these methods are as follows.\\nA. Region Proposal Based Framework\\nThe region proposal based framework, a two-step process,\\nmatches the attentional mechanism of human brain to some\\nextent, which gives a coarse scan of the whole scenario ﬁrstly\\nand then focuses on regions of interest. Among the pre-related\\nworks [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='works [44], [75], [76], the most representative one is Overfeat\\n[44]. This model inserts CNN into sliding window method,\\nwhich predicts bounding boxes directly from locations of\\nthe topmost feature map after obtaining the conﬁdences of\\nunderlying object categories.\\n1) R-CNN: It is of signiﬁcance to improve the quality of\\ncandidate bounding boxes and to take a deep architecture to\\nextract high-level features. To solve these problems, R-CNN\\n[15] was proposed by Ross Girshick in 2014 and obtained a\\nmean average precision (mAP) of 53.3% with more than 30%\\nimprovement over the previous best result (DPM HSC [77]) on\\nPASCAL VOC 2012. Figure 3 shows the ﬂowchart of R-CNN,\\nwhich can be divided into three stages as follows.\\nRegion proposal generation. The R-CNN adopts selective\\nsearch [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='search [78] to generate about 2k region proposals for each\\nimage. The selective search method relies on simple bottom-up\\ngrouping and saliency cues to provide more accurate candidate\\nboxes of arbitrary sizes quickly and to reduce the searching\\nspace in object detection [24], [39].\\nCNN based deep feature extraction. In this stage, each\\nregion proposal is warped or cropped into a ﬁxed resolution\\nand the CNN module in [6] is utilized to extract a 4096-\\ndimensional feature as the ﬁnal representation. Due to large\\nlearning capacity, dominant expressive power and hierarchical\\nstructure of CNNs, a high-level, semantic and robust feature\\nrepresentation for each region proposal can be obtained.\\nClassiﬁcation and localization. With pre-trained category-\\nspeciﬁc linear SVMs for multiple classes, different region pro-\\nposals are scored on a set of positive regions and background\\n(negative) regions. The scored regions are then adjusted with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 4\\nGeneric object \\ndetection\\nRegion proposal \\nbased\\nRegression/\\nClassification \\nbased \\nR-CNN\\n(2014)\\nSPP-net\\n(2015)\\nFRCN\\n(2015)\\nFaster \\nR-CNN\\n(2015)\\nR-FCN\\n(2016)\\nFPN\\n(2017)\\nMask R-CNN\\n(2017)\\nMultiBox\\n(2014)\\nAttentionNet\\n(2015)\\nG-CNN\\n(2016)\\nYOLO\\n(2016)\\nSSD\\n(2016)\\nYOLOv2\\n(2017)\\nSPP \\nlayer\\nMulti-\\ntask\\nRPN\\nFCN\\nFeature\\npyramid\\nInstance\\nSegmentation\\nRegion\\nproposal\\nUnified\\nloss\\nDirection\\niteration\\nJoint Grid\\nregression\\nRPN BN\\nMulti-scale\\nGridregression\\nDSSD\\n(2017)\\nDSOD\\n(2017)\\nStem block\\nDense block\\nResNet101 \\nDeconv layers\\nFig. 2. Two types of frameworks: region proposal based and regression /classiﬁcation based. SPP: Spatial Pyramid Pooling [64], FRCN: Faster R-CNN [16],\\nRPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54].\\nRich feature hierarchies for accurate object detection and semantic segmentation\\nRoss Girshick1 Jeff Donahue1,2 Trevor Darrell1,2 Jitendra Malik1\\n1UC Berkeley and 2ICSI\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012—achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speciﬁc ﬁne-tuning, yields a signiﬁ-\\ncant performance boost. Since we combine region propos-\\nals with CNNs, we call our method R-CNN: Regions with\\nCNN features. We also present experiments that provide\\ninsight into what the network learns, revealing a rich hier-\\narchy of image features. Source code for the complete sys-\\ntem is available at http://www.cs.berkeley.edu/\\n˜rbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='use of SIFT [26] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [12], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the ﬁrst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima’s “neocognitron” [16], a biologically-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiﬁes each\\nregion using class-speciﬁc linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [32] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='recognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training al-\\ngorithm. LeCun et al. [23] provided the missing algorithm\\nby showing that stochastic gradient descent, via backprop-\\nagation, can train convolutional neural networks (CNNs), a\\nclass of models that extend the neocognitron.\\nCNNs saw heavy use in the 1990s ( e.g., [24]), but then\\nfell out of fashion, particularly in computer vision, with the\\nrise of support vector machines. In 2012, Krizhevsky et al.\\n[22] rekindled interest in CNNs by showing substantially\\nhigher image classiﬁcation accuracy on the ImageNet Large\\nScale Visual Recognition Challenge (ILSVRC) [9, 10].\\nTheir success resulted from training a large CNN on 1.2\\nmillion labeled images, together with a few twists on Le-\\nCun’s CNN (e.g., max(x, 0) rectifying non-linearities and\\n“dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Cun’s CNN (e.g., max(x, 0) rectifying non-linearities and\\n“dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\ndebated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classiﬁcation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question decisively by bridging the\\nchasm between image classiﬁcation and object detection.\\nThis paper is the ﬁrst to show that a CNN can lead to dra-\\n1\\nFig. 3. The ﬂowchart of R-CNN [15], which consists of 3 stages: (1) extracts\\nbottom-up region proposals, (2) computes features for each proposal using a\\nCNN, and then (3) classiﬁes each region with class-speciﬁc linear SVMs.\\nbounding box regression and ﬁltered with a greedy non-\\nmaximum suppression (NMS) to produce ﬁnal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufﬁcient labeled data, pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='bounding box regression and ﬁltered with a greedy non-\\nmaximum suppression (NMS) to produce ﬁnal bounding boxes\\nfor preserved object locations.\\nWhen there are scarce or insufﬁcient labeled data, pre-\\ntraining is usually conducted. Instead of unsupervised pre-\\ntraining [79], R-CNN ﬁrstly conducts supervised pre-training\\non ILSVRC, a very large auxiliary dataset, and then takes a\\ndomain-speciﬁc ﬁne-tuning. This scheme has been adopted by\\nmost of subsequent approaches [16], [18].\\nIn spite of its improvements over traditional methods and\\nsigniﬁcance in bringing CNN into practical object detection,\\nthere are still some disadvantages.\\n•Due to the existence of FC layers, the CNN requires a\\nﬁxed-size (e.g., 227×227) input image, which directly leads\\nto the re-computation of the whole CNN for each evaluated\\nregion, taking a great deal of time in the testing period.\\n•Training of R-CNN is a multi-stage pipeline. At ﬁrst,\\na convolutional network (ConvNet) on object proposals is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='region, taking a great deal of time in the testing period.\\n•Training of R-CNN is a multi-stage pipeline. At ﬁrst,\\na convolutional network (ConvNet) on object proposals is\\nﬁne-tuned. Then the softmax classiﬁer learned by ﬁne-\\ntuning is replaced by SVMs to ﬁt in with ConvNet features.\\nFinally, bounding-box regressors are trained.\\n• Training is expensive in space and time. Features are\\nextracted from different region proposals and stored on the\\ndisk. It will take a long time to process a relatively small\\ntraining set with very deep networks, such as VGG16. At the\\nsame time, the storage memory required by these features\\nshould also be a matter of concern.\\n•Although selective search can generate region proposals\\nwith relatively high recalls, the obtained region proposals\\nare still redundant and this procedure is time-consuming\\n(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='(around 2 seconds to extract 2k region proposals).\\nTo solve these problems, many methods have been pro-\\nposed. GOP [80] takes a much faster geodesic based segmen-\\ntation to replace traditional graph cuts. MCG [81] searches\\ndifferent scales of the image for multiple hierarchical segmen-\\ntations and combinatorially groups different regions to produce\\nproposals. Instead of extracting visually distinct segments,\\nthe edge boxes method [82] adopts the idea that objects are\\nmore likely to exist in bounding boxes with fewer contours\\nstraggling their boundaries. Also some researches tried to\\nre-rank or reﬁne pre-extracted region proposals to remove\\nunnecessary ones and obtained a limited number of valuable\\nones, such as DeepBox [83] and SharpMask [84].\\nIn addition, there are some improvements to solve the\\nproblem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='problem of inaccurate localization. Zhang et al. [85] utilized\\na bayesian optimization based search algorithm to guide\\nthe regressions of different bounding boxes sequentially, and\\ntrained class-speciﬁc CNN classiﬁers with a structured loss\\nto penalize the localization inaccuracy explicitly. Saurabh\\nGupta et al. improved object detection for RGB-D images\\nwith semantically rich image and depth features [86], and\\nlearned a new geocentric embedding for depth images to\\nencode each pixel. The combination of object detectors and\\nsuperpixel classiﬁcation framework gains a promising result\\non semantic scene segmentation task. Ouyang et al. proposed\\na deformable deep CNN (DeepID-Net) [87] which introduces\\na novel deformation constrained pooling (def-pooling) layer\\nto impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='to impose geometric penalty on the deformation of various\\nobject parts and makes an ensemble of models with different\\nsettings. Lenc et al. [88] provided an analysis on the role\\nof proposal generation in CNN-based detectors and tried to\\nreplace this stage with a constant and trivial region generation\\nscheme. The goal is achieved by biasing sampling to match\\nthe statistics of the ground truth bounding boxes with K-means\\nclustering. However, more candidate boxes are required to\\nachieve comparable results to those of R-CNN.\\n2) SPP-net: FC layers must take a ﬁxed-size input. That’s\\nwhy R-CNN chooses to warp or crop each region proposal\\ninto the same size. However, the object may exist partly in\\nthe cropped region and unwanted geometric distortion may be\\nproduced due to the warping operation. These content losses or\\ndistortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='distortions will reduce recognition accuracy, especially when\\nthe scales of objects vary.\\nTo solve this problem, He et al. took the theory of spatial\\npyramid matching (SPM) [89], [90] into consideration and\\nproposed a novel CNN architecture named SPP-net [64]. SPM\\ntakes several ﬁner to coarser scales to partition the image into\\na number of divisions and aggregates quantized local features\\ninto mid-level representations.\\nThe architecture of SPP-net for object detection can be\\nfound in Figure 4. Different from R-CNN, SPP-net reuses\\nfeature maps of the 5-th conv layer (conv5) to project region'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 5\\n9\\nmethod VOC 2007 Caltech101\\nVQ [15]† 56.07 74.41 ±1.0\\nLLC [18]† 57.66 76.95 ±0.4\\nFK [19]† 61.69 77.78 ±0.6\\nDeCAF [13] - 86.91 ±0.7\\nZeiler & Fergus [4] 75.90‡ 86.5±0.5\\nOquab et al. [34] 77.7 -\\nChatﬁeld et al. [6] 82.42 88.54±0.3\\nours 82.44 93.42 ±0.5\\nTable 8: Classiﬁcation results for Pascal VOC 2007\\n(mAP) and Caltech101 (accuracy). †numbers reported\\nby [27]. ‡our implementation as in Table 6 (a).\\nTable 8 summarizes our results compared with the\\nstate-of-the-art methods on Caltech101. Our result\\n(93.42%) exceeds the previous record (88.54%) by a\\nsubstantial margin (4.88%).\\n4 SPP- NET FOR OBJECT DETECTION\\nDeep networks have been used for object detection.\\nWe brieﬂy review the recent state-of-the-art R-CNN\\nmethod [7]. R-CNN ﬁrst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='method [7]. R-CNN ﬁrst extracts about 2,000 candi-\\ndate windows from each image via selective search\\n[20]. Then the image region in each window is warped\\nto a ﬁxed size (227 ×227). A pre-trained deep network\\nis used to extract the feature of each window. A\\nbinary SVM classiﬁer is then trained on these features\\nfor detection. R-CNN generates results of compelling\\nquality and substantially outperforms previous meth-\\nods. However, because R-CNN repeatedly applies the\\ndeep convolutional network to about 2,000 windows\\nper image, it is time-consuming. Feature extraction is\\nthe major timing bottleneck in testing.\\nOur SPP-net can also be used for object detection.\\nWe extract the feature maps from the entire image\\nonly once (possibly at multiple scales). Then we ap-\\nply the spatial pyramid pooling on each candidate\\nwindow of the feature maps to pool a ﬁxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='window of the feature maps to pool a ﬁxed-length\\nrepresentation of this window (see Figure 5). Because\\nthe time-consuming convolutions are only applied\\nonce, our method can run orders of magnitude faster.\\nOur method extracts window-wise features from\\nregions of the feature maps, while R-CNN extracts\\ndirectly from image regions. In previous works, the\\nDeformable Part Model (DPM) [23] extracts features\\nfrom windows in HOG [24] feature maps, and the\\nSelective Search (SS) method [20] extracts from win-\\ndows in encoded SIFT feature maps. The Overfeat\\ndetection method [5] also extracts from windows of\\ndeep convolutional feature maps, but needs to pre-\\ndeﬁne the window size. On the contrary, our method\\nenables feature extraction in arbitrary windows from\\nthe deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n…...\\nfully-connected layers (fc6, fc7)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the deep convolutional feature maps.\\nspatial pyramid \\npooling layer\\nfeature maps of conv5\\nconvolutional layers\\nfixed-length representation\\ninput image\\nwindow\\n…...\\nfully-connected layers (fc6, fc7)\\nFigure 5: Pooling features from arbitrary windows\\non feature maps. The feature maps are computed\\nfrom the entire image. The pooling is performed in\\ncandidate windows.\\n4.1 Detection Algorithm\\nWe use the “fast” mode of selective search [20] to\\ngenerate about 2,000 candidate windows per image.\\nThen we resize the image such that min(w,h) = s,\\nand extract the feature maps from the entire image.\\nWe use the SPP-net model of ZF-5 (single-size trained)\\nfor the time being. In each candidate window, we use\\na 4-level spatial pyramid (1 ×1, 2×2, 3×3, 6×6, totally\\n50 bins) to pool the features. This generates a 12,800-\\nd (256 ×50) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='d (256 ×50) representation for each window. These\\nrepresentations are provided to the fully-connected\\nlayers of the network. Then we train a binary linear\\nSVM classiﬁer for each category on these features.\\nOur implementation of the SVM training follows\\n[20], [7]. We use the ground-truth windows to gen-\\nerate the positive samples. The negative samples are\\nthose overlapping a positive window by at most 30%\\n(measured by the intersection-over-union (IoU) ratio).\\nAny negative sample is removed if it overlaps another\\nnegative sample by more than 70%. We apply the stan-\\ndard hard negative mining [23] to train the SVM. This\\nstep is iterated once. It takes less than 1 hour to train\\nSVMs for all 20 categories. In testing, the classiﬁer\\nis used to score the candidate windows. Then we use\\nnon-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) ='),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='non-maximum suppression [23] (threshold of 30%) on\\nthe scored windows.\\nOur method can be improved by multi-scale feature\\nextraction. We resize the image such that min(w,h) =\\ns ∈ S = {480,576,688,864,1200}, and compute the\\nfeature maps of conv 5 for each scale. One strategy of\\ncombining the features from these scales is to pool\\nthem channel-by-channel. But we empirically ﬁnd\\nthat another strategy provides better results. For each\\ncandidate window, we choose a single scale s ∈ S\\nsuch that the scaled candidate window has a number\\nof pixels closest to 224 ×224. Then we only use the\\nfeature maps extracted from this scale to compute\\nFig. 4. The architecture of SPP-net for object detection [64].\\nSPPnet also has notable drawbacks. Like R-CNN, train-\\ning is a multi-stage pipeline that involves extracting fea-\\ntures, ﬁne-tuning a network with log loss, training SVMs,\\nand ﬁnally ﬁtting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the ﬁne-tuning al-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tures, ﬁne-tuning a network with log loss, training SVMs,\\nand ﬁnally ﬁtting bounding-box regressors. Features are\\nalso written to disk. But unlike R-CNN, the ﬁne-tuning al-\\ngorithm proposed in [\\n11] cannot update the convolutional\\nlayers that precede the spatial pyramid pooling. Unsurpris-\\ningly, this limitation (ﬁxed convolutional layers) limits the\\naccuracy of very deep networks.\\n1.2. Contributions\\nWe propose a new training algorithm that ﬁxes the disad-\\nvantages of R-CNN and SPPnet, while improving on their\\nspeed and accuracy. We call this method Fast R-CNN be-\\ncause it’s comparatively fast to train and test. The Fast R-\\nCNN method has several advantages:\\n1. Higher detection quality (mAP) than R-CNN, SPPnet\\n2. Training is single-stage, using a multi-task loss\\n3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='3. Training can update all network layers\\n4. No disk storage is required for feature caching\\nFast R-CNN is written in Python and C++ (Caffe\\n[\\n13]) and is available under the open-source MIT Li-\\ncense at https://github.com/rbgirshick/\\nfast-rcnn.\\n2. Fast R-CNN architecture and training\\nFig. 1 illustrates the Fast R-CNN architecture. A Fast\\nR-CNN network takes as input an entire image and a set\\nof object proposals. The network ﬁrst processes the whole\\nimage with several convolutional ( conv) and max pooling\\nlayers to produce a conv feature map. Then, for each ob-\\nject proposal a region of interest ( RoI) pooling layer ex-\\ntracts a ﬁxed-length feature vector from the feature map.\\nEach feature vector is fed into a sequence of fully connected\\n(fc) layers that ﬁnally branch into two sibling output lay-\\ners: one that produces softmax probability estimates over\\nK object classes plus a catch-all “background” class and\\nanother layer that outputs four real-valued numbers for each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ers: one that produces softmax probability estimates over\\nK object classes plus a catch-all “background” class and\\nanother layer that outputs four real-valued numbers for each\\nof the K object classes. Each set of 4 values encodes reﬁned\\nbounding-box positions for one of the K classes.\\n2.1. The RoI pooling layer\\nThe RoI pooling layer uses max pooling to convert the\\nfeatures inside any valid region of interest into a small fea-\\nture map with a ﬁxed spatial extent of H × W (e.g., 7 × 7),\\nwhere H and W are layer hyper-parameters that are inde-\\npendent of any particular RoI. In this paper, an RoI is a\\nrectangular window into a conv feature map. Each RoI is\\ndeﬁned by a four-tuple (r, c, h, w ) that speciﬁes its top-left\\ncorner (r, c) and its height and width (h, w ).\\nDeep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deep\\nConvNet\\nConv\\nfeature map\\nRoI\\nprojection\\nRoI\\npooling\\nlayer\\nFCs\\nRoI feature\\nvector\\nsoftmax\\nbbox\\nregressor\\nOutputs:\\nFC FC\\nFor each RoI\\nFigure 1. Fast R-CNN architecture. An input image and multi-\\nple regions of interest (RoIs) are input into a fully convolutional\\nnetwork. Each RoI is pooled into a ﬁxed-size feature map and\\nthen mapped to a feature vector by fully connected layers (FCs).\\nThe network has two output vectors per RoI: softmax probabilities\\nand per-class bounding-box regression offsets. The architecture is\\ntrained end-to-end with a multi-task loss.\\nRoI max pooling works by dividing the h × w RoI win-\\ndow into an H × W grid of sub-windows of approximate\\nsize h/H × w/W and then max-pooling the values in each\\nsub-window into the corresponding output grid cell. Pool-\\ning is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets ['),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ing is applied independently to each feature map channel,\\nas in standard max pooling. The RoI layer is simply the\\nspecial-case of the spatial pyramid pooling layer used in\\nSPPnets [\\n11] in which there is only one pyramid level. We\\nuse the pooling sub-window calculation given in [ 11].\\n2.2. Initializing from pre-trained networks\\nWe experiment with three pre-trained ImageNet [ 4] net-\\nworks, each with ﬁve max pooling layers and between ﬁve\\nand thirteen conv layers (see Section\\n4.1 for network de-\\ntails). When a pre-trained network initializes a Fast R-CNN\\nnetwork, it undergoes three transformations.\\nFirst, the last max pooling layer is replaced by a RoI\\npooling layer that is conﬁgured by setting H and W to be\\ncompatible with the net’s ﬁrst fully connected layer ( e.g.,\\nH = W = 7 for VGG16).\\nSecond, the network’s last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classiﬁ-\\ncation) are replaced with the two sibling layers described'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='H = W = 7 for VGG16).\\nSecond, the network’s last fully connected layer and soft-\\nmax (which were trained for 1000-way ImageNet classiﬁ-\\ncation) are replaced with the two sibling layers described\\nearlier (a fully connected layer and softmax over K + 1 cat-\\negories and category-speciﬁc bounding-box regressors).\\nThird, the network is modiﬁed to take two data inputs: a\\nlist of images and a list of RoIs in those images.\\n2.3. Fine-tuning for detection\\nTraining all network weights with back-propagation is an\\nimportant capability of Fast R-CNN. First, let’s elucidate\\nwhy SPPnet is unable to update weights below the spatial\\npyramid pooling layer.\\nThe root cause is that back-propagation through the SPP\\nlayer is highly inefﬁcient when each training sample ( i.e.\\nRoI) comes from a different image, which is exactly how\\nR-CNN and SPPnet networks are trained. The inefﬁciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to ﬁxed-length feature vectors. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN and SPPnet networks are trained. The inefﬁciency\\n1441\\nFig. 5. The architecture of Fast R-CNN [16].\\nproposals of arbitrary sizes to ﬁxed-length feature vectors. The\\nfeasibility of the reusability of these feature maps is due to\\nthe fact that the feature maps not only involve the strength of\\nlocal responses, but also have relationships with their spatial\\npositions [64]. The layer after the ﬁnal conv layer is referred\\nto as spatial pyramid pooling layer (SPP layer). If the number\\nof feature maps in conv5 is 256, taking a 3-level pyramid,\\nthe ﬁnal feature vector for each region proposal obtained after\\nSPP layer has a dimension of 256 ×(12 + 22 + 42) = 5376.\\nSPP-net not only gains better results with correct estimation\\nof different region proposals in their corresponding scales, but\\nalso improves detection efﬁciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='also improves detection efﬁciency in testing period with the\\nsharing of computation cost before SPP layer among different\\nproposals.\\n3) Fast R-CNN: Although SPP-net has achieved impressive\\nimprovements in both accuracy and efﬁciency over R-CNN,\\nit still has some notable drawbacks. SPP-net takes almost\\nthe same multi-stage pipeline as R-CNN, including feature\\nextraction, network ﬁne-tuning, SVM training and bounding-\\nbox regressor ﬁtting. So an additional expense on storage space\\nis still required. Additionally, the conv layers preceding the\\nSPP layer cannot be updated with the ﬁne-tuning algorithm\\nintroduced in [64]. As a result, an accuracy drop of very deep\\nnetworks is unsurprising. To this end, Girshick [16] introduced\\na multi-task loss on classiﬁcation and bounding box regression\\nand proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and proposed a novel CNN architecture named Fast R-CNN.\\nThe architecture of Fast R-CNN is exhibited in Figure 5.\\nSimilar to SPP-net, the whole image is processed with conv\\nlayers to produce feature maps. Then, a ﬁxed-length feature\\nvector is extracted from each region proposal with a region of\\ninterest (RoI) pooling layer. The RoI pooling layer is a special\\ncase of the SPP layer, which has only one pyramid level. Each\\nfeature vector is then fed into a sequence of FC layers before\\nﬁnally branching into two sibling output layers. One output\\nlayer is responsible for producing softmax probabilities for\\nall C+ 1categories (C object classes plus one ‘background’\\nclass) and the other output layer encodes reﬁned bounding-\\nbox positions with four real-valued numbers. All parameters\\nin these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deﬁned as below to jointly train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in these procedures (except the generation of region proposals)\\nare optimized via a multi-task loss in an end-to-end way.\\nThe multi-tasks loss L is deﬁned as below to jointly train\\nclassiﬁcation and bounding-box regression,\\nL(p,u,t u,v) =Lcls(p,u) +λ[u≥1]Lloc(tu,v) (1)\\nwhere Lcls(p,u) =−log pu calculates the log loss for ground\\ntruth class u and pu is driven from the discrete probability\\ndistribution p= (p0,··· ,pC) over the C+1 outputs from the\\nlast FC layer. Lloc(tu,v) is deﬁned over the predicted offsets\\ntu = (tu\\nx,tu\\ny,tu\\nw,tu\\nh) and ground-truth bounding-box regression\\ntargets v = (vx,vy,vw,vh), where x,y,w,h denote the two\\ncoordinates of the box center, width, and height, respectively.\\nEach tu adopts the parameter settings in [15] to specify an\\nobject proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u≥1] is employed to omit all background RoIs. To provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object proposal with a log-space height/width shift and scale-\\ninvariant translation. The Iverson bracket indicator function\\n[u≥1] is employed to omit all background RoIs. To provide\\nmore robustness against outliers and eliminate the sensitivity\\nin exploding gradients, a smooth L1 loss is adopted to ﬁt\\nbounding-box regressors as below\\nLloc(tu,v) =\\n∑\\ni∈x,y,w,h\\nsmoothL1 (tu\\ni −vi) (2)\\nwhere\\nsmoothL1 (x) =\\n{\\n0.5x2 if|x|<1\\n|x|−0.5 otherwise (3)\\nTo accelerate the pipeline of Fast R-CNN, another two tricks\\nare of necessity. On one hand, if training samples (i.e. RoIs)\\ncome from different images, back-propagation through the\\nSPP layer becomes highly inefﬁcient. Fast R-CNN samples\\nmini-batches hierarchically, namely N images sampled ran-\\ndomly at ﬁrst and then R/N RoIs sampled in each image,\\nwhere R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='where R represents the number of RoIs. Critically, computa-\\ntion and memory are shared by RoIs from the same image in\\nthe forward and backward pass. On the other hand, much time\\nis spent in computing the FC layers during the forward pass\\n[16]. The truncated Singular Value Decomposition (SVD) [91]\\ncan be utilized to compress large FC layers and to accelerate\\nthe testing procedure.\\nIn the Fast R-CNN, regardless of region proposal genera-\\ntion, the training of all network layers can be processed in\\na single-stage with a multi-task loss. It saves the additional\\nexpense on storage space, and improves both accuracy and\\nefﬁciency with more reasonable training schemes.\\n4) Faster R-CNN: Despite the attempt to generate candi-\\ndate boxes with biased sampling [88], state-of-the-art object\\ndetection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection networks mainly rely on additional methods, such as\\nselective search and Edgebox, to generate a candidate pool of\\nisolated region proposals. Region proposal computation is also\\na bottleneck in improving efﬁciency. To solve this problem,\\nRen et al. introduced an additional Region Proposal Network\\n(RPN) [18], [92], which acts in a nearly cost-free way by\\nsharing full-image conv features with detection network.\\nRPN is achieved with a fully-convolutional network, which\\nhas the ability to predict object bounds and scores at each\\nposition simultaneously. Similar to [78], RPN takes an image\\nof arbitrary size to generate a set of rectangular object propos-\\nals. RPN operates on a speciﬁc conv layer with the preceding\\nlayers shared with object detection network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 6\\ncar : 1.000\\ndog : 0.997\\nperson : 0.992\\nperson : 0.979\\nhorse : 0.993\\nconv feature map\\nintermediate layer\\n256-d\\n2k scores 4k coordinates\\nsliding window\\nreg layercls layer\\nk anchor boxes\\nbus : 0.996\\nperson : 0.736\\nboat : 0.970\\nperson : 0.989\\nperson : 0.983person : 0.983\\nperson : 0.925\\ncat : 0.982\\ndog : 0.994\\nFigure 1: Left: Region Proposal Network (RPN). Right: Example detections using RPN proposals\\non PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios.\\nfeature map. Each sliding window is mapped to a lower-dimensional vector (256-d for ZF and 512-d\\nfor VGG). This vector is fed into two sibling fully-connected layers—a box-regression layer ( reg)\\nand a box-classiﬁcation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\nﬁeld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and a box-classiﬁcation layer ( cls). We use n = 3in this paper, noting that the effective receptive\\nﬁeld on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-\\nnetwork is illustrated at a single position in Fig. 1 (left). Note that because the mini-network operates\\nin a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This\\narchitecture is naturally implemented with an n×nconv layer followed by two sibling 1 ×1 conv\\nlayers (for reg and cls, respectively). ReLUs [15] are applied to the output of the n×nconv layer.\\nTranslation-Invariant Anchors\\nAt each sliding-window location, we simultaneously predict k region proposals, so the reg layer\\nhas 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='has 4k outputs encoding the coordinates of k boxes. The cls layer outputs 2k scores that estimate\\nprobability of object / not-object for each proposal. 2 The kproposals are parameterized relative to\\nkreference boxes, called anchors. Each anchor is centered at the sliding window in question, and is\\nassociated with a scale and aspect ratio. We use 3 scales and 3 aspect ratios, yieldingk= 9anchors\\nat each sliding position. For a conv feature map of a sizeW×H(typically ∼2,400), there are WHk\\nanchors in total. An important property of our approach is that it is translation invariant, both in\\nterms of the anchors and the functions that compute proposals relative to the anchors.\\nAs a comparison, the MultiBox method [20] uses k-means to generate 800 anchors, which are not\\ntranslation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='translation invariant. If one translates an object in an image, the proposal should translate and the\\nsame function should be able to predict the proposal in either location. Moreover, because the\\nMultiBox anchors are not translation invariant, it requires a (4+1) ×800-dimensional output layer,\\nwhereas our method requires a (4+2)×9-dimensional output layer. Our proposal layers have an order\\nof magnitude fewer parameters (27 million for MultiBox using GoogLeNet [20] vs. 2.4 million for\\nRPN using VGG-16), and thus have less risk of overﬁtting on small datasets, like PASCAL VOC.\\nA Loss Function for Learning Region Proposals\\nFor training RPNs, we assign a binary class label (of being an object or not) to each anchor. We\\nassign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-\\nover-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='over-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher\\nthan 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels\\nto multiple anchors. We assign a negative label to a non-positive anchor if its IoU ratio is lower than\\n0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the\\ntraining objective.\\nWith these deﬁnitions, we minimize an objective function following the multi-task loss in Fast R-\\nCNN [5]. Our loss function for an image is deﬁned as:\\nL({pi},{ti}) = 1\\nNcls\\n∑\\ni\\nLcls (pi,p∗\\ni ) +λ 1\\nNreg\\n∑\\ni\\np∗\\ni Lreg(ti,t∗\\ni ). (1)\\n2For simplicity we implement the cls layer as a two-class softmax layer. Alternatively, one may use logistic\\nregression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K predeﬁned anchor boxes are\\nconvoluted with each sliding window to produce ﬁxed-length vectors which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='regression to produce kscores.\\n3\\nFig. 6. The RPN in Faster R-CNN [18]. K predeﬁned anchor boxes are\\nconvoluted with each sliding window to produce ﬁxed-length vectors which\\nare taken by cls and reg layer to obtain corresponding outputs.\\nThe architecture of RPN is shown in Figure 6. The network\\nslides over the conv feature map and fully connects to an\\nn×n spatial window. A low dimensional vector (512-d for\\nVGG16) is obtained in each sliding window and fed into two\\nsibling FC layers, namely box-classiﬁcation layer (cls) and\\nbox-regression layer (reg). This architecture is implemented\\nwith an n×n conv layer followed by two sibling 1 ×1 conv\\nlayers. To increase non-linearity, ReLU is applied to the output\\nof the n×n conv layer.\\nThe regressions towards true bounding boxes are achieved\\nby comparing proposals relative to reference boxes (anchors).\\nIn the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n∑\\ni\\nLcls(pi,p∗'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\\nare adopted. The loss function is similar to (1).\\nL(pi,ti) = 1\\nNcls\\n∑\\ni\\nLcls(pi,p∗\\ni) +λ 1\\nNreg\\n∑\\ni\\np∗\\niLreg(ti,t∗\\ni)\\n(4)\\nwhere pi shows the predicted probability of the i-th anchor\\nbeing an object. The ground truth label p∗\\ni is 1 if the anchor is\\npositive, otherwise 0. ti stores 4 parameterized coordinates of\\nthe predicted bounding box while t∗\\ni is related to the ground-\\ntruth box overlapping with a positive anchor. Lcls is a binary\\nlog loss and Lreg is a smoothed L1 loss similar to (2). These\\ntwo terms are normalized with the mini-batch size ( Ncls)\\nand the number of anchor locations ( Nreg), respectively. In\\nthe form of fully-convolutional networks, Faster R-CNN can\\nbe trained end-to-end by back-propagation and SGD in an\\nalternate training manner.\\nWith the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='With the proposal of Faster R-CNN, region proposal based\\nCNN architectures for object detection can really be trained\\nin an end-to-end way. Also a frame rate of 5 FPS (Frame\\nPer Second) on a GPU is achieved with state-of-the-art object\\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\\never, the alternate training algorithm is very time-consuming\\nand RPN produces object-like regions (including backgrounds)\\ninstead of object instances and is not skilled in dealing with\\nobjects with extreme scales or shapes.\\n5) R-FCN: Divided by the RoI pooling layer, a prevalent\\nfamily [16], [18] of deep networks for object detection are\\ncomposed of two subnetworks: a shared fully convolutional\\nsubnetwork (independent of RoIs) and an unshared RoI-wise\\nsubnetwork. This decomposition originates from pioneering\\nclassiﬁcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciﬁc spatial pooling layer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='classiﬁcation architectures (e.g. AlexNet [6] and VGG16 [46])\\nwhich consist of a convolutional subnetwork and several FC\\nlayers separated by a speciﬁc spatial pooling layer.\\nRecent state-of-the-art image classiﬁcation networks, such\\nas Residual Nets (ResNets) [47] and GoogLeNets [45], [93],\\nare fully convolutional. To adapt to these architectures, it’s\\nFeature Pyramid Networks for Object Detection\\nTsung-Yi Lin1,2, Piotr Doll´ar1, Ross Girshick1,\\nKaiming He1, Bharath Hariharan1, and Serge Belongie2\\n1Facebook AI Research (FAIR)\\n2Cornell University and Cornell Tech\\nAbstract\\nFeature pyramids are a basic component in recognition\\nsystems for detecting objects at different scales. But recent\\ndeep learning object detectors have avoided pyramid rep-\\nresentations, in part because they are compute and memory\\nintensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='intensive. In this paper, we exploit the inherent multi-scale,\\npyramidal hierarchy of deep convolutional networks to con-\\nstruct feature pyramids with marginal extra cost. A top-\\ndown architecture with lateral connections is developed for\\nbuilding high-level semantic feature maps at all scales. This\\narchitecture, called a Feature Pyramid Network (FPN),\\nshows signiﬁcant improvement as a generic feature extrac-\\ntor in several applications. Using FPN in a basic Faster\\nR-CNN system, our method achieves state-of-the-art single-\\nmodel results on the COCO detection benchmark without\\nbells and whistles, surpassing all existing single-model en-\\ntries including those from the COCO 2016 challenge win-\\nners. In addition, our method can run at 6 FPS on a GPU\\nand thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and thus is a practical and accurate solution to multi-scale\\nobject detection. Code will be made publicly available.\\n1. Introduction\\nRecognizing objects at vastly different scales is a fun-\\ndamental challenge in computer vision. Feature pyramids\\nbuilt upon image pyramids (for short we call these featur-\\nized image pyramids) form the basis of a standard solution\\n[1] (Fig. 1(a)). These pyramids are scale-invariant in the\\nsense that an object’s scale change is offset by shifting its\\nlevel in the pyramid. Intuitively, this property enables a\\nmodel to detect objects across a large range of scales by\\nscanning the model over both positions and pyramid levels.\\nFeaturized image pyramids were heavily used in the\\nera of hand-engineered features [5, 25]. They were so\\ncritical that object detectors like DPM [7] required dense\\nscale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scale sampling to achieve good results ( e.g., 10 scales per\\noctave). For recognition tasks, engineered features have\\n(a) Featurized image pyramid\\npredict\\npredict\\npredict\\npredict\\n(b) Single feature map\\npredict\\n(d) Feature Pyramid Network\\npredict\\npredict\\npredict\\n(c) Pyramidal feature hierarchy\\npredict\\npredict\\npredict\\nFigure 1. (a) Using an image pyramid to build a feature pyramid.\\nFeatures are computed on each of the image scales independently,\\nwhich is slow. (b) Recent detection systems have opted to use\\nonly single scale features for faster detection. (c) An alternative is\\nto reuse the pyramidal feature hierarchy computed by a ConvNet\\nas if it were a featurized image pyramid. (d) Our proposed Feature\\nPyramid Network (FPN) is fast like (b) and (c), but more accurate.\\nIn this ﬁgure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='In this ﬁgure, feature maps are indicate by blue outlines and thicker\\noutlines denote semantically stronger features.\\nlargely been replaced with features computed by deep con-\\nvolutional networks (ConvNets) [19, 20]. Aside from being\\ncapable of representing higher-level semantics, ConvNets\\nare also more robust to variance in scale and thus facilitate\\nrecognition from features computed on a single input scale\\n[15, 11, 29] (Fig. 1(b)). But even with this robustness, pyra-\\nmids are still needed to get the most accurate results. All re-\\ncent top entries in the ImageNet [33] and COCO [21] detec-\\ntion challenges use multi-scale testing on featurized image\\npyramids (e.g., [16, 35]). The principle advantage of fea-\\nturizing each level of an image pyramid is that it produces\\na multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a multi-scale feature representation in which all levels are\\nsemantically strong, including the high-resolution levels.\\nNevertheless, featurizing each level of an image pyra-\\nmid has obvious limitations. Inference time increases con-\\nsiderably (e.g., by four times [11]), making this approach\\nimpractical for real applications. Moreover, training deep\\n1\\narXiv:1612.03144v2  [cs.CV]  19 Apr 2017\\nFig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid\\nto build a feature pyramid. (b) Only single scale features is adopted for faster\\ndetection. (c) An alternative to the featurized image pyramid is to reuse the\\npyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both\\n(b) and (c). Blue outlines indicate feature maps and thicker outlines denote\\nsemantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='semantically stronger features.\\nnatural to construct a fully convolutional object detection net-\\nwork without RoI-wise subnetwork. However, it turns out to be\\ninferior with such a naive solution [47]. This inconsistence is\\ndue to the dilemma of respecting translation variance in object\\ndetection compared with increasing translation invariance in\\nimage classiﬁcation. In other words, shifting an object inside\\nan image should be indiscriminative in image classiﬁcation\\nwhile any translation of an object in a bounding box may\\nbe meaningful in object detection. A manual insertion of\\nthe RoI pooling layer into convolutions can break down\\ntranslation invariance at the expense of additional unshared\\nregion-wise layers. So Li et al. [65] proposed a region-based\\nfully convolutional networks (R-FCN, Fig. S2).\\nDifferent from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a ﬁxed grid of k×k ﬁrstly and a position-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Different from Faster R-CNN, for each category, the last\\nconv layer of R-FCN produces a total of k2 position-sensitive\\nscore maps with a ﬁxed grid of k×k ﬁrstly and a position-\\nsensitive RoI pooling layer is then appended to aggregate the\\nresponses from these score maps. Finally, in each RoI, k2\\nposition-sensitive scores are averaged to produce a C + 1-d\\nvector and softmax responses across categories are computed.\\nAnother 4k2-d conv layer is appended to obtain class-agnostic\\nbounding boxes.\\nWith R-FCN, more powerful classiﬁcation networks can be\\nadopted to accomplish object detection in a fully-convolutional\\narchitecture by sharing nearly all the layers, and state-of-the-\\nart results are obtained on both PASCAL VOC and Microsoft\\nCOCO [94] datasets at a test speed of 170ms per image.\\n6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='6) FPN: Feature pyramids built upon image pyramids\\n(featurized image pyramids) have been widely applied in\\nmany object detection systems to improve scale invariance\\n[24], [64] (Figure 7(a)). However, training time and memory\\nconsumption increase rapidly. To this end, some techniques\\ntake only a single input scale to represent high-level semantics\\nand increase the robustness to scale changes (Figure 7(b)),\\nand image pyramids are built at test time which results in\\nan inconsistency between train/test-time inferences [16], [18].\\nThe in-network feature hierarchy in a deep ConvNet produces\\nfeature maps of different spatial resolutions while introduces\\nlarge semantic gaps caused by different depths (Figure 7(c)).\\nTo avoid using low-level features, pioneer works [71], [95]\\nusually build the pyramid starting from middle layers or\\njust sum transformed feature responses, missing the higher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 7\\nFig. 8. The Mask R-CNN framework for instance segmentation [67].\\nresolution maps of the feature hierarchy.\\nDifferent from these approaches, FPN [66] holds an ar-\\nchitecture with a bottom-up pathway, a top-down pathway\\nand several lateral connections to combine low-resolution and\\nsemantically strong features with high-resolution and seman-\\ntically weak features (Figure 7(d)). The bottom-up pathway,\\nwhich is the basic forward backbone ConvNet, produces a\\nfeature hierarchy by downsampling the corresponding feature\\nmaps with a stride of 2. The layers owning the same size of\\noutput maps are grouped into the same network stage and the\\noutput of the last layer of each stage is chosen as the reference\\nset of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at ﬁrst and then enhanced with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='set of feature maps to build the following top-down pathway.\\nTo build the top-down pathway, feature maps from higher\\nnetwork stages are upsampled at ﬁrst and then enhanced with\\nthose of the same spatial size from the bottom-up pathway\\nvia lateral connections. A 1 ×1 conv layer is appended to\\nthe upsampled map to reduce channel dimensions and the\\nmergence is achieved by element-wise addition. Finally, a3×3\\nconvolution is also appended to each merged map to reduce\\nthe aliasing effect of upsampling and the ﬁnal feature map is\\ngenerated. This process is iterated until the ﬁnest resolution\\nmap is generated.\\nAs feature pyramid can extract rich semantics from all\\nlevels and be trained end-to-end with all scales, state-of-the-\\nart representation can be obtained without sacriﬁcing speed\\nand memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and memory. Meanwhile, FPN is independent of the backbone\\nCNN architectures and can be applied to different stages of\\nobject detection (e.g. region proposal generation) and to many\\nother computer vision tasks (e.g. instance segmentation).\\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\\ning task which requires detecting all objects in an image and\\nsegmenting each instance (semantic segmentation [97]). These\\ntwo tasks are usually regarded as two independent processes.\\nAnd the multi-task scheme will create spurious edge and\\nexhibit systematic errors on overlapping instances [98]. To\\nsolve this problem, parallel to the existing branches in Faster\\nR-CNN for classiﬁcation and bounding box regression, the\\nMask R-CNN [67] adds a branch to predict segmentation\\nmasks in a pixel-to-pixel manner (Figure 8).\\nDifferent from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m×m mask to maintain the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Different from the other two branches which are inevitably\\ncollapsed into short output vectors by FC layers, the segmen-\\ntation mask branch encodes an m×m mask to maintain the\\nexplicit object spatial layout. This kind of fully convolutional\\nrepresentation requires fewer parameters but is more accurate\\nthan that of [97]. Formally, besides the two losses in (1) for\\nclassiﬁcation and bounding box regression, an additional loss\\nfor segmentation mask branch is deﬁned to reach a multi-task\\nloss. An this loss is only associated with ground-truth class\\nand relies on the classiﬁcation branch to predict the category.\\nBecause RoI pooling, the core operation in Faster R-CNN,\\nperforms a coarse spatial quantization for feature extraction,\\nmisalignment is introduced between the RoI and the features.\\nIt affects classiﬁcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='It affects classiﬁcation little because of its robustness to small\\ntranslations. However, it has a large negative effect on pixel-\\nto-pixel mask prediction. To solve this problem, Mask R-CNN\\nadopts a simple and quantization-free layer, namely RoIAlign,\\nto preserve the explicit per-pixel spatial correspondence faith-\\nfully. RoIAlign is achieved by replacing the harsh quantization\\nof RoI pooling with bilinear interpolation [99], computing the\\nexact values of the input features at four regularly sampled\\nlocations in each RoI bin. In spite of its simplicity, this\\nseemingly minor change improves mask accuracy greatly,\\nespecially under strict localization metrics.\\nGiven the Faster R-CNN framework, the mask branch only\\nadds a small computational burden and its cooperation with\\nother tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='other tasks provides complementary information for object\\ndetection. As a result, Mask R-CNN is simple to implement\\nwith promising instance segmentation and object detection\\nresults. In a word, Mask R-CNN is a ﬂexible and efﬁcient\\nframework for instance-level recognition, which can be easily\\ngeneralized to other tasks (e.g. human pose estimation [7][S4])\\nwith minimal modiﬁcation.\\n8) Multi-task Learning, Multi-scale Representation and\\nContextual Modelling: Although the Faster R-CNN gets\\npromising results with several hundred proposals, it still strug-\\ngles in small-size object detection and localization, mainly due\\nto the coarseness of its feature maps and limited information\\nprovided in particular candidate boxes. The phenomenon is\\nmore obvious on the Microsoft COCO dataset which consists\\nof objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of objects at a broad range of scales, less prototypical images,\\nand requires more precise localization. To tackle these prob-\\nlems, it is of necessity to accomplish object detection with\\nmulti-task learning [100], multi-scale representation [95] and\\ncontext modelling [101] to combine complementary informa-\\ntion from multiple sources.\\nMulti-task Learning learns a useful representation for\\nmultiple correlated tasks from the same input [102], [103].\\nBrahmbhatt et al. introduced conv features trained for ob-\\nject segmentation and ‘stuff’ (amorphous categories such as\\nground and water) to guide accurate object detection of small\\nobjects (StuffNet) [100]. Dai et al. [97] presented Multitask\\nNetwork Cascades of three networks, namely class-agnostic\\nregion proposal generation, pixel-level instance segmentation\\nand regional instance classiﬁcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='region proposal generation, pixel-level instance segmentation\\nand regional instance classiﬁcation. Li et al. incorporated the\\nweakly-supervised object segmentation cues and region-based\\nobject detection into a multi-stage architecture to fully exploit\\nthe learned segmentation features [104].\\nMulti-scale Representation combines activations from\\nmultiple layers with skip-layer connections to provide seman-\\ntic information of different spatial resolutions [66]. Cai et\\nal. proposed the MS-CNN [105] to ease the inconsistency\\nbetween the sizes of objects and receptive ﬁelds with multiple\\nscale-independent output layers. Yang et al. investigated two\\nstrategies, namely scale-dependent pooling (SDP) and layer-\\nwise cascaded rejection classiﬁers (CRC), to exploit appropri-\\nate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ate scale-dependent conv features [33]. Kong et al. proposed\\nthe HyperNet to calculate the shared features between RPN\\nand object detection network by aggregating and compressing\\nhierarchical feature maps from different resolutions into a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 8\\nuniform space [101].\\nContextual Modelling improves detection performance by\\nexploiting features from or around RoIs of different support\\nregions and resolutions to deal with occlusions and local\\nsimilarities [95]. Zhu et al. proposed the SegDeepM to exploit\\nobject segmentation which reduces the dependency on initial\\ncandidate boxes with Markov Random Field [106]. Moysset\\net al. took advantage of 4 directional 2D-LSTMs [107] to\\nconvey global context between different local regions and re-\\nduced trainable parameters with local parameter-sharing [108].\\nZeng et al. proposed a novel GBD-Net by introducing gated\\nfunctions to control message transmission between different\\nsupport regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='support regions [109].\\nThe Combination incorporates different components above\\ninto the same model to improve detection performance further.\\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)\\nmodel [110] to capture different aspects of an object, the\\ndistinct appearances of various object parts and semantic\\nsegmentation-aware features. To obtain contextual and multi-\\nscale representations, Bell et al. proposed the Inside-Outside\\nNet (ION) by exploiting information both inside and outside\\nthe RoI [95] with spatial recurrent neural networks [111] and\\nskip pooling [101]. Zagoruyko et al. proposed the MultiPath\\narchitecture by introducing three modiﬁcations to the Fast\\nR-CNN [112], including multi-scale skip connections [95],\\na modiﬁed foveal structure [110] and a novel loss function\\nsumming different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='summing different IoU losses.\\n9) Thinking in Deep Learning based Object Detection:\\nApart from the above approaches, there are still many impor-\\ntant factors for continued progress.\\nThere is a large imbalance between the number of annotated\\nobjects and background examples. To address this problem,\\nShrivastava et al. proposed an effective online mining algo-\\nrithm (OHEM) [113] for automatic selection of the hard ex-\\namples, which leads to a more effective and efﬁcient training.\\nInstead of concentrating on feature extraction, Ren et al.\\nmade a detailed analysis on object classiﬁers [114], and\\nfound that it is of particular importance for object detection\\nto construct a deep and convolutional per-region classiﬁer\\ncarefully, especially for ResNets [47] and GoogLeNets [45].\\nTraditional CNN framework for object detection is not\\nskilled in handling signiﬁcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Traditional CNN framework for object detection is not\\nskilled in handling signiﬁcant scale variation, occlusion or\\ntruncation, especially when only 2D object detection is in-\\nvolved. To address this problem, Xiang et al. proposed a\\nnovel subcategory-aware region proposal network [60], which\\nguides the generation of region proposals with subcategory\\ninformation related to object poses and jointly optimize object\\ndetection and subcategory classiﬁcation.\\nOuyang et al. found that the samples from different classes\\nfollow a longtailed distribution [115], which indicates that dif-\\nferent classes with distinct numbers of samples have different\\ndegrees of impacts on feature learning. To this end, objects are\\nﬁrstly clustered into visually similar class groups, and then a\\nhierarchical feature learning scheme is adopted to learn deep\\nrepresentations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‘deep and thin’ design'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='representations for each group separately.\\nIn order to minimize computational cost and achieve the\\nstate-of-the-art performance, with the ‘deep and thin’ design\\nprinciple and following the pipeline of Fast R-CNN, Hong et\\nal. proposed the architecture of PV ANET [116], which adopts\\nsome building blocks including concatenated ReLU [117],\\nInception [45], and HyperNet [101] to reduce the expense on\\nmulti-scale feature extraction and trains the network with batch\\nnormalization [43], residual connections [47], and learning\\nrate scheduling based on plateau detection [47]. The PV ANET\\nachieves the state-of-the-art performance and can be processed\\nin real time on Titan X GPU (21 FPS).\\nB. Regression /Classiﬁcation Based Framework\\nRegion proposal based frameworks are composed of sev-\\neral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiﬁcation and bounding box\\nregression, which are usually trained separately. Even in recent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='eral correlated stages, including region proposal generation,\\nfeature extraction with CNN, classiﬁcation and bounding box\\nregression, which are usually trained separately. Even in recent\\nend-to-end module Faster R-CNN, an alternative training is\\nstill required to obtain shared convolution parameters between\\nRPN and detection network. As a result, the time spent in\\nhandling different components becomes the bottleneck in real-\\ntime application.\\nOne-step frameworks based on global regres-\\nsion/classiﬁcation, mapping straightly from image pixels\\nto bounding box coordinates and class probabilities, can\\nreduce time expense. We ﬁrstly reviews some pioneer CNN\\nmodels, and then focus on two signiﬁcant frameworks,\\nnamely You only look once (YOLO) [17] and Single Shot\\nMultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiﬁcation task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MultiBox Detector (SSD) [71].\\n1) Pioneer Works: Previous to YOLO and SSD, many\\nresearchers have already tried to model object detection as\\na regression or classiﬁcation task.\\nSzegedy et al. formulated object detection task as a DNN-\\nbased regression [118], generating a binary mask for the\\ntest image and extracting detections with a simple bounding\\nbox inference. However, the model has difﬁculty in handling\\noverlapping objects, and bounding boxes generated by direct\\nupsampling is far from perfect.\\nPinheiro et al. proposed a CNN model with two branches:\\none generates class agnostic segmentation masks and the\\nother predicts the likelihood of a given patch centered on\\nan object [119]. Inference is efﬁcient since class scores and\\nsegmentation can be obtained in a single model with most of\\nthe CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniﬁed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the CNN operations shared.\\nErhan et al. proposed regression based MultiBox to produce\\nscored class-agnostic region proposals [68], [120]. A uniﬁed\\nloss was introduced to bias both localization and conﬁdences\\nof multiple components to predict the coordinates of class-\\nagnostic bounding boxes. However, a large quantity of addi-\\ntional parameters are introduced to the ﬁnal layer.\\nYoo et al. adopted an iterative classiﬁcation approach to\\nhandle object detection and proposed an impressive end-to-\\nend CNN architecture named AttentionNet [69]. Starting from\\nthe top-left (TL) and bottom-right (BR) corner of an image,\\nAttentionNet points to a target object by generating quantized\\nweak directions and converges to an accurate object bound-\\nary box with an ensemble of iterative predictions. However,\\nthe model becomes quite inefﬁcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the model becomes quite inefﬁcient when handling multiple\\ncategories with a progressive two-step procedure.\\nNajibi et al. proposed a proposal-free iterative grid based\\nobject detector (G-CNN), which models object detection as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 9\\nFig. 9. Main idea of YOLO [17].\\nﬁnding a path from a ﬁxed grid to boxes tightly surrounding\\nthe objects [70]. Starting with a ﬁxed multi-scale bounding box\\ngrid, G-CNN trains a regressor to move and scale elements of\\nthe grid towards objects iteratively. However, G-CNN has a\\ndifﬁculty in dealing with small or highly overlapping objects.\\n2) YOLO: Redmon et al. [17] proposed a novel framework\\ncalled YOLO, which makes use of the whole topmost feature\\nmap to predict both conﬁdences for multiple categories and\\nbounding boxes. The basic idea of YOLO is exhibited in\\nFigure 9. YOLO divides the input image into an S×S grid and\\neach grid cell is responsible for predicting the object centered\\nin that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conﬁdence scores. Formally, conﬁ-\\ndence scores are deﬁned as Pr(Object) ∗IOUtruth\\npred , which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in that grid cell. Each grid cell predicts B bounding boxes\\nand their corresponding conﬁdence scores. Formally, conﬁ-\\ndence scores are deﬁned as Pr(Object) ∗IOUtruth\\npred , which\\nindicates how likely there exist objects ( Pr(Object) ≥0) and\\nshows conﬁdences of its prediction ( IOUtruth\\npred ). At the same\\ntime, regardless of the number of boxes, C conditional class\\nprobabilities (Pr(Classi|Object)) should also be predicted in\\neach grid cell. It should be noticed that only the contribution\\nfrom the grid cell containing an object is calculated.\\nAt test time, class-speciﬁc conﬁdence scores for each box\\nare achieved by multiplying the individual box conﬁdence\\npredictions with the conditional class probabilities as follows.\\nPr(Object) ∗IOUtruth\\npred ∗Pr(Classi|Object)\\n= Pr(Classi) ∗IOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speciﬁc objects in the\\nbox and the ﬁtness between the predicted box and the object\\nare both taken into consideration.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='= Pr(Classi) ∗IOUtruth\\npred\\n(5)\\nwhere the existing probability of class-speciﬁc objects in the\\nbox and the ﬁtness between the predicted box and the object\\nare both taken into consideration.\\nDuring training, the following loss function is optimized,\\nλcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n[\\n(xi −ˆxi)2 + (yi −ˆyi)2]\\n+λcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n[(√wi −\\n√\\nˆwi)2 + (\\n√\\nhi −\\n√\\nˆhi\\n)2]\\n+\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n(\\nCi −ˆCi\\n)2\\n+λnoobj\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 noobj\\nij\\n(\\nCi −ˆCi\\n)2\\n+\\nS2\\n∑\\ni=0\\n1 obj\\ni\\n∑\\nc∈classes\\n(pi(c) −ˆpi(c))2\\n(6)\\nIn a certain cell i, (xi,yi) denote the center of the box relative\\nto the bounds of the grid cell,(wi,hi) are the normalized width\\nand height relative to the image size, Ci represents conﬁdence\\nscores, 1 obj\\ni indicates the existence of objects and 1 obj\\nij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiﬁcation errors. Similarly,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ij denotes\\nthat the prediction is conducted by the jth bounding box\\npredictor. Note that only when an object is present in that grid\\ncell, the loss function penalizes classiﬁcation errors. Similarly,\\nwhen the predictor is ‘responsible’ for the ground truth box\\n(i.e. the highest IoU of any predictor in that grid cell is\\nachieved), bounding box coordinate errors are penalized.\\nThe YOLO consists of 24 conv layers and 2 FC layers,\\nof which some conv layers construct ensembles of inception\\nmodules with 1 ×1 reduction layers followed by 3 ×3 conv\\nlayers. The network can process images in real-time at 45\\nFPS and a simpliﬁed version Fast YOLO can reach 155 FPS\\nwith better results than other real-time detectors. Furthermore,\\nYOLO produces fewer false positives on background, which\\nmakes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='makes the cooperation with Fast R-CNN become possible. An\\nimproved version, YOLOv2, was later proposed in [72], which\\nadopts several impressive strategies, such as BN, anchor boxes,\\ndimension cluster and multi-scale training.\\n3) SSD: YOLO has a difﬁculty in dealing with small\\nobjects in groups, which is caused by strong spatial constraints\\nimposed on bounding box predictions [17]. Meanwhile, YOLO\\nstruggles to generalize to objects in new/unusual aspect ratios/\\nconﬁgurations and produces relatively coarse features due to\\nmultiple downsampling operations.\\nAiming at these problems, Liu et al. proposed a Single Shot\\nMultiBox Detector (SSD) [71], which was inspired by the\\nanchors adopted in MultiBox [68], RPN [18] and multi-scale\\nrepresentation [95]. Given a speciﬁc feature map, instead of\\nﬁxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ﬁxed grids adopted in YOLO, the SSD takes advantage of a set\\nof default anchor boxes with different aspect ratios and scales\\nto discretize the output space of bounding boxes. To handle\\nobjects with various sizes, the network fuses predictions from\\nmultiple feature maps with different resolutions .\\nThe architecture of SSD is demonstrated in Figure 10. Given\\nthe VGG16 backbone architecture, SSD adds several feature\\nlayers to the end of the network, which are responsible for\\npredicting the offsets to default boxes with different scales and\\naspect ratios and their associated conﬁdences. The network is\\ntrained with a weighted sum of localization loss (e.g. Smooth\\nL1) and conﬁdence loss (e.g. Softmax), which is similar to\\n(1). Final detection results are obtained by conducting NMS\\non multi-scale reﬁned bounding boxes.\\nIntegrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiﬁcantly outperforms the Faster R-CNN in terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Integrating with hard negative mining, data augmentation\\nand a larger number of carefully chosen default anchors,\\nSSD signiﬁcantly outperforms the Faster R-CNN in terms of\\naccuracy on PASCAL VOC and COCO, while being three\\ntimes faster. The SSD300 (input image size is 300×300) runs\\nat 59 FPS, which is more accurate and efﬁcient than YOLO.\\nHowever, SSD is not skilled at dealing with small objects,\\nwhich can be relieved by adopting better feature extractor\\nbackbone (e.g. ResNet101), adding deconvolution layers with\\nskip connections to introduce additional large-scale context\\n[73] and designing better network structure (e.g. Stem Block\\nand Dense Block) [74].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 10\\nFig. 10. The architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor\\nboxes and their associated conﬁdences. Final detection results are obtained by conducting NMS on multi-scale reﬁned bounding boxes.\\nC. Experimental Evaluation\\nWe compare various object detection methods on three\\nbenchmark datasets, including PASCAL VOC 2007 [25],\\nPASCAL VOC 2012 [121] and Microsoft COCO [94]. The\\nevaluated approaches include R-CNN [15], SPP-net [64], Fast\\nR-CNN [16], NOC [114], Bayes [85], MR-CNN &S-CNN\\n[105], Faster R-CNN [18], HyperNet [101], ION [95], MS-\\nGR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM\\n[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\\n[109], PV ANET [116], YOLO [17], YOLOv2 [72], R-FCN\\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD\\n[74]. If no speciﬁc instructions for the adopted framework\\nare provided, the utilized model is a VGG16 [46] pretrained\\non 1000-way ImageNet classiﬁcation task [39]. Due to the\\nlimitation of paper length, we only provide an overview, in-\\ncluding proposal, learning method, loss function, programming\\nlanguage and platform, of the prominent architectures in Table\\nI. Detailed experimental settings, which can be found in the\\noriginal papers, are missed. In addition to the comparisons of\\ndetection accuracy, another comparison is provided to evaluate\\ntheir test consumption on PASCAL VOC 2007.\\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\\n2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2012 datasets consist of 20 categories. The evaluation terms\\nare Average Precision (AP) in each single category and mean\\nAverage Precision (mAP) across all the 20 categories. Com-\\nparative results are exhibited in Table II and III, from which\\nthe following remarks can be obtained.\\n•If incorporated with a proper way, more powerful back-\\nbone CNN models can deﬁnitely improve object detection\\nperformance (the comparison among R-CNN with AlexNet,\\nR-CNN with VGG16 and SPP-net with ZF-Net [122]).\\n• With the introduction of SPP layer (SPP-net), end-to-\\nend multi-task architecture (FRCN) and RPN (Faster R-\\nCNN), object detection performance is improved gradually\\nand apparently.\\n•Due to large quantities of trainable parameters, in order to\\nobtain multi-level robust features, data augmentation is very\\nimportant for deep learning based models (Faster R-CNN\\nwith ‘07’ ,‘07+12’ and ‘07+12+coco’).\\n•Apart from basic models, there are still many other factors'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='important for deep learning based models (Faster R-CNN\\nwith ‘07’ ,‘07+12’ and ‘07+12+coco’).\\n•Apart from basic models, there are still many other factors\\naffecting object detection performance, such as multi-scale\\nand multi-region feature extraction (e.g. MR-CNN), modi-\\nﬁed classiﬁcation networks (e.g. NOC), additional informa-\\ntion from other correlated tasks (e.g. StuffNet, HyperNet),\\nmulti-scale representation (e.g. ION) and mining of hard\\nnegative samples (e.g. OHEM).\\n•As YOLO is not skilled in producing object localizations\\nof high IoU, it obtains a very poor result on VOC 2012.\\nHowever, with the complementary information from Fast\\nR-CNN (YOLO+FRCN) and the aid of other strategies,\\nsuch as anchor boxes, BN and ﬁne grained features, the\\nlocalization errors are corrected (YOLOv2).\\n•By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='•By combining many recent tricks and modelling the whole\\nnetwork as a fully convolutional one, R-FCN achieves a\\nmore obvious improvement of detection performance over\\nother approaches.\\n2) Microsoft COCO: Microsoft COCO is composed of\\n300,000 fully segmented images, in which each image has\\nan average of 7 object instances from a total of 80 categories.\\nAs there are a lot of less iconic objects with a broad range\\nof scales and a stricter requirement on object localization,\\nthis dataset is more challenging than PASCAL 2012. Object\\ndetection performance is evaluated by AP computed under\\ndifferent degrees of IoUs and on different object sizes. The\\nresults are shown in Table IV.\\nBesides similar remarks to those of PASCAL VOC, some\\nother conclusions can be drawn as follows from Table IV.\\n• Multi-scale training and test are beneﬁcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='• Multi-scale training and test are beneﬁcial in improv-\\ning object detection performance, which provide additional\\ninformation in different resolutions (R-FCN). FPN and\\nDSSD provide some better ways to build feature pyramids\\nto achieve multi-scale representation. The complementary\\ninformation from other related tasks is also helpful for\\naccurate object localization (Mask R-CNN with instance\\nsegmentation task).\\n• Overall, region proposal based methods, such as\\nFaster R-CNN and R-FCN, perform better than regres-\\nsion/classﬁcation based approaches, namely YOLO and\\nSSD, due to the fact that quite a lot of localization errors\\nare produced by regression/classﬁcation based approaches.\\n• Context modelling is helpful to locate small objects,\\nwhich provides additional information by consulting nearby\\nobjects and surroundings (GBD-Net and multi-path).\\n•Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='objects and surroundings (GBD-Net and multi-path).\\n•Due to the existence of a large number of nonstandard\\nsmall objects, the results on this dataset are much worse\\nthan those of VOC 2007/2012. With the introduction of\\nother powerful frameworks (e.g. ResNeXt [123]) and useful\\nstrategies (e.g. multi-task learning [67], [124]), the perfor-\\nmance can be improved.\\n•The success of DSOD in training from scratch stresses the\\nimportance of network design to release the requirements\\nfor perfect pre-trained classiﬁers on relevant tasks and large\\nnumbers of annotated samples.\\n3) Timing Analysis: Timing analysis (Table V) is conducted\\non Intel i7-6700K CPU with a single core and NVIDIA Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 11\\nTABLE I\\nAN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES .\\nFramework Proposal Multi-scale Input Learning Method Loss Function Softmax Layer End-to-end Train Platform Language\\nR-CNN [15] Selective Search - SGD,BP Hinge loss (classiﬁcation),Bounding box regression + - Caffe Matlab\\nSPP-net [64] EdgeBoxes + SGD Hinge loss (classiﬁcation),Bounding box regression + - Caffe Matlab\\nFast RCNN [16] Selective Search + SGD Class Log loss+bounding box regression + - Caffe Python\\nFaster R-CNN [18] RPN + SGD Class Log loss+bounding box regression + + Caffe Python/Matlab\\nR-FCN [65] RPN + SGD Class Log loss+bounding box regression - + Caffe Matlab\\nMask R-CNN [67] RPN + SGD Class Log loss+bounding box regression + + TensorFlow/Keras Python+Semantic sigmoid loss\\nFPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='FPN [66] RPN + Synchronized SGD Class Log loss+bounding box regression + + TensorFlow Python\\nYOLO [17] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object conﬁdence+background conﬁdence\\nSSD [71] - - SGD Class softmax loss+bounding box regression - + Caffe C++\\nYOLOv2 [72] - - SGD Class sum-squared error loss+bounding box regression + + Darknet C+object conﬁdence+background conﬁdence\\n* ‘+’ denotes that corresponding techniques are employed while ‘-’ denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the\\nother architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions\\nby the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='by the referenced authors.\\nTABLE II\\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN (Alex) [15] 07 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 68.6 58.5\\nR-CNN(VGG16) [15] 07 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nSPP-net(ZF) [64] 07 68.5 71.7 58.7 41.9 42.5 67.7 72.1 73.8 34.7 67.0 63.4 66.0 72.5 71.3 58.9 32.8 60.9 56.1 67.9 68.8 60.9\\nGCNN [70] 07 68.3 77.3 68.5 52.4 38.6 78.5 79.5 81.0 47.1 73.6 64.5 77.2 80.5 75.8 66.6 34.3 65.2 64.4 75.6 66.4 66.8\\nBayes [85] 07 74.1 83.2 67.0 50.8 51.6 76.2 81.4 77.2 48.1 78.9 65.6 77.3 78.4 75.1 70.1 41.4 69.6 60.8 70.2 73.7 68.5\\nFast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Fast R-CNN [16] 07+12 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 70.0\\nSDP+CRC [33] 07 76.1 79.4 68.2 52.6 46.0 78.4 78.4 81.0 46.7 73.5 65.3 78.6 81.0 76.7 77.3 39.0 65.1 67.2 77.5 70.3 68.9\\nSubCNN [60] 07 70.2 80.5 69.5 60.3 47.9 79.0 78.7 84.2 48.5 73.9 63.0 82.7 80.6 76.0 70.2 38.2 62.4 67.7 77.7 60.5 68.5\\nStuffNet30 [100] 07 72.6 81.7 70.6 60.5 53.0 81.5 83.7 83.9 52.2 78.9 70.7 85.0 85.7 77.0 78.7 42.2 73.6 69.2 79.2 73.8 72.7\\nNOC [114] 07+12 76.3 81.4 74.4 61.7 60.8 84.7 78.2 82.9 53.0 79.2 69.2 83.2 83.2 78.5 68.0 45.0 71.6 76.7 82.2 75.7 73.3\\nMR-CNN&S-CNN [110] 07+12 80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0 76.4 48.5 76.3 75.5 85.0 81.0 78.2\\nHyperNet [101] 07+12 77.4 83.3 75.0 69.1 62.4 83.1 87.4 87.4 57.1 79.8 71.4 85.1 85.1 80.0 79.1 51.2 79.1 75.7 80.9 76.5 76.3\\nMS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='MS-GR [104] 07+12 80.0 81.0 77.4 72.1 64.3 88.2 88.1 88.4 64.4 85.4 73.1 87.3 87.4 85.1 79.6 50.1 78.4 79.5 86.9 75.5 78.6\\nOHEM+Fast R-CNN [113] 07+12 80.6 85.7 79.8 69.9 60.8 88.3 87.9 89.6 59.7 85.1 76.5 87.1 87.3 82.4 78.8 53.7 80.5 78.7 84.5 80.7 78.9\\nION [95] 07+12+S 80.2 85.2 78.8 70.9 62.6 86.6 86.9 89.8 61.7 86.9 76.5 88.4 87.5 83.4 80.5 52.4 78.1 77.2 86.9 83.5 79.2\\nFaster R-CNN [18] 07 70.0 80.6 70.1 57.3 49.9 78.2 80.4 82.0 52.2 75.3 67.2 80.3 79.8 75.0 76.3 39.1 68.3 67.3 81.1 67.6 69.9\\nFaster R-CNN [18] 07+12 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 73.2\\nFaster R-CNN [18] 07+12+COCO 84.3 82.0 77.7 68.9 65.7 88.1 88.4 88.9 63.6 86.3 70.8 85.9 87.6 80.1 82.3 53.6 80.4 75.8 86.6 78.9 78.8\\nSSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD300 [71] 07+12+COCO 80.9 86.3 79.0 76.2 57.6 87.3 88.2 88.6 60.5 85.4 76.7 87.5 89.2 84.5 81.4 55.0 81.9 81.5 85.9 78.9 79.6\\nSSD512 [71] 07+12+COCO 86.6 88.3 82.4 76.0 66.3 88.6 88.9 89.1 65.1 88.4 73.6 86.5 88.9 85.3 84.6 59.1 85.0 80.4 87.4 81.2 81.6\\n* ‘07’: VOC2007 trainval, ‘07+12’: union of VOC2007 and VOC2012 trainval, ‘07+12+COCO’: trained on COCO trainval35k at ﬁrst and then ﬁne-tuned on 07+12. The S in ION ‘07+12+S’ denotes SBD segmentation labels.\\nTABLE III\\nCOMPARATIVE RESULTS ON VOC 2012 TEST SET (%).\\nMethods Trained on areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN(Alex) [15] 12 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1 53.3\\nR-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='R-CNN(VGG16) [15] 12 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3 62.4\\nBayes [85] 12 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2 66.4\\nFast R-CNN [16] 07++12 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2 68.4\\nSutffNet30 [100] 12 83.0 76.9 71.2 51.6 50.1 76.4 75.7 87.8 48.3 74.8 55.7 85.7 81.2 80.3 79.5 44.2 71.8 61.0 78.5 65.4 70.0\\nNOC [114] 07+12 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1 68.8\\nMR-CNN&S-CNN [110] 07++12 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0 73.9\\nHyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='HyperNet [101] 07++12 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 71.4\\nOHEM+Fast R-CNN [113] 07++12+coco 90.1 87.4 79.9 65.8 66.3 86.1 85.0 92.9 62.4 83.4 69.5 90.6 88.9 88.9 83.6 59.0 82.0 74.7 88.2 77.3 80.1\\nION [95] 07+12+S 87.5 84.7 76.8 63.8 58.3 82.6 79.0 90.9 57.8 82.0 64.7 88.9 86.5 84.7 82.3 51.4 78.2 69.2 85.2 73.5 76.4\\nFaster R-CNN [18] 07++12 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 70.4\\nFaster R-CNN [18] 07++12+coco 87.4 83.6 76.8 62.9 59.6 81.9 82.0 91.3 54.9 82.6 59.0 89.0 85.5 84.7 84.1 52.2 78.9 65.5 85.4 70.2 75.9\\nYOLO [17] 07++12 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8 57.9\\nYOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLO+Fast R-CNN [17] 07++12 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2 70.7\\nYOLOv2 [72] 07++12+coco 88.8 87.0 77.8 64.9 51.8 85.2 79.3 93.1 64.4 81.4 70.2 91.3 88.1 87.2 81.0 57.7 78.1 71.0 88.5 76.8 78.2\\nSSD300 [71] 07++12+coco 91.0 86.0 78.1 65.0 55.4 84.9 84.0 93.4 62.1 83.6 67.3 91.3 88.9 88.6 85.6 54.7 83.8 77.3 88.3 76.5 79.3\\nSSD512 [71] 07++12+coco 91.4 88.6 82.6 71.4 63.1 87.4 88.1 93.9 66.9 86.6 66.3 92.0 91.7 90.8 88.5 60.9 87.0 75.4 90.2 80.4 82.2\\nR-FCN (ResNet101) [16] 07++12+coco 92.3 89.9 86.7 74.7 75.2 86.7 89.0 95.8 70.2 90.4 66.5 95.0 93.2 92.1 91.1 71.0 89.7 76.0 92.0 83.4 85.0\\n* ‘07++12’: union of VOC2007 trainval and test and VOC2012 trainval. ‘07++12+COCO’: trained on COCO trainval35k at ﬁrst then ﬁne-tuned on 07++12.\\nTABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='TABLE IV\\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\\nMethods Trained on 0.5:0.95 0.5 0.75 S M L 1 10 100 S M L\\nFast R-CNN [16] train 20.5 39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0\\nION [95] train 23.6 43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6\\nNOC+FRCN(VGG16) [114] train 21.2 41.5 19.7 - - - - - - - - -\\nNOC+FRCN(Google) [114] train 24.8 44.4 25.2 - - - - - - - - -\\nNOC+FRCN (ResNet101) [114] train 27.2 48.4 27.6 - - - - - - - - -\\nGBD-Net [109] train 27.0 45.8 - - - - - - - - - -\\nOHEM+FRCN [113] train 22.6 42.5 22.2 5.0 23.7 34.6 - - - - - -\\nOHEM+FRCN* [113] train 24.4 44.4 24.8 7.1 26.4 37.9 - - - - - -\\nOHEM+FRCN* [113] trainval 25.5 45.9 26.1 7.4 27.7 38.5 - - - - - -\\nFaster R-CNN [18] trainval 24.2 45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4\\nYOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='YOLOv2 [72] trainval35k 21.6 44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\\nSSD300 [71] trainval35k 23.2 41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5\\nSSD512 [71] trainval35k 26.8 46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0\\nR-FCN (ResNet101) [65] trainval 29.2 51.5 - 10.8 32.8 45.0 - - - - - -\\nR-FCN*(ResNet101) [65] trainval 29.9 51.9 - 10.4 32.4 43.3 - - - - - -\\nR-FCN**(ResNet101) [65] trainval 31.5 53.2 - 14.3 35.5 44.2 - - - - - -\\nMulti-path [112] trainval 33.2 51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4\\nFPN (ResNet101) [66] trainval35k 36.2 59.1 39.0 18.2 39.0 48.2 - - - - - -\\nMask (ResNet101+FPN) [67] trainval35k 38.2 60.3 41.7 20.1 41.1 50.2 - - - - - -\\nMask (ResNeXt101+FPN) [67] trainval35k 39.8 62.3 43.4 22.1 43.2 51.2 - - - - - -\\nDSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='DSSD513 (ResNet101) [73] trainval35k 33.2 53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\\nDSOD300 [74] trainval 29.3 47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0\\n* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN\\nwith multi-scale training and testing, Mask: Mask R-CNN.\\nX GPU. Except for ‘SS’ which is processed with CPU, the\\nother procedures related to CNN are all evaluated on GPU.\\nFrom Table V, we can draw some conclusions as follows.\\n• By computing CNN features on shared feature maps\\n(SPP-net), test consumption is reduced largely. Test time is\\nfurther reduced with the uniﬁed multi-task learning (FRCN)\\nand removal of additional region proposal generation stage\\n(Faster R-CNN). It’s also helpful to compress the parameters\\nof FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of FC layers with SVD [91] (PA VNET and FRCN).\\nTABLE V\\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET .\\nMethods Trained on mAP(%) Test time(sec/img) Rate(FPS)\\nSS+R-CNN [15] 07 66.0 32.84 0.03\\nSS+SPP-net [64] 07 63.1 2.3 0.44\\nSS+FRCN [16] 07+12 66.9 1.72 0.6\\nSDP+CRC [33] 07 68.9 0.47 2.1\\nSS+HyperNet* [101] 07+12 76.3 0.20 5\\nMR-CNN&S-CNN [110] 07+12 78.2 30 0.03\\nION [95] 07+12+S 79.2 1.92 0.5\\nFaster R-CNN(VGG16) [18] 07+12 73.2 0.11 9.1\\nFaster R-CNN(ResNet101) [18] 07+12 83.8 2.24 0.4\\nYOLO [17] 07+12 63.4 0.02 45\\nSSD300 [71] 07+12 74.3 0.02 46\\nSSD512 [71] 07+12 76.8 0.05 19\\nR-FCN(ResNet101) [65] 07+12+coco 83.6 0.17 5.9\\nYOLOv2(544*544) [72] 07+12 78.6 0.03 40\\nDSSD321(ResNet101) [73] 07+12 78.6 0.07 13.6\\nDSOD300 [74] 07+12+coco 81.7 0.06 17.4\\nPV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: ‘fast mode’ Selective Search [16], HyperNet*: the speed up version of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='PV ANET+ [116] 07+12+coco 83.8 0.05 21.7\\nPV ANET+(compress) [116] 07+12+coco 82.9 0.03 31.3\\n* SS: Selective Search [15], SS*: ‘fast mode’ Selective Search [16], HyperNet*: the speed up version of\\nHyperNet and PA VNET+ (compresss): PA VNET with additional bounding box voting and compressed fully\\nconvolutional layers.\\n•It takes additional test time to extract multi-scale fea-\\ntures and contextual information (ION and MR-RCNN &S-\\nRCNN).\\n•It takes more time to train a more complex and deeper\\nnetwork (ResNet101 against VGG16) and this time con-\\nsumption can be reduced by adding as many layers into\\nshared fully convolutional layers as possible (FRCN).\\n•Regression based models can usually be processed in real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 12\\ntime at the cost of a drop in accuracy compared with region\\nproposal based models. Also, region proposal based models\\ncan be modiﬁed into real-time systems with the introduction\\nof other tricks [116] (PV ANET), such as BN [43], residual\\nconnections [123].\\nIV. S ALIENT OBJECT DETECTION\\nVisual saliency detection, one of the most important and\\nchallenging tasks in computer vision, aims to highlight the\\nmost dominant object regions in an image. Numerous ap-\\nplications incorporate the visual saliency to improve their\\nperformance, such as image cropping [125] and segmentation\\n[126], image retrieval [57] and object detection [66].\\nBroadly, there are two branches of approaches in salient\\nobject detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='object detection, namely bottom-up (BU) [127] and top-down\\n(TD) [128]. Local feature contrast plays the central role in BU\\nsalient object detection, regardless of the semantic contents of\\nthe scene. To learn local feature contrast, various local and\\nglobal features are extracted from pixels, e.g. edges [129],\\nspatial information [130]. However, high-level and multi-scale\\nsemantic information cannot be explored with these low-level\\nfeatures. As a result, low contrast salient maps instead of\\nsalient objects are obtained. TD salient object detection is task-\\noriented and takes prior knowledge about object categories\\nto guide the generation of salient maps. Taking semantic\\nsegmentation as an example, a saliency map is generated in the\\nsegmentation to assign pixels to particular object categories via\\na TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='a TD approach [131]. In a word, TD saliency can be viewed\\nas a focus-of-attention mechanism, which prunes BU salient\\npoints that are unlikely to be parts of the object [132].\\nA. Deep learning in Salient Object Detection\\nDue to the signiﬁcance for providing high-level and multi-\\nscale feature representation and the successful applications\\nin many correlated computer vision tasks, such as semantic\\nsegmentation [131], edge detection [133] and generic object\\ndetection [16], it is feasible and necessary to extend CNN to\\nsalient object detection.\\nThe early work by Eleonora Vig et al. [28] follows a\\ncompletely automatic data-driven approach to perform a large-\\nscale search for optimal features, namely an ensemble of deep\\nnetworks with different layers and parameters. To address the\\nproblem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='problem of limited training data, Kummerer et al. proposed the\\nDeep Gaze [134] by transferring from the AlexNet to generate\\na high dimensional feature space and create a saliency map. A\\nsimilar architecture was proposed by Huang et al. to integrate\\nsaliency prediction into pre-trained object recognition DNNs\\n[135]. The transfer is accomplished by ﬁne-tuning DNNs’\\nweights with an objective function based on the saliency\\nevaluation metrics, such as Similarity, KL-Divergence and\\nNormalized Scanpath Saliency.\\nSome works combined local and global visual clues to\\nimprove salient object detection performance. Wang et al.\\ntrained two independent deep CNNs (DNN-L and DNN-G)\\nto capture local information and global contrast and predicted\\nsaliency maps by integrating both local estimation and global\\nsearch [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reﬁned the results'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='search [136]. Cholakkal et al. proposed a weakly supervised\\nsaliency detection framework to combine visual saliency from\\nbottom-up and top-down saliency maps, and reﬁned the results\\nwith a multi-scale superpixel-averaging [137]. Zhao et al.\\nproposed a multi-context deep learning framework, which\\nutilizes a uniﬁed learning framework to model global and\\nlocal context jointly with the aid of superpixel segmentation\\n[138]. To predict saliency in videos, Bak et al. fused two\\nstatic saliency models, namely spatial stream net and tem-\\nporal stream net, into a two-stream framework with a novel\\nempirically grounded data augmentation technique [139].\\nComplementary information from semantic segmentation\\nand context modeling is beneﬁcial. To learn internal represen-\\ntations of saliency efﬁciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tations of saliency efﬁciently, He et al. proposed a novel su-\\nperpixelwise CNN approach called SuperCNN [140], in which\\nsalient object detection is formulated as a binary labeling\\nproblem. Based on a fully convolutional neural network, Li\\net al. proposed a multi-task deep saliency model, in which\\nintrinsic correlations between saliency detection and semantic\\nsegmentation are set up [141]. However, due to the conv layers\\nwith large receptive ﬁelds and pooling layers, blurry object\\nboundaries and coarse saliency maps are produced. Tang et\\nal. proposed a novel saliency detection framework (CRPSD)\\n[142], which combines region-level saliency estimation and\\npixel-level saliency prediction together with three closely\\nrelated CNNs. Li et al. proposed a deep contrast network\\nto combine segment-wise spatial pooling and pixel-level fully\\nconvolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiﬁcance for improving detection performance. Based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='convolutional streams [143].\\nThe proper integration of multi-scale feature maps is also\\nof signiﬁcance for improving detection performance. Based\\non Fast R-CNN, Wang et al. proposed the RegionNet by\\nperforming salient object detection with end-to-end edge pre-\\nserving and multi-scale contextual modelling [144]. Liu et al.\\n[27] proposed a multi-resolution convolutional neural network\\n(Mr-CNN) to predict eye ﬁxations, which is achieved by\\nlearning both bottom-up visual saliency and top-down visual\\nfactors from raw image data simultaneously. Cornia et al.\\nproposed an architecture which combines features extracted at\\ndifferent levels of the CNN [145]. Li et al. proposed a multi-\\nscale deep CNN framework to extract three scales of deep\\ncontrast features [146], namely the mean-subtracted region,\\nthe bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efﬁcient and accurate to train a direct pixel-wise'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='the bounding box of its immediate neighboring regions and\\nthe masked entire image, from each candidate region.\\nIt is efﬁcient and accurate to train a direct pixel-wise\\nCNN architecture to predict salient objects with the aids of\\nRNNs and deconvolution networks. Pan et al. formulated\\nsaliency prediction as a minimization optimization on the\\nEuclidean distance between the predicted saliency map and\\nthe ground truth and proposed two kinds of architectures\\n[147]: a shallow one trained from scratch and a deeper one\\nadapted from deconvoluted VGG network. As convolutional-\\ndeconvolution networks are not expert in recognizing objects\\nof multiple scales, Kuen et al. proposed a recurrent attentional\\nconvolutional-deconvolution network (RACDNN) with several\\nspatial transformer and recurrent network units to conquer\\nthis problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='this problem [148]. To fuse local, global and contextual\\ninformation of salient objects, Tang et al. developed a deeply-\\nsupervised recurrent convolutional neural network (DSRCNN)\\nto perform a full image-to-image saliency detection [149].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 13\\nB. Experimental Evaluation\\nFour representative datasets, including ECSSD [156], HKU-\\nIS [146], PASCALS [157], and SOD [158], are used to\\nevaluate several state-of-the-art methods. ECSSD consists of\\n1000 structurally complex but semantically meaningful natural\\nimages. HKU-IS is a large-scale dataset containing over 4000\\nchallenging images. Most of these images have more than\\none salient object and own low contrast. PASCALS is a\\nsubset chosen from the validation set of PASCAL VOC 2010\\nsegmentation dataset and is composed of 850 natural images.\\nThe SOD dataset possesses 300 images containing multiple\\nsalient objects. The training and validation sets for different\\ndatasets are kept the same as those in [152].\\nTwo standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Two standard metrics, namely F-measure and the mean\\nabsolute error (MAE), are utilized to evaluate the quality of a\\nsaliency map. Given precision and recall values pre-computed\\non the union of generated binary mask B and ground truth Z,\\nF-measure is deﬁned as below\\nFβ = (1 +β2)Presion ×Recall\\nβ2Presion + Recall (7)\\nwhere β2 is set to 0.3 in order to stress the importance of the\\nprecision value.\\nThe MAE score is computed with the following equation\\nMAE = 1\\nH×W\\nH∑\\ni=1\\nW∑\\nj=1\\n⏐⏐⏐ˆS(i,j) = ˆZ(i,j)\\n⏐⏐⏐ (8)\\nwhere ˆZ and ˆS represent the ground truth and the continuous\\nsaliency map, respectively. W and H are the width and\\nheight of the salient area, respectively. This score stresses\\nthe importance of successfully detected salient objects over\\ndetected non-salient pixels [159].\\nThe following approaches are evaluated: CHM [150], RC\\n[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\\nNLDF [154] and DSSC [155]. Among these methods, CHM,\\nRC and DRFI are classical ones with the best performance\\n[159], while the other methods are all associated with CNN.\\nF-measure and MAE scores are shown in Table VI.\\nFrom Table VI, we can ﬁnd that CNN based methods\\nperform better than classic methods. MC and MDF combine\\nthe information from local and global context to reach a\\nmore accurate saliency. ELD refers to low-level handcrafted\\nfeatures for complementary information. LEGS adopts generic\\nregion proposals to provide initial salient regions, which may\\nbe insufﬁcient for salient detection. DSR and MT act in\\ndifferent ways by introducing recurrent network and semantic\\nsegmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='segmentation, which provide insights for future improvements.\\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\\nrepresentations and superpixel segmentation, which provide\\nrobust salient regions and smooth boundaries. DCL, NLDF\\nand DSSC perform the best on these four datasets. DSSC\\nearns the best performance by modelling scale-to-scale short-\\nconnections.\\nOverall, as CNN mainly provides salient information in\\nlocal regions, most of CNN based methods need to model\\nvisual saliency along region boundaries with the aid of su-\\nperpixel segmentation. Meanwhile, the extraction of multi-\\nscale deep CNN features is of signiﬁcance for measuring local\\nconspicuity. Finally, it’s necessary to strengthen local con-\\nnections between different CNN layers and as well to utilize\\ncomplementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='complementary information from local and global context.\\nV. F ACE DETECTION\\nFace detection is essential to many face applications and acts\\nas an important pre-processing procedure to face recognition\\n[160]–[162], face synthesis [163], [164] and facial expression\\nanalysis [165]. Different from generic object detection, this\\ntask is to recognize and locate face regions covering a very\\nlarge range of scales (30-300 pts vs. 10-1000 pts). At the same\\ntime, faces have their unique object structural conﬁgurations\\n(e.g. the distribution of different face parts) and characteristics\\n(e.g. skin color). All these differences lead to special attention\\nto this task. However, large visual variations of faces, such as\\nocclusions, pose variations and illumination changes, impose\\ngreat challenges for this task in real applications.\\nThe most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiﬁers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The most famous face detector proposed by Viola and\\nJones [166] trains cascaded classiﬁers with Haar-Like features\\nand AdaBoost, achieving good performance with real-time\\nefﬁciency. However, this detector may degrade signiﬁcantly\\nin real-world applications due to larger visual variations of\\nhuman faces. Different from this cascade structure, Felzen-\\nszwalb et al. proposed a deformable part model (DPM) for face\\ndetection [24]. However, for these traditional face detection\\nmethods, high computational expenses and large quantities\\nof annotations are required to achieve a reasonable result.\\nBesides, their performance is greatly restricted by manually\\ndesigned features and shallow architecture.\\nA. Deep learning in Face Detection\\nRecently, some CNN based face detection approaches have\\nbeen proposed [167]–[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='been proposed [167]–[169].As less accurate localization re-\\nsults from independent regressions of object coordinates, Yu\\net al. [167] proposed a novel IoU loss function for predicting\\nthe four bounds of box jointly. Farfade et al. [168] proposed a\\nDeep Dense Face Detector (DDFD) to conduct multi-view face\\ndetection, which is able to detect faces in a wide range of ori-\\nentations without requirement of pose/landmark annotations.\\nYang et al. proposed a novel deep learning based face detection\\nframework [169], which collects the responses from local fa-\\ncial parts (e.g. eyes, nose and mouths) to address face detection\\nunder severe occlusions and unconstrained pose variations.\\nYang et al. [170] proposed a scale-friendly detection network\\nnamed ScaleFace, which splits a large range of target scales\\ninto smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='into smaller sub-ranges. Different specialized sub-networks are\\nconstructed on these sub-scales and combined into a single\\none to conduct end-to-end optimization. Hao et al. designed an\\nefﬁcient CNN to predict the scale distribution histogram of the\\nfaces and took this histogram to guide the zoom-in and zoom-\\nout of the image [171]. Since the faces are approximately\\nin uniform scale after zoom, compared with other state-of-\\nthe-art baselines, better performance is achieved with less\\ncomputation cost. Besides, some generic detection frameworks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 14\\nTABLE VI\\nCOMPARISON BETWEEN STATE OF THE ART METHODS .\\nDataset Metrics CHM [150] RC [151] DRFI [152] MC [138] MDF [146] LEGS [136] DSR [149] MTDNN [141] CRPSD [142] DCL [143] ELD [153] NLDF [154] DSSC [155]\\nPASCAL-S wFβ 0.631 0.640 0.679 0.721 0.764 0.756 0.697 0.818 0.776 0.822 0.767 0.831 0.830\\nMAE 0.222 0.225 0.221 0.147 0.145 0.157 0.128 0.170 0.063 0.108 0.121 0.099 0.080\\nECSSD wFβ 0.722 0.741 0.787 0.822 0.833 0.827 0.872 0.810 0.849 0.898 0.865 0.905 0.915\\nMAE 0.195 0.187 0.166 0.107 0.108 0.118 0.037 0.160 0.046 0.071 0.098 0.063 0.052\\nHKU-IS wFβ 0.728 0.726 0.783 0.781 0.860 0.770 0.833 - 0.821 0.907 0.844 0.902 0.913\\nMAE 0.158 0.165 0.143 0.098 0.129 0.118 0.040 - 0.043 0.048 0.071 0.048 0.039\\nSOD wFβ 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SOD wFβ 0.655 0.657 0.712 0.708 0.785 0.707 - 0.781 - 0.832 0.760 0.810 0.842\\nMAE 0.249 0.242 0.215 0.184 0.155 0.205 - 0.150 - 0.126 0.154 0.143 0.118\\n* The bigger wFβ is or the smaller MAE is, the better the performance is.\\nare extended to face detection with different modiﬁcations, e.g.\\nFaster R-CNN [29], [172], [173].\\nSome authors trained CNNs with other complementary\\ntasks, such as 3D modelling and face landmarks, in a multi-\\ntask learning manner. Huang et al. proposed a uniﬁed end-\\nto-end FCN framework called DenseBox to jointly conduct\\nface detection and landmark localization [174]. Li et al.\\n[175] proposed a multi-task discriminative learning framework\\nwhich integrates a ConvNet with a ﬁxed 3D mean face model\\nin an end-to-end manner. In the framework, two issues are\\naddressed to transfer from generic object detection to face\\ndetection, namely eliminating predeﬁned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='addressed to transfer from generic object detection to face\\ndetection, namely eliminating predeﬁned anchor boxes by a\\n3D mean face model and replacing RoI pooling layer with\\na conﬁguration pooling layer. Zhang et al. [176] proposed a\\ndeep cascaded multi-task framework named MTCNN which\\nexploits the inherent correlations between face detection and\\nalignment in unconstrained environment to boost up detection\\nperformance in a coarse-to-ﬁne manner.\\nReducing computational expenses is of necessity in real ap-\\nplications. To achieve real-time detection on mobile platform,\\nKalinovskii and Spitsyn proposed a new solution of frontal\\nface detection based on compact CNN cascades [177]. This\\nmethod takes a cascade of three simple CNNs to generate,\\nclassify and reﬁne candidate object positions progressively.\\nTo reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='To reduce the effects of large pose variations, Chen et al.\\nproposed a cascaded CNN denoted by Supervised Transformer\\nNetwork [31]. This network takes a multi-task RPN to predict\\ncandidate face regions along with associated facial landmarks\\nsimultaneously, and adopts a generic R-CNN to verify the\\nexistence of valid faces. Yang et al. proposed a three-stage\\ncascade structure based on FCNs [8], while in each stage, a\\nmulti-scale FCN is utilized to reﬁne the positions of possible\\nfaces. Qin et al. proposed a uniﬁed framework which achieves\\nbetter results with the complementary information from dif-\\nferent jointly trained CNNs [178].\\nB. Experimental Evaluation\\nThe FDDB [179] dataset has a total of 2,845 pictures in\\nwhich 5,171 faces are annotated with elliptical shape. Two\\ntypes of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reﬂect the dependence of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='types of evaluations are used: the discrete score and continuous\\nscore. By varying the threshold of the decision rule, the ROC\\ncurve for the discrete scores can reﬂect the dependence of\\nthe detected face fractions on the number of false alarms.\\nCompared with annotations, any detection with an IoU ratio\\nexceeding 0.5 is treated as positive. Each annotation is only\\nassociated with one detection. The ROC curve for the contin-\\nuous scores is the reﬂection of face localization quality.\\nThe evaluated models cover DDFD [168], CascadeCNN\\n[180], ACF-multiscale [181], Pico [182], HeadHunter [183],\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='NPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(a) Discrete ROC curves\\n 0\\n 0.1\\n 0.2\\n 0.3\\n 0.4\\n 0.5\\n 0.6\\n 0.7\\n 0.8\\n 0.9\\n 1\\n 0  500  1000  1500  2000\\nTrue positive rate\\nFalse positive\\nDDFD\\nCascadeCNN\\nACF-multiscale\\nPico\\nHeadHunter\\nJoint Cascade\\nSURF-multiview\\nViola-Jones\\nNPDFace\\nFaceness\\nCCF\\nMTCNN\\nConv3D\\nHyperface\\nUnitBox\\nLDCF+\\nDeepIR\\nHR-ER\\nFace-R-CNN\\nScaleFace\\n(b) Continuous ROC curves\\nFig. 11. The ROC curves of state-of-the-art methods on FDDB.\\nJoint Cascade [30], SURF-multiview [184], Viola-Jones [166],\\nNPDFace [185], Faceness [169], CCF [186], MTCNN [176],\\nConv3D [175], Hyperface [187], UnitBox [167], LDCF+ [S2],\\nDeepIR [173], HR-ER [188], Face-R-CNN [172] and Scale-\\nFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade,\\nSURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\\non classic hand-crafted features while the rest methods are\\nbased on deep CNN features. The ROC curves are shown in\\nFigure 11.\\nFrom Figure 11(a), in spite of relatively competitive results\\nproduced by LDCF+, it can be observed that most of classic\\nmethods perform with similar results and are outperformed\\nby CNN based methods by a signiﬁcant margin. From Figure\\n11(b), it can be observed that most of CNN based methods\\nearn similar true positive rates between 60% and 70% while\\nDeepIR and HR-ER perform much better than them. Among\\nclassic methods, Joint Cascade is still competitive. As earlier\\nworks, DDFD and CCF directly make use of generated feature\\nmaps and obtain relatively poor results. CascadeCNN builds\\ncascaded CNNs to locate face regions, which is efﬁcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='cascaded CNNs to locate face regions, which is efﬁcient but in-\\naccurate. Faceness combines the decisions from different part\\ndetectors, resulting in precise face localizations while being\\ntime-consuming. The outstanding performance of MTCNN,\\nConv3D and Hyperface proves the effectiveness of multi-task\\nlearning. HR-ER and ScaleFace adaptively detect faces of\\ndifferent scales, and make a balance between accuracy and\\nefﬁciency. DeepIR and Face-R-CNN are two extensions of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 15\\nFaster R-CNN architecture to face detection, which validate\\nthe signiﬁcance and effectiveness of Faster R-CNN. Unitbox\\nprovides an alternative choice for performance improvements\\nby carefully designing optimization loss.\\nFrom these results, we can draw the conclusion that\\nCNN based methods are in the leading position. The perfor-\\nmance can be improved by the following strategies: designing\\nnovel optimization loss, modifying generic detection pipelines,\\nbuilding meaningful network cascades, adapting scale-aware\\ndetection and learning multi-task shared CNN features.\\nVI. P EDESTRIAN DETECTION\\nRecently, pedestrian detection has been intensively studied,\\nwhich has a close relationship to pedestrian tracking [189],\\n[190], person re-identiﬁcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='which has a close relationship to pedestrian tracking [189],\\n[190], person re-identiﬁcation [191], [192] and robot naviga-\\ntion [193], [194]. Prior to the recent progress in DCNN based\\nmethods [195], [196], some researchers combined boosted\\ndecision forests with hand-crafted features to obtain pedestrian\\ndetectors [197]–[199]. At the same time, to explicitly model\\nthe deformation and occlusion, part-based models [200] and\\nexplicit occlusion handling [201], [202] are of concern.\\nAs there are many pedestrian instances of small sizes\\nin typical scenarios of pedestrian detection (e.g. automatic\\ndriving and intelligent surveillance), the application of RoI\\npooling layer in generic object detection pipeline may result\\nin ‘plain’ features due to collapsing bins. In the meantime, the\\nmain source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='main source of false predictions in pedestrian detection is the\\nconfusion of hard background instances, which is in contrast\\nto the interference from multiple categories in generic object\\ndetection. As a result, different conﬁgurations and components\\nare required to accomplish accurate pedestrian detection.\\nA. Deep learning in Pedestrian Detection\\nAlthough DCNNs have obtained excellent performance on\\ngeneric object detection [16], [72], none of these approaches\\nhave achieved better results than the best hand-crafted feature\\nbased method [198] for a long time, even when part-based\\ninformation and occlusion handling are incorporated [202].\\nThereby, some researches have been conducted to analyze the\\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\\n[18] to pedestrian detection [203]. They modiﬁed the down-\\nstream classiﬁer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[18] to pedestrian detection [203]. They modiﬁed the down-\\nstream classiﬁer by adding boosted forests to shared, high-\\nresolution conv feature maps and taking a RPN to handle small\\ninstances and hard negative examples. To deal with complex\\nocclusions existing in pedestrian images, inspired by DPM\\n[24], Tian et al. proposed a deep learning framework called\\nDeepParts [204], which makes decisions based an ensemble of\\nextensive part detectors. DeepParts has advantages in dealing\\nwith weakly labeled data, low IoU positive proposals and\\npartial occlusion.\\nOther researchers also tried to combine complementary in-\\nformation from multiple data sources. CompACT-Deep adopts\\na complexity-aware cascade to combine hand-crafted features\\nand ﬁne-tuned DCNNs [195]. Based on Faster R-CNN, Liu et\\nal. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='al. proposed multi-spectral deep neural networks for pedestrian\\ndetection to combine complementary information from color\\nand thermal images [205]. Tian et al. [206] proposed a task-\\nassistant CNN (TA-CNN) to jointly learn multiple tasks with\\nTABLE VII\\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF\\nSTATE-OF-THE -ART MODELS ON CALTECH PEDESTRIAN DATASET . ALL\\nNUMBERS ARE REPORTED IN L-AMR.\\nMethod Reasonable All Far Medium Near none partial heavy\\nCheckerboards+ [198] 17.1 68.4 100 58.3 5.1 15.6 31.4 78.4\\nLDCF++[S2] 15.2 67.1 100 58.4 5.4 13.3 33.3 76.2\\nSCF+AlexNet [210] 23.3 70.3 100 62.3 10.2 20.0 48.5 74.7\\nSA-FastRCNN [211] 9.7 62.6 100 51.8 0 7.7 24.8 64.3\\nMS-CNN [105] 10.0 61.0 97.2 49.1 2.6 8.2 19.2 60.0\\nDeepParts [204] 11.9 64.8 100 56.4 4.8 10.6 19.9 60.4\\nCompACT-Deep [195] 11.8 64.4 100 53.2 4.0 9.6 25.1 65.8\\nRPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='RPN+BF [203] 9.6 64.7 100 53.9 2.3 7.7 24.2 74.2\\nF-DNN+SS [207] 8.2 50.3 77.5 33.2 2.8 6.7 15.1 53.4\\nmultiple data sources and to combine pedestrian attributes\\nwith semantic scene attributes together. Du et al. proposed\\na deep neural network fusion architecture for fast and robust\\npedestrian detection [207]. Based on the candidate bounding\\nboxes generated with SSD detectors [71], multiple binary\\nclassiﬁers are processed parallelly to conduct soft-rejection\\nbased network fusion (SNF) by consulting their aggregated\\ndegree of conﬁdences.\\nHowever, most of these approaches are much more sophisti-\\ncated than the standard R-CNN framework. CompACT-Deep\\nconsists of a variety of hand-crafted features, a small CNN\\nmodel and a large VGG16 model [195]. DeepParts contains\\n45 ﬁne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiﬁcation and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='45 ﬁne-tuned DCNN models, and a set of strategies, including\\nbounding box shifting handling and part selection, are required\\nto arrive at the reported results [204]. So the modiﬁcation and\\nsimpliﬁcation is of signiﬁcance to reduce the burden on both\\nsoftware and hardware to satisfy real-time detection demand.\\nTome et al. proposed a novel solution to adapt generic object\\ndetection pipeline to pedestrian detection by optimizing most\\nof its stages [59]. Hu et al. [208] trained an ensemble of\\nboosted decision models by reusing the conv feature maps, and\\na further improvement was gained with simple pixel labelling\\nand additional complementary hand-crafted features. Tome\\net al. [209] proposed a reduced memory region based deep\\nCNN architecture, which fuses regional responses from both\\nACF detectors and SVM classiﬁers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='ACF detectors and SVM classiﬁers into R-CNN. Ribeiro et\\nal. addressed the problem of Human-Aware Navigation [32]\\nand proposed a vision-based person tracking system guided\\nby multiple camera sensors.\\nB. Experimental Evaluation\\nThe evaluation is conducted on the most popular Caltech\\nPedestrian dataset [3]. The dataset was collected from the\\nvideos of a vehicle driving through an urban environment\\nand consists of 250,000 frames with about 2300 unique\\npedestrians and 350,000 annotated bounding boxes (BBs).\\nThree kinds of labels, namely ‘Person (clear identiﬁcations)’,\\n‘Person? (unclear identiﬁcations)’ and ‘People (large group of\\nindividuals)’, are assigned to different BBs. The performance\\nis measured with the log-average miss rate (L-AMR) which\\nis computed evenly spaced in log-space in the range 10−2 to\\n1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1 by averaging miss rate at the rate of nine false positives\\nper image (FPPI) [3]. According to the differences in the\\nheight and visible part of the BBs, a total of 9 popular settings\\nare adopted to evaluate different properties of these models.\\nDetails of these settings are as [3].\\nEvaluated methods include Checkerboards+ [198], LDCF++\\n[S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 16\\n[105], DeepParts [204], CompACT-Deep [195], RPN+BF\\n[203] and F-DNN+SS [207]. The ﬁrst two methods are based\\non hand-crafted features while the rest ones rely on deep CNN\\nfeatures. All results are exhibited in Table VII. From this table,\\nwe observe that different from other tasks, classic handcrafted\\nfeatures can still earn competitive results with boosted decision\\nforests [203], ACF [197] and HOG+LUV channels [S2]. As\\nan early attempt to adapt CNN to pedestrian detection, the\\nfeatures generated by SCF+AlexNet are not so discriminant\\nand produce relatively poor results. Based on multiple CNNs,\\nDeepParts and CompACT-Deep accomplish detection tasks via\\ndifferent strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='different strategies, namely local part integration and cascade\\nnetwork. The responses from different local part detectors\\nmake DeepParts robust to partial occlusions. However, due to\\ncomplexity, it is too time-consuming to achieve real-time de-\\ntection. The multi-scale representation of MS-CNN improves\\naccuracy of pedestrian locations. SA-FastRCNN extends Fast\\nR-CNN to automatically detecting pedestrians according to\\ntheir different scales, which has trouble when there are partial\\nocclusions. RPN+BF combines the detectors produced by\\nFaster R-CNN with boosting decision forest to accurately\\nlocate different pedestrians. F-DNN+SS, which is composed\\nof multiple parallel classiﬁers with soft rejections, performs\\nthe best followed by RPN+BF, SA-FastRCNN and MS-CNN.\\nIn short, CNN based methods can provide more accurate\\ncandidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='candidate boxes and multi-level semantic information for\\nidentifying and locating pedestrians. Meanwhile, handcrafted\\nfeatures are complementary and can be combined with CNN\\nto achieve better results. The improvements over existing CNN\\nmethods can be obtained by carefully designing the framework\\nand classiﬁers, extracting multi-scale and part based semantic\\ninformation and searching for complementary information\\nfrom other related tasks, such as segmentation.\\nVII. P ROMISING FUTURE DIRECTIONS AND TASKS\\nIn spite of rapid development and achieved promising\\nprogress of object detection, there are still many open issues\\nfor future work.\\nThe ﬁrst one is small object detection such as occurring\\nin COCO dataset and in face detection task. To improve\\nlocalization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n• Multi-task joint optimization and multi-modal infor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='localization accuracy on small objects under partial occlusions,\\nit is necessary to modify network architectures from the\\nfollowing aspects.\\n• Multi-task joint optimization and multi-modal infor-\\nmation fusion. Due to the correlations between different\\ntasks within and outside object detection, multi-task joint\\noptimization has already been studied by many researchers\\n[16] [18]. However, apart from the tasks mentioned in\\nSubs. III-A8, it is desirable to think over the characteristics\\nof different sub-tasks of object detection (e.g. superpixel\\nsemantic segmentation in salient object detection) and ex-\\ntend multi-task optimization to other applications such as\\ninstance segmentation [66], multi-object tracking [202] and\\nmulti-person pose estimation [S4]. Besides, given a speciﬁc\\napplication, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='application, the information from different modalities, such\\nas text [212], thermal data [205] and images [65], can be\\nfused together to achieve a more discriminant network.\\n•Scale adaption. Objects usually exist in different scales,\\nwhich is more apparent in face detection and pedestrian\\ndetection. To increase the robustness to scale changes, it\\nis demanded to train scale-invariant, multi-scale or scale-\\nadaptive detectors. For scale-invariant detectors, more pow-\\nerful backbone architectures (e.g. ResNext [123]), negative\\nsample mining [113], reverse connection [213] and sub-\\ncategory modelling [60] are all beneﬁcial. For multi-scale\\ndetectors, both the FPN [66] which produces multi-scale\\nfeature maps and Generative Adversarial Network [214]\\nwhich narrows representation differences between small ob-\\njects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='jects and the large ones with a low-cost architecture provide\\ninsights into generating meaningful feature pyramid. For\\nscale-adaptive detectors, it is useful to combine knowledge\\ngraph [215], attentional mechanism [216], cascade network\\n[180] and scale distribution estimation [171] to detect ob-\\njects adaptively.\\n•Spatial correlations and contextual modelling. Spatial\\ndistribution plays an important role in object detection. So\\nregion proposal generation and grid regression are taken\\nto obtain probable object locations. However, the corre-\\nlations between multiple proposals and object categories\\nare ignored. Besides, the global structure information is\\nabandoned by the position-sensitive score maps in R-FCN.\\nTo solve these problems, we can refer to diverse subset\\nselection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='selection [217] and sequential reasoning tasks [218] for\\npossible solutions. It is also meaningful to mask salient parts\\nand couple them with the global structure in a joint-learning\\nmanner [219].\\nThe second one is to release the burden on manual labor and\\naccomplish real-time object detection, with the emergence of\\nlarge-scale image and video data. The following three aspects\\ncan be taken into account.\\n•Cascade network. In a cascade network, a cascade of\\ndetectors are built in different stages or layers [180], [220].\\nAnd easily distinguishable examples are rejected at shallow\\nlayers so that features and classiﬁers at latter stages can\\nhandle more difﬁcult samples with the aid of the decisions\\nfrom previous stages. However, current cascades are built in\\na greedy manner, where previous stages in cascade are ﬁxed\\nwhen training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='when training a new stage. So the optimizations of different\\nCNNs are isolated, which stresses the necessity of end-to-\\nend optimization for CNN cascade. At the same time, it\\nis also a matter of concern to build contextual associated\\ncascade networks with existing layers.\\n• Unsupervised and weakly supervised learning. It’s\\nvery time consuming to manually draw large quantities\\nof bounding boxes. To release this burden, semantic prior\\n[55], unsupervised object discovery [221], multiple instance\\nlearning [222] and deep neural network prediction [47] can\\nbe integrated to make best use of image-level supervision to\\nassign object category tags to corresponding object regions\\nand reﬁne object boundaries. Furthermore, weakly annota-\\ntions (e.g. center-click annotations [223]) are also helpful\\nfor achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n• Network optimization. Given speciﬁc applications and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='for achieving high-quality detectors with modest annotation\\nefforts, especially aided by the mobile platform.\\n• Network optimization. Given speciﬁc applications and\\nplatforms, it is signiﬁcant to make a balance among speed,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 17\\nmemory and accuracy by selecting an optimal detection\\narchitecture [116], [224]. However, despite that detection\\naccuracy is reduced, it is more meaningful to learn compact\\nmodels with fewer number of parameters [209]. And this\\nsituation can be relieved by introducing better pre-training\\nschemes [225], knowledge distillation [226] and hint learn-\\ning [227]. DSOD also provides a promising guideline to\\ntrain from scratch to bridge the gap between different image\\nsources and tasks [74].\\nThe third one is to extend typical methods for 2D object de-\\ntection to adapt 3D object detection and video object detection,\\nwith the requirements from autonomous driving, intelligent\\ntransportation and intelligent surveillance.\\n•3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='transportation and intelligent surveillance.\\n•3D object detection. With the applications of 3D sensors\\n(e.g. LIDAR and camera), additional depth information can\\nbe utilized to better understand the images in 2D and extend\\nthe image-level knowledge to the real world. However,\\nseldom of these 3D-aware techniques aim to place correct\\n3D bounding boxes around detected objects. To achieve\\nbetter bounding results, multi-view representation [181] and\\n3D proposal network [228] may provide some guidelines to\\nencode depth information with the aid of inertial sensors\\n(accelerometer and gyrometer) [229].\\n• Video object detection. Temporal information across\\ndifferent frames play an important role in understanding\\nthe behaviors of different objects. However, the accuracy\\nsuffers from degenerated object appearances (e.g., motion\\nblur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical ﬂow [199] and LSTM [107] should'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='blur and video defocus) in videos and the network is\\nusually not trained end-to-end. To this end, spatiotemporal\\ntubelets [230], optical ﬂow [199] and LSTM [107] should\\nbe considered to fundamentally model object associations\\nbetween consecutive frames.\\nVIII. C ONCLUSION\\nDue to its powerful learning ability and advantages in\\ndealing with occlusion, scale transformation and background\\nswitches, deep learning based object detection has been a\\nresearch hotspot in recent years. This paper provides a detailed\\nreview on deep learning based object detection frameworks\\nwhich handle different sub-problems, such as occlusion, clutter\\nand low resolution, with different degrees of modiﬁcations\\non R-CNN. The review starts on generic object detection\\npipelines which provide base architectures for other related\\ntasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieﬂy reviewed. Finally, we propose several promising future'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='tasks. Then, three other common tasks, namely salient object\\ndetection, face detection and pedestrian detection, are also\\nbrieﬂy reviewed. Finally, we propose several promising future\\ndirections to gain a thorough understanding of the object\\ndetection landscape. This review is also meaningful for the\\ndevelopments in neural networks and related learning systems,\\nwhich provides valuable insights and guidelines for future\\nprogress.\\nACKNOWLEDGMENTS\\nThis research was supported by the National Natural Sci-\\nence Foundation of China (No.61672203 & 61375047 &\\n91746209), the National Key Research and Development Pro-\\ngram of China (2016YFB1000901), and Anhui Natural Sci-\\nence Funds for Distinguished Young Scholar (No.170808J08).\\nREFERENCES\\n[1] P. F. Felzenszwalb, R. B. Girshick, D. Mcallester, and D. Ramanan,\\n“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 9, p. 1627, 2010.\\n[2] K. K. Sung and T. Poggio, “Example-based learning for view-based\\nhuman face detection,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\\nno. 1, pp. 39–51, 2002.\\n[3] C. Wojek, P. Dollar, B. Schiele, and P. Perona, “Pedestrian detection:\\nAn evaluation of the state of the art,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 34, no. 4, p. 743, 2012.\\n[4] H. Kobatake and Y . Yoshinaga, “Detection of spicules on mammogram\\nbased on skeleton analysis.” IEEE Trans. Med. Imag. , vol. 15, no. 3,\\npp. 235–245, 1996.\\n[5] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for\\nfast feature embedding,” in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in NIPS, 2012.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='fast feature embedding,” in ACM MM, 2014.\\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in NIPS, 2012.\\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh, “Realtime multi-person\\n2d pose estimation using part afﬁnity ﬁelds,” in CVPR, 2017.\\n[8] Z. Yang and R. Nevatia, “A multi-scale cascade fully convolutional\\nnetwork face detector,” in ICPR, 2016.\\n[9] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, “Deepdriving:\\nLearning affordance for direct perception in autonomous driving,” in\\nICCV, 2015.\\n[10] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object\\ndetection network for autonomous driving,” in CVPR, 2017.\\n[11] A. Dundar, J. Jin, B. Martini, and E. Culurciello, “Embedded streaming\\ndeep neural networks accelerator with applications,” IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572–1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, “Low-complexity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Neural Netw. & Learning Syst. , vol. 28, no. 7, pp. 1572–1583, 2017.\\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, “Low-complexity\\napproximate convolutional neural networks,”IEEE Trans. Neural Netw.\\n& Learning Syst. , vol. PP, no. 99, pp. 1–12, 2018.\\n[13] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and R. Togneri,\\n“Cost-sensitive learning of deep feature representations from imbal-\\nanced data.” IEEE Trans. Neural Netw. & Learning Syst. , vol. PP,\\nno. 99, pp. 1–15, 2017.\\n[14] A. Stuhlsatz, J. Lippel, and T. Zielke, “Feature extraction with deep\\nneural networks by a generalized discriminant analysis.” IEEE Trans.\\nNeural Netw. & Learning Syst. , vol. 23, no. 4, pp. 596–608, 2012.\\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature\\nhierarchies for accurate object detection and semantic segmentation,”\\nin CVPR, 2014.\\n[16] R. Girshick, “Fast r-cnn,” in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='in CVPR, 2014.\\n[16] R. Girshick, “Fast r-cnn,” in ICCV, 2015.\\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look\\nonce: Uniﬁed, real-time object detection,” in CVPR, 2016.\\n[18] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-\\ntime object detection with region proposal networks,” in NIPS, 2015,\\npp. 91–99.\\n[19] D. G. Lowe, “Distinctive image features from scale-invariant key-\\npoints,” Int. J. of Comput. Vision , vol. 60, no. 2, pp. 91–110, 2004.\\n[20] N. Dalal and B. Triggs, “Histograms of oriented gradients for human\\ndetection,” in CVPR, 2005.\\n[21] R. Lienhart and J. Maydt, “An extended set of haar-like features for\\nrapid object detection,” in ICIP, 2002.\\n[22] C. Cortes and V . Vapnik, “Support vector machine,”Machine Learning,\\nvol. 20, no. 3, pp. 273–297, 1995.\\n[23] Y . Freund and R. E. Schapire, “A desicion-theoretic generalization of\\non-line learning and an application to boosting,” J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663–671, 1997.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[23] Y . Freund and R. E. Schapire, “A desicion-theoretic generalization of\\non-line learning and an application to boosting,” J. of Comput. & Sys.\\nSci., vol. 13, no. 5, pp. 663–671, 1997.\\n[24] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\\n“Object detection with discriminatively trained part-based models,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1627–1645, 2010.\\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman, “The pascal visual object classes challenge 2007 (voc 2007)\\nresults (2007),” 2008.\\n[26] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature, vol.\\n521, no. 7553, pp. 436–444, 2015.\\n[27] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu, “Predicting eye ﬁxations\\nusing convolutional neural networks,” in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, “Large-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,” in CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='using convolutional neural networks,” in CVPR, 2015.\\n[28] E. Vig, M. Dorr, and D. Cox, “Large-scale optimization of hierarchical\\nfeatures for saliency prediction in natural images,” in CVPR, 2014.\\n[29] H. Jiang and E. Learned-Miller, “Face detection with the faster r-cnn,”\\nin FG, 2017.\\n[30] D. Chen, S. Ren, Y . Wei, X. Cao, and J. Sun, “Joint cascade face\\ndetection and alignment,” in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 18\\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, “Supervised transformer network\\nfor efﬁcient face detection,” in ECCV, 2016.\\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, “A real-time\\npedestrian detector using deep learning for human-aware navigation,”\\narXiv:1607.04441, 2016.\\n[33] F. Yang, W. Choi, and Y . Lin, “Exploit all the layers: Fast and accurate\\ncnn object detector with scale dependent pooling and cascaded rejection\\nclassiﬁers,” in CVPR, 2016.\\n[34] P. Druzhkov and V . Kustikova, “A survey of deep learning methods and\\nsoftware tools for image classiﬁcation and object detection,” Pattern\\nRecognition and Image Anal. , vol. 26, no. 1, p. 9, 2016.\\n[35] W. Pitts and W. S. McCulloch, “How we know universals the perception\\nof auditory and visual forms,”The Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127–147, 1947.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[35] W. Pitts and W. S. McCulloch, “How we know universals the perception\\nof auditory and visual forms,”The Bulletin of Mathematical Biophysics,\\nvol. 9, no. 3, pp. 127–147, 1947.\\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal\\nrepresentation by back-propagation of errors,” Nature, vol. 323, no.\\n323, pp. 533–536, 1986.\\n[37] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality\\nof data with neural networks,” Sci., vol. 313, pp. 504–507, 2006.\\n[38] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\\nA. Senior, V . Vanhoucke, P. Nguyen, T. N. Sainathet al., “Deep neural\\nnetworks for acoustic modeling in speech recognition: The shared\\nviews of four research groups,” IEEE Signal Process. Mag. , vol. 29,\\nno. 6, pp. 82–97, 2012.\\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\\nA large-scale hierarchical image database,” in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='A large-scale hierarchical image database,” in CVPR, 2009.\\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\\nG. Hinton, “Binary coding of speech spectrograms using a deep auto-\\nencoder,” in INTERSPEECH, 2010.\\n[41] G. Dahl, A.-r. Mohamed, G. E. Hinton et al., “Phone recognition with\\nthe mean-covariance restricted boltzmann machine,” in NIPS, 2010.\\n[42] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov, “Improving neural networks by preventing co-\\nadaptation of feature detectors,” arXiv:1207.0580, 2012.\\n[43] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,” in ICML, 2015.\\n[44] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . LeCun,\\n“Overfeat: Integrated recognition, localization and detection using\\nconvolutional networks,” arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='“Overfeat: Integrated recognition, localization and detection using\\nconvolutional networks,” arXiv:1312.6229, 2013.\\n[45] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich, “Going deeper with\\nconvolutions,” in CVPR, 2015.\\n[46] K. Simonyan and A. Zisserman, “Very deep convolutional networks\\nfor large-scale image recognition,” arXiv:1409.1556, 2014.\\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in CVPR, 2016.\\n[48] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\\nboltzmann machines,” in ICML, 2010.\\n[49] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al. , “Weakly supervised\\nobject recognition with convolutional neural networks,” in NIPS, 2014.\\n[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\\nmid-level image representations using convolutional neural networks,”\\nin CVPR, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\\nmid-level image representations using convolutional neural networks,”\\nin CVPR, 2014.\\n[51] F. M. Wadley, “Probit analysis: a statistical treatment of the sigmoid\\nresponse curve,” Annals of the Entomological Soc. of America , vol. 67,\\nno. 4, pp. 549–553, 1947.\\n[52] K. Kavukcuoglu, R. Fergus, Y . LeCun et al. , “Learning invariant\\nfeatures through topographic ﬁlter maps,” in CVPR, 2009.\\n[53] K. Kavukcuoglu, P. Sermanet, Y .-L. Boureau, K. Gregor, M. Mathieu,\\nand Y . LeCun, “Learning convolutional feature hierarchies for visual\\nrecognition,” in NIPS, 2010.\\n[54] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, “Deconvolu-\\ntional networks,” in CVPR, 2010.\\n[55] H. Noh, S. Hong, and B. Han, “Learning deconvolution network for\\nsemantic segmentation,” in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, “Plant leaf iden-\\ntiﬁcation via a growing convolution neural network with progressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='semantic segmentation,” in ICCV, 2015.\\n[56] Z.-Q. Zhao, B.-J. Xie, Y .-m. Cheung, and X. Wu, “Plant leaf iden-\\ntiﬁcation via a growing convolution neural network with progressive\\nsample learning,” in ACCV, 2014.\\n[57] A. Babenko, A. Slesarev, A. Chigorin, and V . Lempitsky, “Neural codes\\nfor image retrieval,” in ECCV, 2014.\\n[58] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y . Zhang, and J. Li,\\n“Deep learning for content-based image retrieval: A comprehensive\\nstudy,” in ACM MM, 2014.\\n[59] D. Tom `e, F. Monti, L. Barofﬁo, L. Bondi, M. Tagliasacchi, and\\nS. Tubaro, “Deep convolutional neural networks for pedestrian detec-\\ntion,” Signal Process.: Image Commun. , vol. 47, pp. 482–489, 2016.\\n[60] Y . Xiang, W. Choi, Y . Lin, and S. Savarese, “Subcategory-aware\\nconvolutional neural networks for object proposals and detection,” in\\nWACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, “Pedestrian\\ndetection based on fast r-cnn and batch normalization,” in ICIC, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='WACV, 2017.\\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, “Pedestrian\\ndetection based on fast r-cnn and batch normalization,” in ICIC, 2017.\\n[62] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y . Ng,\\n“Multimodal deep learning,” in ICML, 2011.\\n[63] Z. Wu, X. Wang, Y .-G. Jiang, H. Ye, and X. Xue, “Modeling spatial-\\ntemporal clues in a hybrid deep learning framework for video classiﬁ-\\ncation,” in ACM MM, 2015.\\n[64] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\\nconvolutional networks for visual recognition,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 9, pp. 1904–1916, 2015.\\n[65] Y . Li, K. He, J. Sun et al., “R-fcn: Object detection via region-based\\nfully convolutional networks,” in NIPS, 2016, pp. 379–387.\\n[66] T.-Y . Lin, P. Doll ´ar, R. B. Girshick, K. He, B. Hariharan, and S. J.\\nBelongie, “Feature pyramid networks for object detection,” in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll ´ar, and R. B. Girshick, “Mask r-cnn,” in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Belongie, “Feature pyramid networks for object detection,” in CVPR,\\n2017.\\n[67] K. He, G. Gkioxari, P. Doll ´ar, and R. B. Girshick, “Mask r-cnn,” in\\nICCV, 2017.\\n[68] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable object\\ndetection using deep neural networks,” in CVPR, 2014.\\n[69] D. Yoo, S. Park, J.-Y . Lee, A. S. Paek, and I. So Kweon, “Attentionnet:\\nAggregating weak directions for accurate object detection,” in CVPR,\\n2015.\\n[70] M. Najibi, M. Rastegari, and L. S. Davis, “G-cnn: an iterative grid\\nbased object detector,” in CVPR, 2016.\\n[71] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and\\nA. C. Berg, “Ssd: Single shot multibox detector,” in ECCV, 2016.\\n[72] J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,”\\narXiv:1612.08242, 2016.\\n[73] C. Y . Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “Dssd:\\nDeconvolutional single shot detector,” arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, “Dsod:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Deconvolutional single shot detector,” arXiv:1701.06659, 2017.\\n[74] Z. Shen, Z. Liu, J. Li, Y . G. Jiang, Y . Chen, and X. Xue, “Dsod:\\nLearning deeply supervised object detectors from scratch,” in ICCV,\\n2017.\\n[75] G. E. Hinton, A. Krizhevsky, and S. D. Wang, “Transforming auto-\\nencoders,” in ICANN, 2011.\\n[76] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus, “Learning invariance\\nthrough imitation,” in CVPR, 2011.\\n[77] X. Ren and D. Ramanan, “Histograms of sparse codes for object\\ndetection,” in CVPR, 2013.\\n[78] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\\n“Selective search for object recognition,” Int. J. of Comput. Vision, vol.\\n104, no. 2, pp. 154–171, 2013.\\n[79] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun, “Pedestrian\\ndetection with unsupervised multi-stage feature learning,” in CVPR,\\n2013.\\n[80] P. Kr ¨ahenb¨uhl and V . Koltun, “Geodesic object proposals,” in ECCV,\\n2014.\\n[81] P. Arbel ´aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2013.\\n[80] P. Kr ¨ahenb¨uhl and V . Koltun, “Geodesic object proposals,” in ECCV,\\n2014.\\n[81] P. Arbel ´aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\\n“Multiscale combinatorial grouping,” in CVPR, 2014.\\n[82] C. L. Zitnick and P. Doll ´ar, “Edge boxes: Locating object proposals\\nfrom edges,” in ECCV, 2014.\\n[83] W. Kuo, B. Hariharan, and J. Malik, “Deepbox: Learning objectness\\nwith convolutional networks,” in ICCV, 2015.\\n[84] P. O. Pinheiro, T.-Y . Lin, R. Collobert, and P. Doll ´ar, “Learning to\\nreﬁne object segments,” in ECCV, 2016.\\n[85] Y . Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, “Improving object\\ndetection with deep convolutional networks via bayesian optimization\\nand structured prediction,” in CVPR, 2015.\\n[86] S. Gupta, R. Girshick, P. Arbel ´aez, and J. Malik, “Learning rich features\\nfrom rgb-d images for object detection and segmentation,” in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='from rgb-d images for object detection and segmentation,” in ECCV,\\n2014.\\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y . Tian, H. Li, S. Yang,\\nZ. Wang, C.-C. Loy et al., “Deepid-net: Deformable deep convolutional\\nneural networks for object detection,” in CVPR, 2015.\\n[88] K. Lenc and A. Vedaldi, “R-cnn minus r,” arXiv:1506.06981, 2015.\\n[89] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features:\\nSpatial pyramid matching for recognizing natural scene categories,”\\nin CVPR, 2006.\\n[90] F. Perronnin, J. S ´anchez, and T. Mensink, “Improving the ﬁsher kernel\\nfor large-scale image classiﬁcation,” in ECCV, 2010.\\n[91] J. Xue, J. Li, and Y . Gong, “Restructuring of deep neural network\\nacoustic models with singular value decomposition.” in Interspeech,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 19\\n[92] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\\nobject detection with region proposal networks,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 39, no. 6, pp. 1137–1149, 2017.\\n[93] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-\\ning the inception architecture for computer vision,” in CVPR, 2016.\\n[94] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\\ncontext,” in ECCV, 2014.\\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, “Inside-outside\\nnet: Detecting objects in context with skip pooling and recurrent neural\\nnetworks,” in CVPR, 2016.\\n[96] A. Arnab and P. H. S. Torr, “Pixelwise instance segmentation with a\\ndynamically instantiated network,” in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[96] A. Arnab and P. H. S. Torr, “Pixelwise instance segmentation with a\\ndynamically instantiated network,” in CVPR, 2017.\\n[97] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via\\nmulti-task network cascades,” in CVPR, 2016.\\n[98] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei, “Fully convolutional instance-\\naware semantic segmentation,” in CVPR, 2017.\\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\\n“Spatial transformer networks,” in CVPR, 2015.\\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, “Stuffnet: Using stuffto\\nimprove object detection,” in WACV, 2017.\\n[101] T. Kong, A. Yao, Y . Chen, and F. Sun, “Hypernet: Towards accurate\\nregion proposal generation and joint object detection,” in CVPR, 2016.\\n[102] A. Pentina, V . Sharmanska, and C. H. Lampert, “Curriculum learning\\nof multiple tasks,” in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, “Rotating your\\nface using multi-task deep neural network,” in CVPR, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='of multiple tasks,” in CVPR, 2015.\\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, “Rotating your\\nface using multi-task deep neural network,” in CVPR, 2015.\\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, “Multi-stage object\\ndetection with group recursive learning,” arXiv:1608.05159, 2016.\\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, “A uniﬁed multi-scale\\ndeep convolutional neural network for fast object detection,” in ECCV,\\n2016.\\n[106] Y . Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, “segdeepm:\\nExploiting segmentation and context in deep neural networks for object\\ndetection,” in CVPR, 2015.\\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, “Scene labeling\\nwith lstm recurrent neural networks,” in CVPR, 2015.\\n[108] B. Moysset, C. Kermorvant, and C. Wolf, “Learning to detect and\\nlocalize many objects from few examples,” arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, “Gated bi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='localize many objects from few examples,” arXiv:1611.05664, 2016.\\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, “Gated bi-\\ndirectional cnn for object detection,” in ECCV, 2016.\\n[110] S. Gidaris and N. Komodakis, “Object detection via a multi-region and\\nsemantic segmentation-aware cnn model,” in CVPR, 2015.\\n[111] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural net-\\nworks,” IEEE Trans. Signal Process. , vol. 45, pp. 2673–2681, 1997.\\n[112] S. Zagoruyko, A. Lerer, T.-Y . Lin, P. O. Pinheiro, S. Gross, S. Chin-\\ntala, and P. Doll ´ar, “A multipath network for object detection,”\\narXiv:1604.02135, 2016.\\n[113] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based\\nobject detectors with online hard example mining,” in CVPR, 2016.\\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object detection\\nnetworks on convolutional feature maps,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476–1481, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, “Object detection\\nnetworks on convolutional feature maps,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 39, no. 7, pp. 1476–1481, 2017.\\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, “Factors in ﬁnetuning\\ndeep model for object detection with long-tail distribution,” in CVPR,\\n2016.\\n[116] S. Hong, B. Roh, K.-H. Kim, Y . Cheon, and M. Park, “Pvanet:\\nLightweight deep neural networks for real-time object detection,”\\narXiv:1611.08588, 2016.\\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, “Understanding and\\nimproving convolutional neural networks via concatenated rectiﬁed\\nlinear units,” in ICML, 2016.\\n[118] C. Szegedy, A. Toshev, and D. Erhan, “Deep neural networks for object\\ndetection,” in NIPS, 2013.\\n[119] P. O. Pinheiro, R. Collobert, and P. Doll ´ar, “Learning to segment object\\ncandidates,” in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, “Scalable,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[119] P. O. Pinheiro, R. Collobert, and P. Doll ´ar, “Learning to segment object\\ncandidates,” in NIPS, 2015.\\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, “Scalable,\\nhigh-quality object detection,” arXiv:1412.1441, 2014.\\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\\n“The pascal visual object classes challenge 2012 (voc2012) results\\n(2012),” in http://www.pascal-network.org/challenges/VOC/voc2011/\\nworkshop/index.html, 2011.\\n[122] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-\\ntional networks,” in ECCV, 2014.\\n[123] S. Xie, R. B. Girshick, P. Doll ´ar, Z. Tu, and K. He, “Aggregated residual\\ntransformations for deep neural networks,” in CVPR, 2017.\\n[124] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei,\\n“Deformable convolutional networks,” arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, “Autocollage,”ACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847–852, 2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='“Deformable convolutional networks,” arXiv:1703.06211, 2017.\\n[125] C. Rother, L. Bordeaux, Y . Hamadi, and A. Blake, “Autocollage,”ACM\\nTrans. on Graphics, vol. 25, no. 3, pp. 847–852, 2006.\\n[126] C. Jung and C. Kim, “A uniﬁed spectral-domain approach for saliency\\ndetection and its application to automatic object segmentation,” IEEE\\nTrans. Image Process., vol. 21, no. 3, pp. 1272–1283, 2012.\\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y . Chien, “Real-time salient object\\ndetection with a minimum spanning tree,” in CVPR, 2016.\\n[128] J. Yang and M.-H. Yang, “Top-down visual saliency via joint crf and\\ndictionary learning,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 39,\\nno. 3, pp. 576–588, 2017.\\n[129] P. L. Rosin, “A simple method for detecting salient regions,” Pattern\\nRecognition, vol. 42, no. 11, pp. 2363–2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n“Learning to detect a salient object,” IEEE Trans. Pattern Anal. Mach.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Recognition, vol. 42, no. 11, pp. 2363–2371, 2009.\\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y . Shum,\\n“Learning to detect a salient object,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 33, no. 2, pp. 353–367, 2011.\\n[131] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\\nfor semantic segmentation,” in CVPR, 2015.\\n[132] D. Gao, S. Han, and N. Vasconcelos, “Discriminant saliency, the detec-\\ntion of suspicious coincidences, and applications to visual recognition,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, pp. 989–1005, 2009.\\n[133] S. Xie and Z. Tu, “Holistically-nested edge detection,” in ICCV, 2015.\\n[134] M. K ¨ummerer, L. Theis, and M. Bethge, “Deep gaze i: Boost-\\ning saliency prediction with feature maps trained on imagenet,”\\narXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, “Salicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,”\\nin ICCV, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='arXiv:1411.1045, 2014.\\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, “Salicon: Reducing the\\nsemantic gap in saliency prediction by adapting deep neural networks,”\\nin ICCV, 2015.\\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, “Deep networks for saliency\\ndetection via local estimation and global search,” in CVPR, 2015.\\n[137] H. Cholakkal, J. Johnson, and D. Rajan, “Weakly supervised top-down\\nsalient object detection,” arXiv:1611.05345, 2016.\\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, “Saliency detection by\\nmulti-context deep learning,” in CVPR, 2015.\\n[139] C ¸ . Bak, A. Erdem, and E. Erdem, “Two-stream convolutional networks\\nfor dynamic saliency prediction,” arXiv:1607.04730, 2016.\\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, “Supercnn: A su-\\nperpixelwise convolutional neural network for salient object detection,”\\nInt. J. of Comput. Vision , vol. 115, no. 3, pp. 330–344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Int. J. of Comput. Vision , vol. 115, no. 3, pp. 330–344, 2015.\\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y . Zhuang, H. Ling, and\\nJ. Wang, “Deepsaliency: Multi-task deep neural network model for\\nsalient object detection,” IEEE Trans. Image Process. , vol. 25, no. 8,\\npp. 3919–3930, 2016.\\n[142] Y . Tang and X. Wu, “Saliency detection via combining region-level\\nand pixel-level predictions with cnns,” in ECCV, 2016.\\n[143] G. Li and Y . Yu, “Deep contrast learning for salient object detection,”\\nin CVPR, 2016.\\n[144] X. Wang, H. Ma, S. You, and X. Chen, “Edge preserving and\\nmulti-scale contextual neural network for salient object detection,”\\narXiv:1608.08029, 2016.\\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, “A deep multi-level\\nnetwork for saliency prediction,” in ICPR, 2016.\\n[146] G. Li and Y . Yu, “Visual saliency detection based on multiscale deep\\ncnn features,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012–\\n5024, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[146] G. Li and Y . Yu, “Visual saliency detection based on multiscale deep\\ncnn features,” IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012–\\n5024, 2016.\\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O’Connor,\\n“Shallow and deep convolutional networks for saliency prediction,” in\\nCVPR, 2016.\\n[148] J. Kuen, Z. Wang, and G. Wang, “Recurrent attentional networks for\\nsaliency detection,” in CVPR, 2016.\\n[149] Y . Tang, X. Wu, and W. Bu, “Deeply-supervised recurrent convolutional\\nneural network for saliency detection,” in ACM MM, 2016.\\n[150] X. Li, Y . Li, C. Shen, A. Dick, and A. Van Den Hengel, “Contextual\\nhypergraph modeling for salient object detection,” in ICCV, 2013.\\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, “Global\\ncontrast based salient region detection,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569–582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, “Salient object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='contrast based salient region detection,” IEEE Trans. Pattern Anal.\\nMach. Intell., vol. 37, no. 3, pp. 569–582, 2015.\\n[152] H. Jiang, J. Wang, Z. Yuan, Y . Wu, N. Zheng, and S. Li, “Salient object\\ndetection: A discriminative regional feature integration approach,” in\\nCVPR, 2013.\\n[153] G. Lee, Y .-W. Tai, and J. Kim, “Deep saliency with encoded low level\\ndistance map and high level features,” in CVPR, 2016.\\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\\n“Non-local deep features for salient object detection,” in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 20\\n[155] Q. Hou, M.-M. Cheng, X.-W. Hu, A. Borji, Z. Tu, and P. Torr,\\n“Deeply supervised salient object detection with short connections,”\\narXiv:1611.04849, 2016.\\n[156] Q. Yan, L. Xu, J. Shi, and J. Jia, “Hierarchical saliency detection,” in\\nCVPR, 2013.\\n[157] Y . Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, “The secrets of\\nsalient object segmentation,” in CVPR, 2014.\\n[158] V . Movahedi and J. H. Elder, “Design and perceptual validation of\\nperformance measures for salient object segmentation,” in CVPRW,\\n2010.\\n[159] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, “Salient object detection:\\nA benchmark,” IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706–\\n5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, “Graphical representation for\\nheterogeneous face recognition,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301–312, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='5722, 2015.\\n[160] C. Peng, X. Gao, N. Wang, and J. Li, “Graphical representation for\\nheterogeneous face recognition,” IEEE Trans. Pattern Anal. Mach.\\nIntell., vol. 39, no. 2, pp. 301–312, 2015.\\n[161] C. Peng, N. Wang, X. Gao, and J. Li, “Face recognition from multiple\\nstylistic sketches: Scenarios, datasets, and evaluation,” in ECCV, 2016.\\n[162] X. Gao, N. Wang, D. Tao, and X. Li, “Face sketchcphoto synthesis\\nand retrieval using sparse representation,” IEEE Trans. Circuits Syst.\\nVideo Technol., vol. 22, no. 8, pp. 1213–1226, 2012.\\n[163] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, “A comprehensive survey\\nto face hallucination,” Int. J. of Comput. Vision , vol. 106, no. 1, pp.\\n9–30, 2014.\\n[164] C. Peng, X. Gao, N. Wang, D. Tao, X. Li, and J. Li, “Multiple\\nrepresentations-based face sketch-photo synthesis.”IEEE Trans. Neural\\nNetw. & Learning Syst. , vol. 27, no. 11, pp. 2201–2215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, “Automatic facial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Netw. & Learning Syst. , vol. 27, no. 11, pp. 2201–2215, 2016.\\n[165] A. Majumder, L. Behera, and V . K. Subramanian, “Automatic facial\\nexpression recognition system using deep network-based data fusion,”\\nIEEE Trans. Cybern. , vol. 48, pp. 103–114, 2018.\\n[166] P. Viola and M. Jones, “Robust real-time face detection,” Int. J. of\\nComput. Vision, vol. 57, no. 2, pp. 137–154, 2004.\\n[167] J. Yu, Y . Jiang, Z. Wang, Z. Cao, and T. Huang, “Unitbox: An advanced\\nobject detection network,” in ACM MM, 2016.\\n[168] S. S. Farfade, M. J. Saberian, and L.-J. Li, “Multi-view face detection\\nusing deep convolutional neural networks,” in ICMR, 2015.\\n[169] S. Yang, P. Luo, C.-C. Loy, and X. Tang, “From facial parts responses\\nto face detection: A deep learning approach,” in ICCV, 2015.\\n[170] S. Yang, Y . Xiong, C. C. Loy, and X. Tang, “Face detection through\\nscale-friendly deep convolutional networks,” in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, “Scale-aware face'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='scale-friendly deep convolutional networks,” in CVPR, 2017.\\n[171] Z. Hao, Y . Liu, H. Qin, J. Yan, X. Li, and X. Hu, “Scale-aware face\\ndetection,” in CVPR, 2017.\\n[172] H. Wang, Z. Li, X. Ji, and Y . Wang, “Face r-cnn,” arXiv:1706.01061,\\n2017.\\n[173] X. Sun, P. Wu, and S. C. Hoi, “Face detection using deep learning: An\\nimproved faster rcnn approach,” arXiv:1701.08289, 2017.\\n[174] L. Huang, Y . Yang, Y . Deng, and Y . Yu, “Densebox: Unifying landmark\\nlocalization with end to end object detection,” arXiv:1509.04874, 2015.\\n[175] Y . Li, B. Sun, T. Wu, and Y . Wang, “face detection with end-to-end\\nintegration of a convnet and a 3d model,” in ECCV, 2016.\\n[176] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, “Joint face detection and\\nalignment using multitask cascaded convolutional networks,” IEEE\\nSignal Process. Lett. , vol. 23, no. 10, pp. 1499–1503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, “Compact convolutional neural\\nnetwork cascadefor face detection,” in CEUR Workshop, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Signal Process. Lett. , vol. 23, no. 10, pp. 1499–1503, 2016.\\n[177] I. A. Kalinovsky and V . G. Spitsyn, “Compact convolutional neural\\nnetwork cascadefor face detection,” in CEUR Workshop, 2016.\\n[178] H. Qin, J. Yan, X. Li, and X. Hu, “Joint training of cascaded cnn for\\nface detection,” in CVPR, 2016.\\n[179] V . Jain and E. Learned-Miller, “Fddb: A benchmark for face detection\\nin unconstrained settings,” Tech. Rep., 2010.\\n[180] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, “A convolutional neural\\nnetwork cascade for face detection,” in CVPR, 2015.\\n[181] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Aggregate channel features for\\nmulti-view face detection,” in IJCB, 2014.\\n[182] N. Marku ˇs, M. Frljak, I. S. Pand ˇzi´c, J. Ahlberg, and R. Forchheimer,\\n“Object detection with pixel intensity comparisons organized in deci-\\nsion trees,” arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, “Face\\ndetection without bells and whistles,” in ECCV, 2014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='sion trees,” arXiv:1305.4537, 2013.\\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, “Face\\ndetection without bells and whistles,” in ECCV, 2014.\\n[184] J. Li and Y . Zhang, “Learning surf cascade for fast and accurate object\\ndetection,” in CVPR, 2013.\\n[185] S. Liao, A. K. Jain, and S. Z. Li, “A fast and accurate unconstrained\\nface detector,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. 2,\\npp. 211–223, 2016.\\n[186] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Convolutional channel features,”\\nin ICCV, 2015.\\n[187] R. Ranjan, V . M. Patel, and R. Chellappa, “Hyperface: A deep multi-\\ntask learning framework for face detection, landmark localization, pose\\nestimation, and gender recognition,” arXiv:1603.01249, 2016.\\n[188] P. Hu and D. Ramanan, “Finding tiny faces,” in CVPR, 2017.\\n[189] Z. Jiang and D. Q. Huynh, “Multiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,” IEEE Trans.\\nImage Process., vol. 27, pp. 1361–1375, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[189] Z. Jiang and D. Q. Huynh, “Multiple pedestrian tracking from monoc-\\nular videos in an interacting multiple model framework,” IEEE Trans.\\nImage Process., vol. 27, pp. 1361–1375, 2018.\\n[190] D. Gavrila and S. Munder, “Multi-cue pedestrian detection and tracking\\nfrom a moving vehicle,” Int. J. of Comput. Vision , vol. 73, pp. 41–59,\\n2006.\\n[191] S. Xu, Y . Cheng, K. Gu, Y . Yang, S. Chang, and P. Zhou, “Jointly\\nattentive spatial-temporal pooling networks for video-based person re-\\nidentiﬁcation,” in ICCV, 2017.\\n[192] Z. Liu, D. Wang, and H. Lu, “Stepwise metric promotion for unsuper-\\nvised video person re-identiﬁcation,” in ICCV, 2017.\\n[193] A. Khan, B. Rinner, and A. Cavallaro, “Cooperative robots to observe\\nmoving targets: Review,” IEEE Trans. Cybern. , vol. 48, pp. 187–198,\\n2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\\nThe kitti dataset,” Int. J. of Robotics Res. , vol. 32, pp. 1231–1237,\\n2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='2018.\\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\\nThe kitti dataset,” Int. J. of Robotics Res. , vol. 32, pp. 1231–1237,\\n2013.\\n[195] Z. Cai, M. Saberian, and N. Vasconcelos, “Learning complexity-aware\\ncascades for deep pedestrian detection,” in ICCV, 2015.\\n[196] Y . Tian, P. Luo, X. Wang, and X. Tang, “Deep learning strong parts\\nfor pedestrian detection,” in CVPR, 2015.\\n[197] P. Doll ´ar, R. Appel, S. Belongie, and P. Perona, “Fast feature pyramids\\nfor object detection,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 36,\\nno. 8, pp. 1532–1545, 2014.\\n[198] S. Zhang, R. Benenson, and B. Schiele, “Filtered channel features for\\npedestrian detection,” in CVPR, 2015.\\n[199] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, “Pedestrian detec-\\ntion with spatially pooled features and structured ensemble learning,”\\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243–1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, “Discriminatively trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243–1257, 2016.\\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, “Discriminatively trained\\nand-or graph models for object shape detection,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 37, no. 5, pp. 959–972, 2015.\\n[201] M. Mathias, R. Benenson, R. Timofte, and L. Van Gool, “Handling\\nocclusions with franken-classiﬁers,” in ICCV, 2013.\\n[202] S. Tang, M. Andriluka, and B. Schiele, “Detection and tracking of\\noccluded people,” Int. J. of Comput. Vision, vol. 110, pp. 58–69, 2014.\\n[203] L. Zhang, L. Lin, X. Liang, and K. He, “Is faster r-cnn doing well for\\npedestrian detection?” in ECCV, 2016.\\n[204] Y . Tian, P. Luo, X. Wang, and X. Tang, “Deep learning strong parts\\nfor pedestrian detection,” in ICCV, 2015.\\n[205] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, “Multispectral deep\\nneural networks for pedestrian detection,” arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, “Pedestrian detection aided by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='neural networks for pedestrian detection,” arXiv:1611.02644, 2016.\\n[206] Y . Tian, P. Luo, X. Wang, and X. Tang, “Pedestrian detection aided by\\ndeep learning semantic tasks,” in CVPR, 2015.\\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, “Fused dnn: A deep neural\\nnetwork fusion approach to fast and robust pedestrian detection,” in\\nWACV, 2017.\\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, “Pushing\\nthe limits of deep cnns for pedestrian detection,” IEEE Trans. Circuits\\nSyst. Video Technol., 2017.\\n[209] D. Tom ´e, L. Bondi, L. Barofﬁo, S. Tubaro, E. Plebani, and D. Pau,\\n“Reduced memory region based deep convolutional neural network\\ndetection,” in ICCE-Berlin, 2016.\\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, “Taking a deeper\\nlook at pedestrians,” in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, “Scale-aware fast\\nr-cnn for pedestrian detection,” arXiv:1510.08160, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='look at pedestrians,” in CVPR, 2015.\\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, “Scale-aware fast\\nr-cnn for pedestrian detection,” arXiv:1510.08160, 2015.\\n[212] Y . Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu, “Visual-textual\\njoint relevance learning for tag-based social image search,”IEEE Trans.\\nImage Process., vol. 22, no. 1, pp. 363–376, 2013.\\n[213] T. Kong, F. Sun, A. Yao, H. Liu, M. Lv, and Y . Chen, “Ron: Reverse\\nconnection with objectness prior networks for object detection,” in\\nCVPR, 2017.\\n[214] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. C. Courville, and Y . Bengio, “Generative adversarial nets,”\\nin NIPS, 2014.\\n[215] Y . Fang, K. Kuan, J. Lin, C. Tan, and V . Chandrasekhar, “Object\\ndetection meets knowledge graphs,” in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, “Saliency-based sequential\\nimage attention with multiset prediction,” in NIPS, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection meets knowledge graphs,” in IJCAI, 2017.\\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, “Saliency-based sequential\\nimage attention with multiset prediction,” in NIPS, 2017.\\n[217] S. Azadi, J. Feng, and T. Darrell, “Learning detection with diverse\\nproposals,” in CVPR, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION 21\\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, “End-to-end\\nmemory networks,” in NIPS, 2015.\\n[219] P. Dabkowski and Y . Gal, “Real time image saliency for black box\\nclassiﬁers,” in NIPS, 2017.\\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Craft objects from images,” in\\nCVPR, 2016.\\n[221] I. Croitoru, S.-V . Bogolin, and M. Leordeanu, “Unsupervised learning\\nfrom video to detect foreground objects in single images,” in ICCV,\\n2017.\\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, “Weakly supervised object\\nlocalization with latent category learning,” in ECCV, 2014.\\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V . Ferrari,\\n“Training object class detectors with click supervision,” inCVPR, 2017.\\n[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, “Speed/accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[224] J. Huang, V . Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\\nZ. Wojna, Y . S. Song, S. Guadarrama, and K. Murphy, “Speed/accuracy\\ntrade-offs for modern convolutional object detectors,” in CVPR, 2017.\\n[225] Q. Li, S. Jin, and J. Yan, “Mimicking very efﬁcient network for object\\ndetection,” in CVPR, 2017.\\n[226] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\\nneural network,” Comput. Sci., vol. 14, no. 7, pp. 38–39, 2015.\\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY . Bengio, “Fitnets: Hints for thin deep nets,” Comput. Sci., 2014.\\n[228] X. Chen, K. Kundu, Y . Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\\nR. Urtasun, “3d object proposals for accurate object class detection,”\\nin NIPS, 2015.\\n[229] J. Dong, X. Fei, and S. Soatto, “Visual-inertial-semantic scene repre-\\nsentation for 3d object detection,” in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='[229] J. Dong, X. Fei, and S. Soatto, “Visual-inertial-semantic scene repre-\\nsentation for 3d object detection,” in CVPR, 2017.\\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\\n“Object detection in videos with tubelet proposal networks,” in CVPR,\\n2017.\\nZhong-Qiu Zhao is a professor at Hefei Univer-\\nsity of Technology, China. He obtained the Ph.D.\\ndegree in Pattern Recognition & Intelligent System\\nat University of Science and Technology, China, in\\n2007. From April 2008 to November 2009, he held a\\npostdoctoral position in image processing in CNRS\\nUMR6168 Lab Sciences de lInformation et des\\nSyst`emes, France. From January 2013 to December\\n2014, he held a research fellow position in image\\nprocessing at the Department of Computer Science\\nof Hongkong Baptist University, Hongkong, China.\\nHis research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='His research is about pattern recognition, image processing, and computer\\nvision.\\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\\nversity of Technology since 2010. He received his\\nBachelor’s degree in 2010 from Hefei University of\\nTechnology. His interests cover pattern recognition,\\nimage processing and computer vision.\\nShou-tao Xu is a Master student at Hefei University\\nof Technology. His research interests cover pattern\\nrecognition, image processing, deep learning and\\ncomputer vision.\\nXindong Wu is an Alfred and Helen Lamson En-\\ndowed Professor in Computer Science, University\\nof Louisiana at Lafayette (USA), and a Fellow of\\nthe IEEE and the AAAS. He received his Ph.D.\\ndegree in Artiﬁcial Intelligence from the University\\nof Edinburgh, Britain. His research interests include\\ndata mining, knowledge-based systems, and Web in-\\nformation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-04-17T00:45:22+00:00', 'author': '', 'keywords': '', 'moddate': '2019-04-17T00:45:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='formation exploration. He is the Steering Committee\\nChair of the IEEE International Conference on Data\\nMining (ICDM), the Editor-in-Chief of Knowledge\\nand Information Systems (KAIS, by Springer), and\\na Series Editor of the Springer Book Series on Advanced Information and\\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\\nComputer Society) between 2005 and 2008.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-10T12:55:11+00:00', 'title': 'Blue and White Colour Blocks Nurse Cover Letter', 'moddate': '2025-08-10T12:55:11+00:00', 'keywords': 'DAGvrFvTRX4,BAGL286-tNA,0', 'author': 'Darius Bengali', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='I am writing to present our proposal for a strategic skilling partnership between Krish Naik\\nAcademy (KrishAI Technologies Pvt. Ltd.) and the Nipuna Karnataka Scheme, aimed at\\ntransforming Karnataka into India’s AI Talent Engine.\\nAs a proud native of Gulbarga, Karnataka, my journey in AI began in 2017, overcoming resource\\nconstraints to build one of the world’s largest AI and Data Science learning communities, now\\nwith over 1.5 million learners trained across 180+ countries and 60,000+ successful career\\ntransitions. Our mission is to make AI & Data Science education accessible, affordable, and\\nindustry-aligned, ensuring every learner, regardless of location, has a pathway to global\\ntechnology careers.\\nOur proposal aligns directly with Nipuna Karnataka’s vision of skilling 1 crore youth by 2030,\\nfocusing on:\\nComprehensive AI & Data Skilling – Industry-aligned training tracks including Data Analyst,\\nData Scientist, GenAI Developer, AI Engineer (MLOps), and Big Data Engineer.'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-10T12:55:11+00:00', 'title': 'Blue and White Colour Blocks Nurse Cover Letter', 'moddate': '2025-08-10T12:55:11+00:00', 'keywords': 'DAGvrFvTRX4,BAGL286-tNA,0', 'author': 'Darius Bengali', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='focusing on:\\nComprehensive AI & Data Skilling – Industry-aligned training tracks including Data Analyst,\\nData Scientist, GenAI Developer, AI Engineer (MLOps), and Big Data Engineer.\\nPlacement-Focused Learning – Leveraging our AI Mock Interview Platform, structured\\nhiring network, and measurable KPIs to ensure employability.\\nRural & Inclusive Outreach – Hybrid delivery in Kannada + English, targeting Tier-2/3\\ndistricts and underrepresented groups.\\nFaculty Enablement – Train-the-Trainer programs for government institution educators to\\ncreate a self-sustaining AI talent ecosystem.\\nBy combining our proven expertise in large-scale, outcome-driven AI skilling with the\\nGovernment of Karnataka’s vision, we can build a future-ready workforce, strengthen\\nKarnataka’s leadership in technology, and make the state a national lighthouse for AI skills\\ndevelopment.\\nWe are excited about the possibility of contributing to this transformative journey and look'),\n",
       " Document(metadata={'producer': 'Canva', 'creator': 'Canva', 'creationdate': '2025-08-10T12:55:11+00:00', 'title': 'Blue and White Colour Blocks Nurse Cover Letter', 'moddate': '2025-08-10T12:55:11+00:00', 'keywords': 'DAGvrFvTRX4,BAGL286-tNA,0', 'author': 'Darius Bengali', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Karnataka’s leadership in technology, and make the state a national lighthouse for AI skills\\ndevelopment.\\nWe are excited about the possibility of contributing to this transformative journey and look\\nforward to discussing how we can work together to achieve the ambitious goals of the Nipuna\\nKarnataka Scheme.\\nThank you for your time and consideration.\\nWarm regards,\\nKrish Naik\\n10 August, 2025\\nTo Hon’ble Minister Priyank Kharge\\nIT BT Minister Karnataka\\nKrish Naik\\nFounder & CEO, KrishAI Technologies Pvt. Ltd.\\ncontact@krishnaik.in\\n+91 88673 53949\\nLinkedIn | YouTube')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bde24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 359 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12/12 [00:06<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (359, 384)\n",
      "Adding 359 documents to vector store...\n",
      "Successfully added 359 documents to vector store\n",
      "Total documents in collection: 1077\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498acd10",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7b0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351730b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x2604efc0ec0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e78529",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrag_retriever\u001b[49m.retrieve(\u001b[33m\"\u001b[39m\u001b[33mExecute a SELECT on all columns\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'rag_retriever' is not defined"
     ]
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d8141ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 112.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_bd5cc745_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'page': 2,\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941,\n",
       "   'doc_index': 61,\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'page_label': '3',\n",
       "   'total_pages': 27,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'keywords': '',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_442dbe7b_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'total_pages': 27,\n",
       "   'page': 2,\n",
       "   'file_type': 'pdf',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'doc_index': 61,\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'content_length': 941,\n",
       "   'page_label': '3',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'keywords': '',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_45e2cfa9_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'page_label': '3',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'total_pages': 27,\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 941,\n",
       "   'doc_index': 61,\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'keywords': '',\n",
       "   'page': 2},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_3497d198_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'page': 4,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'doc_index': 71,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'content_length': 937,\n",
       "   'total_pages': 27,\n",
       "   'page_label': '5',\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'keywords': '',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1'},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_bb00aef7_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'keywords': '',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'total_pages': 27,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'doc_index': 71,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'content_length': 937,\n",
       "   'page': 4,\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'page_label': '5',\n",
       "   'file_type': 'pdf',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00'},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23783e",
   "metadata": {},
   "source": [
    "### RAG Pipeline- VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4b617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40bba05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"gemma2-9b-it\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc0f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: gemma2-9b-it\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4110c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_bd5cc745_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'keywords': '',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'total_pages': 27,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'page_label': '3',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'content_length': 941,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'page': 2,\n",
       "   'doc_index': 61,\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_442dbe7b_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'total_pages': 27,\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 61,\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'keywords': '',\n",
       "   'content_length': 941,\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_45e2cfa9_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'keywords': '',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'total_pages': 27,\n",
       "   'content_length': 941,\n",
       "   'page': 2,\n",
       "   'doc_index': 61,\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'file_type': 'pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'page_label': '3',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'title': 'QZhou-Embedding Technical Report'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_3497d198_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'page': 4,\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'page_label': '5',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'total_pages': 27,\n",
       "   'content_length': 937,\n",
       "   'doc_index': 71,\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_bb00aef7_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'total_pages': 27,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'content_length': 937,\n",
       "   'page': 4,\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'doc_index': 71,\n",
       "   'page_label': '5',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea465ac",
   "metadata": {},
   "source": [
    "### Integration Vectordb Context pipeline With LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"gemma2-9b-it\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df1bf366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention mechanism?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An attention mechanism is a function that maps a query and a set of key-value pairs to an output vector, using a weighted sum of the values.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is attention mechanism?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857b1c2",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2832fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Hard Negative Mining Technqiues'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 95.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The text describes several hard negative mining techniques used in contrastive learning for retrieval models:\n",
      "\n",
      "* **ANCE:** Uses asynchronous ANN indexing and checkpoint states to periodically update hard negatives.\n",
      "* **Conan-Embedding:** Employs a dynamic strategy, excluding and refreshing samples based on score thresholds.\n",
      "* **NV-Retriever:**  Proposes positive-aware mining with TopK-MarginPos and TopKPercPos filtering to reduce false negatives.\n",
      "* **LGAI-Embedding:** Builds on NV-Retriever, using ANNA IR as a teacher model to identify high-quality hard negatives and TopKPercPos filtering. \n",
      "\n",
      "\n",
      "\n",
      "Sources: [{'source': 'emneddings.pdf', 'page': 4, 'score': 0.18709993362426758, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}, {'source': 'emneddings.pdf', 'page': 4, 'score': 0.18709993362426758, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}, {'source': 'emneddings.pdf', 'page': 4, 'score': 0.18709993362426758, 'preview': 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives u...'}]\n",
      "Confidence: 0.18709993362426758\n",
      "Context Preview: QZhou-Embedding Technical Report\n",
      " Kingsoft AI\n",
      "2.4 Hard Negative Mining Techniques\n",
      "Hard negatives serve as essential components in contrastive lear ning for retrieval model\n",
      "training. Early work like ANCE[\n",
      "46] proposed an asynchronous ANN indexing mech-\n",
      "anism that periodically updates hard negatives u\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Hard Negative Mining Technqiues\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa6150d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is attention is all you need'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "3.2 Attention\n",
      "An attention functi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n",
      "\n",
      "Question: what is attention is all you need\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: \"Attention Is All You Need\" is a paper that introduced the Transformer model, which relies solely on attention mechanisms for sequence transduction tasks, eliminating the need for recurrent or convolutional networks.  \n",
      "\n",
      "\n",
      "Citations:\n",
      "[1] attention.pdf (page 2)\n",
      "[2] attention.pdf (page 2)\n",
      "[3] attention.pdf (page 2)\n",
      "Summary: The paper \"Attention Is All You Need\" introduced the Transformer model, a novel architecture for sequence transduction tasks.  This model utilizes only attention mechanisms, dispensing with traditional recurrent or convolutional networks. \n",
      "\n",
      "\n",
      "\n",
      "History: {'question': 'what is attention is all you need', 'answer': '\"Attention Is All You Need\" is a paper that introduced the Transformer model, which relies solely on attention mechanisms for sequence transduction tasks, eliminating the need for recurrent or convolutional networks.  \\n', 'sources': [{'source': 'attention.pdf', 'page': 2, 'score': 0.1399550437927246, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}, {'source': 'attention.pdf', 'page': 2, 'score': 0.1399550437927246, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}, {'source': 'attention.pdf', 'page': 2, 'score': 0.1399550437927246, 'preview': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere...'}], 'summary': 'The paper \"Attention Is All You Need\" introduced the Transformer model, a novel architecture for sequence transduction tasks.  This model utilizes only attention mechanisms, dispensing with traditional recurrent or convolutional networks. \\n\\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is attention is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695e1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
